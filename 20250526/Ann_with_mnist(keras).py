# %%
import tensorflow as tf
from tensorflow.keras.datasets import mnist
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten

(x_train, y_train), ( x_test, y_test) = mnist.load_data()
x_train = x_train/255.0
x_test = x_test/255.0

# %% [markdown]
# (x_train, y_train), (x_test, y_test) = mnist.load_data(): í›ˆë ¨ ë°ì´í„° 6ë§Œê°œê³¼ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹
# 1ë§Œê°œë¥¼ ê°ê° (x_train, y_train)ê³¼ (x_test, y_test) ë³€ìˆ˜ì— ì €ì¥.
# x_train/255.0ê³¼ x_test/255.0: x_trainê³¼ x_testë¥¼ 0ê³¼ 1 ì‚¬ì´ì˜ ê°’ìœ¼ë¡œ ì •ê·œí™”.

# %%
model = Sequential([Flatten(input_shape=(28,28)),
                    Dense(123, activation='relu'),
                    Dense(10,activation='softmax')])
model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])
model.fit(x_train,y_train,epochs = 10, batch_size =32, validation_data=(x_test,y_test))

# %% [markdown]
# Flatten(input_shape=(28,28)ì…ë ¥ ë°ì´í„°ë¥¼ 2ì°¨ì›ì—ì„œ 1ì°¨ì›ìœ¼ë¡œ í‰íƒ„í™”(flatten)
# ì…ë ¥ ë°ì´í„°ì˜ í¬ê¸°ê°€ (28, 28)ì´ë¯€ë¡œ, ì´ë¥¼ 1ì°¨ì›ì¸ (784,) í˜•íƒœë¡œ ë³€í™˜.
# Dense(128,activation='relu):fully connected(ì™„ì „ ì—°ê²°) ë ˆì´ì–´ë¡œ, 128ê°œì˜ ë‰´ëŸ°
# ë ˆì´ì–´ëŠ” ì…ë ¥ê³¼ ì¶œë ¥ì´ ì™„ì „íˆ ì—°ê²°ë˜ì–´ ìˆê³ , ReLU í•¨ìˆ˜ë¥¼ í†µê³¼í•œ ê²°ê³¼ë¥¼ ì¶œë ¥.
# Dense(10,activation='softmax):ë§ˆì§€ë§‰ìœ¼ë¡œ ì¶œë ¥ ë ˆì´ì–´, 10ê°œì˜ ë‰´ëŸ°
# 10ê°œì˜ í´ë˜ìŠ¤ì— ëŒ€í•œ í™•ë¥  ë¶„í¬ë¥¼ ì¶œë ¥

# %%
x_train = x_train.reshape(-1,784)/255.0
x_test = x_test.reshape(-1,784)/255.0
model = Sequential([Dense(128,activation='relu', input_dim=784),
                    Dense(10,activation='softmax')])

# %% [markdown]
# â€¢x_test.reshape(-1, 784)
# â€¢2ì°¨ì› í˜•íƒœ: (N, 784)
# â€¢ì°¨ì›ë³„ íŠ¹ì„±ì˜ ì´ë¦„:
# ì²« ë²ˆì§¸ ì°¨ì›: ìƒ˜í”Œ ê°œìˆ˜ (N)
# â€¢ë‘ ë²ˆì§¸ ì°¨ì›: íŠ¹ì„± ê°œìˆ˜
# (784)

# %%
atest_loss, atest_accuracy = model.evaluate(x_test,y_test)
print('ANN Test loss', atest_loss)
print('ANN Test Accuracy:',atest_accuracy)

# %%
import tensorflow as tf
from tensorflow.keras.datasets import mnist
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D,MaxPool2D,Dense,Flatten

(x_train,y_train),(x_test,y_test) = mnist.load_data()
x_train = x_train.reshape(-1,28,28,1)/255.0
x_test = x_test.reshape(-1,28,28,1)/255.0


# %% [markdown]
# x_train.reshape(-1,28,28,1)í›ˆë ¨ ë°ì´í„°ì…‹ì¸ x_trainì„ 4ì°¨ì› ë°°ì—´ë¡œ ë³€í™˜
# reshape í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ë°°ì—´ì˜ í˜•íƒœë¥¼ (ìƒ˜í”Œ ê°œìˆ˜, ë†’ì´, ë„ˆë¹„, ì±„ë„)ë¡œ ë³€ê²½.
# -1ì€ ë‚¨ì€ ì°¨ì›ì„ ìë™ìœ¼ë¡œ ê³„ì‚°. 28, 28, 1ì€ ê°ê° ì´ë¯¸ì§€ì˜ ë†’ì´, ë„ˆë¹„, ì±„ë„.
# ì…ë ¥ ì´ë¯¸ì§€ëŠ” í‘ë°± ì´ë¯¸ì§€ì´ë¯€ë¡œ ì±„ë„ì€ 1.
# ì´ë¯¸ì§€ì˜ ê³µê°„ì ì¸ ì •ë³´ë¥¼ ë³´ì¡´í•˜ê¸° ìœ„í•œ êµ¬ì¡° => 4ì°¨ì› í˜•íƒœë¡œ ë³€í™˜í•˜ì—¬ ì…ë ¥
# â€¢ì²« ë²ˆì§¸ ì°¨ì›: ìƒ˜í”Œì˜ ê°œìˆ˜ (ì´ë¯¸ì§€ì˜ ìˆ˜)
#  í•œ ë²ˆì— ëª¨ë¸ì— ì…ë ¥ë˜ëŠ” ì´ë¯¸ì§€ì˜ ìˆ˜,
#  í•™ìŠµê³¼ í…ŒìŠ¤íŠ¸ ê³¼ì •ì—ì„œ ë°ì´í„°ë¥¼ ë¯¸ë‹ˆë°°ì¹˜
# â€¢ë‘ ë²ˆì§¸ ì°¨ì›: ì´ë¯¸ì§€ì˜ ë†’ì´
# â€¢ì„¸ ë²ˆì§¸ ì°¨ì›: ì´ë¯¸ì§€ì˜ ë„ˆë¹„
# â€¢ë„¤ ë²ˆì§¸ ì°¨ì›: ì´ë¯¸ì§€ì˜ ì±„ë„ ìˆ˜ (í‘ë°± ì´ë¯¸ì§€ì˜ ê²½ìš° 1, ì»¬ëŸ¬ ì´ë¯¸ì§€ì˜ ê²½ìš° 3)
# x_train = x_train.reshape(-1,28,28,1)/255.0:0ê³¼ 1 ì‚¬ì´ì˜ ê°’ìœ¼ë¡œ ì •ê·œí™”.
# x_test = x_test.reshape(-1,28,28,1)/255.0: x_testë„ 4ì°¨ì› ë°°ì—´ë¡œ ë³€í™˜í•˜ê³  ì •ê·œí™”.

# %%
model = Sequential([Conv2D(64,(4,4), activation='relu', input_shape = (28,28,1)),
                    MaxPool2D((2,2)),
                    Flatten(),
                    Dense(128,activation='relu'),
                    Dense(10,activation='softmax')])
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
model.fit(x_train,y_train,epochs=10,batch_size=32,validation_data=(x_test,y_test))

# %% [markdown]
# ì†ì‹¤ê°’ê³¼ ì •í™•ë„ê°€ ë‚®ì•„ì§€ëŠ” ê²½ìš°ì˜ ìƒí™©(?)
# 1.ëª¨ë¸ì˜ ì•„í‚¤í…ì²˜ë‚˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •ì˜ ë¬¸ì œ:
# ë„¤íŠ¸ì›Œí¬ì˜ ê¹Šì´, ë‰´ëŸ°ì˜ ìˆ˜, í•™ìŠµë¥  ë“±
# 2.ê³¼ì í•©(overfitting):
# ëª¨ë¸ì´ í›ˆë ¨ ë°ì´í„°ì— ê³¼ë„í•˜ê²Œ ë§ì¶°ì ¸ì„œ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ëŒ€í•œ ì¼ë°˜í™” ëŠ¥ë ¥ì´
# ë–¨ì–´ì§ˆ ìˆ˜ ìˆë‹¤.
# 3.ë” ë³µì¡í•œ ëª¨ë¸ í•„ìš”:
# ë” ê¹Šê±°ë‚˜ ë³µì¡í•œ ëª¨ë¸ êµ¬ì¡°ë¥¼ ì‚¬ìš©í•˜ì—¬ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤

# %% [markdown]
# ì£¼ìš” ì°¨ì´ì ì€ CNN ë°©ì‹ì—ì„œ Conv2Dê³¼ MaxPooling2Dì„ ì‚¬ìš©.
# cnnì€ ì´ë¯¸ì§€ì™€ ê°™ì€ ê³µê°„ ì •ë³´ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ì²˜ë¦¬
# .
# ANN ë°©ì‹ì€ 2D ì´ë¯¸ì§€ ë°ì´í„°ë¥¼ ë‹¨ìˆœíˆ 1d ë°±í„°ë¡œ í‰íƒ„í™”í•˜ê³ , í›„ì† ì—°ì‚°ì—ëŠ” ì™„ì „ ì—°ê²°(Dense)ì¸µ
# ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
# cnnì€ ì´ë¯¸ì§€ì˜ ê³µê°„ì •ë³´ (4dë°°ì—´)ë¥¼ ì²˜ë¦¬í•˜ëŠ” ë° íŠ¹í™” ë˜ì–´ ìˆë‹¤
# annì€ ì¼ë°˜ì €ê¸ë‹ˆ ë¶„ë¥˜ì‘ì—…ì— ë”ë§ì´ ì‚¬ìš©
# annì˜ ë‹¨ì ìœ¼ë¡œ
# â€¢ì´ë¯¸ì§€ì™€ ê°™ì€ 2D ë°ì´í„°ì˜ ê³µê°„ ì •ë³´ë¥¼ ì˜ í™œìš©í•˜ì§€ ëª»í•  ìˆ˜ ìˆë‹¤.
# â€¢ë§ì€ ì€ë‹‰ì¸µì„ ê°€ì§ˆ ê²½ìš°, ê³¼ì í•¨(overfitting)ë¬¸ì œê°€ ë°œìƒ.
# cnnì˜ ë‹¨ì 
# â€¢ANNë³´ë‹¤ êµ¬ì¡°ê°€ ë³µì¡í•˜ê³  ê³„ì‚°ëŸ‰ì´ ë§ì•„
# â€¢ëŒ€ê·œëª¨ ì´ë¯¸ì§€ ë°ì´í„°ì…‹ í•™ìŠµì„ ìœ„í•´ì„  ë§ì€ ì»´í“¨íŒ… ìì›ì´ í•„ìš”í•  ìˆ˜ ìˆë‹¤.

# %% [markdown]
# êµ¬ë¶„ ANN CNN
# ë°ì´í„° êµ¬ì¡° ì…ë ¥ ë°ì´í„°ë¥¼ 1ì°¨ì› ë²¡í„°ë¡œ í¼ì¹¨ ì…ë ¥ ë°ì´í„°ì˜ ê³µê°„ êµ¬ì¡°ë¥¼ ìœ ì§€
# ì£¼ìš” íŠ¹ì§• ê³µê°„ ì •ë³´ ì†ì‹¤ ê³µê°„ ì •ë³´ ë³´ì¡´
# ì˜ˆì‹œ ì´ë¯¸ì§€ í”½ì…€ì„ ì¼ë ¬ë¡œ í¼ì¹¨ ì´ë¯¸ì§€ì˜ ì§€ì—­ì  íŒ¨í„´ì„ ì¸ì‹
# ì‚¬ìš© ì‚¬ë¡€ í…ìŠ¤íŠ¸ ë¶„ë¥˜, ìˆ«ì ì¸ì‹ ë“± ì´ë¯¸ì§€ ë¶„ë¥˜, ê°ì²´ ê°ì§€ ë“±

# %% [markdown]
# ANN (Artificial Neural Network) CNN (Convolutional Neural Network)
# ë°ì´í„° ì „ì²˜ë¦¬ ì…ë ¥ ë°ì´í„°ë¥¼ 1ì°¨ì› ë²¡í„°ë¡œ í¼ì¹¨ ì…ë ¥ ë°ì´í„°ì˜ ê³µê°„ êµ¬ì¡°ë¥¼ ìœ ì§€
# ëª¨ë¸ êµ¬ì„± model = Sequential() model = Sequential()
# model.add(Dense(units=64, activation='relu', input_s
# hape=(784,)))
# model.add(Conv2D(32, kernel_size=(3, 3), activation='relu'
# , input_shape=(28, 28, 1)))
# model.add(Dense(units=10, activation='softmax'))
# model.add(MaxPooling2D(pool_size=(2, 2)))
# Flatten(),
#  Dense(64, activation='relu'),
#  Dense(10, activation='softmax')
# í•™ìŠµ ê³¼ì • model.compile(optimizer='adam', loss='categorical_c
# rossentropy', metrics=['accuracy'])
# model.compile(optimizer='adam', loss='categorical_crosse
# ntropy', metrics=['accuracy'])
# model.fit(X_train, y_train, epochs=10, batch_size=32) model.fit(X_train, y_train, epochs=10, batch_size=32)

# %% [markdown]
# í‘œì¤€í™” ë°ì´í„°ì˜ í‰ê· ì„ 0ìœ¼ë¡œ, í‘œì¤€í¸ì°¨ë¥¼ 1ë¡œ ë§Œë“¤ì–´ ë°ì´í„°ì˜ ë¶„í¬ë¥¼ ì¡°ì •í•˜ëŠ” ì‘ì—….
# í‘œì¤€í™”ëŠ” ë°ì´í„°ë¥¼ ì •ê·œë¶„í¬ë¡œ ë³€í™˜í•˜ì—¬ í‰ê· ê³¼ í‘œì¤€í¸ì°¨ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë°ì´í„°ë¥¼ ì¡°ì •. ì´ë¥¼
# í†µí•´ ë°ì´í„°ì˜ ë²”ìœ„ê°€ í‘œì¤€í™”ë˜ì–´ ê° íŠ¹ì„±ì´ ë™ì¼í•œ ì¤‘ìš”ë„ë¥¼ ê°€ì§€ê²Œ ëœë‹¤.
# í‰ê·  0ì„ ê¸°ì¤€ìœ¼ë¡œ ì¢Œìš°(-,+)ë¡œ ìœ„ì¹˜í•˜ëŠ” ìˆ«ì í‘œì‹œ
# 2)ì •ê·œí™”
# ë°ì´í„°ì˜ ë²”ìœ„ë¥¼ 0ê³¼ 1 ì‚¬ì´ë¡œ ì¡°ì •í•˜ì—¬ ë°ì´í„°ë¥¼ ë¹„ìœ¨ì ìœ¼ë¡œ ì¼ì¹˜ì‹œí‚¤ëŠ” ì‘ì—….
# ì •ê·œí™”ëŠ” ë°ì´í„°ë¥¼ ìµœì†Œê°’ê³¼ ìµœëŒ€ê°’ìœ¼ë¡œ ë‚˜ëˆ„ì–´ì„œ ë°ì´í„°ë¥¼ 0ê³¼ 1 ì‚¬ì´ì˜ ë²”ìœ„ë¡œ ì¡°ì •.
# ë°ì´í„°ì˜ ìƒëŒ€ì ì¸ ë¹„ìœ¨ì„ ìœ ì§€í•˜ë©´ì„œ ìŠ¤ì¼€ì¼ì„ ì¡°ì •.
# ìµœëŒ€ê°’ìœ¼ë¡œ ë‚˜ëˆ„ì—ˆìœ¼ë¯€ë¡œ 0 ~ 1 ì‚¬ì´ ê°’
# 3) ì›í•«ì¸ì½”ë”©
# ë²”ì£¼í˜• ë°ì´í„°ë¥¼ ìˆ«ìë¡œ ë³€í™˜í•˜ëŠ” ê³¼ì • ë²”ì£¼ì— ëŒ€í•´ í•´ë‹¹í•˜ëŠ” ìœ„ì¹˜ëŠ” 1ë¡œ í‘œì‹œí•˜ê³ , ë‚˜ë¨¸ì§€
# ìœ„ì¹˜ëŠ” 0ìœ¼ë¡œ í‘œì‹œ. ì´ë¥¼ í†µí•´ ë²”ì£¼í˜• ë°ì´í„°ë¥¼ ë‹¤ë£° ìˆ˜ ìˆê²Œ ëœë‹¤.

# %% [markdown]
# ì•„ë˜ëŠ” ì´ë¯¸ì§€ì—ì„œ ì¶”ì¶œí•œ í…ìŠ¤íŠ¸ë¥¼ ì •ë¦¬í•˜ê³  í•´ì„¤ì„ ë§ë¶™ì¸ ë‚´ìš©ì…ë‹ˆë‹¤. CNNì—ì„œ `Conv2D` ë ˆì´ì–´ë¥¼ ì‚¬ìš©í•  ë•Œ `input_shape`, `kernel_size`, `filters` ë“±ì˜ íŒŒë¼ë¯¸í„° ì‚¬ìš© ë°©ì‹ì´ ì˜ ì„¤ëª…ë˜ì–´ ìˆìŠµë‹ˆë‹¤.
# 
# ---
# 
# ## ğŸ”¹ Conv2D ë ˆì´ì–´ ì‚¬ìš© ì˜ˆì‹œ ë° ì„¤ëª…
# 
# ### âœ… ë‹¨ì¼ ì±„ë„ í‘ë°± ì´ë¯¸ì§€ ì…ë ¥
# 
# ```python
# Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1))
# ```
# 
# * ì…ë ¥ ì´ë¯¸ì§€ í¬ê¸°: 28x28
# * ì±„ë„ ìˆ˜: 1 (í‘ë°± ì´ë¯¸ì§€)
# 
# ---
# 
# ### âœ… RGB ì´ë¯¸ì§€ ì…ë ¥ (ì»¬ëŸ¬ ì´ë¯¸ì§€)
# 
# ```python
# Conv2D(64, (5, 5), activation='relu', input_shape=(32, 32, 3))
# ```
# 
# * ì…ë ¥ ì´ë¯¸ì§€ í¬ê¸°: 32x32
# * ì±„ë„ ìˆ˜: 3 (RGB)
# * í•„í„° ê°œìˆ˜: 64ê°œ
# * ì»¤ë„ ì‚¬ì´ì¦ˆ: 5x5
# 
# ---
# 
# ### âœ… ì…ë ¥ í¬ê¸° ìœ ë™ì ì¸ ê²½ìš°
# 
# ```python
# Conv2D(16, (3, 3), activation='relu', input_shape=(None, None, 1))
# ```
# 
# * ì…ë ¥ í¬ê¸°: ê°€ë³€ (ì˜ˆ: ë‹¤ì–‘í•œ í•´ìƒë„ì˜ ë‹¨ì¼ ì±„ë„ ì´ë¯¸ì§€)
# * ì±„ë„ ìˆ˜: 1 (í‘ë°± ì´ë¯¸ì§€)
# 
# ---
# 
# ### âœ… ë‹¤ë¥¸ í‘œê¸° ë°©ì‹ ì˜ˆì‹œ
# 
# ```python
# Conv2D(activation='relu', input_shape=(32, 32, 3), filters=64, kernel_size=(3, 3))
# ```
# 
# * ì¸ì ìˆœì„œë¥¼ ë°”ê¾¼ í‘œí˜„ ë°©ì‹
# * `filters`ì™€ `kernel_size`ë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì‘ì„± (ìˆœì„œì™€ ë¬´ê´€)
# 
# > âœ… **ì¤‘ìš”**:
# > Conv2Dì˜ ì¸ì ìˆœì„œëŠ” ë³€ê²½í•´ë„ ë¬´ë°©í•˜ì§€ë§Œ, **ê°€ë…ì„±**ì„ ê³ ë ¤í•´ ì¼ê´€ë˜ê²Œ ì‘ì„±í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.
# 
# ---
# 
# ## ğŸ”¸ ìš”ì•½ í¬ì¸íŠ¸
# 
# * `input_shape=(height, width, channels)` ë¡œ ì •ì˜
# * `kernel_size=(3, 3)` ì€ 3x3 í•„í„°ë¥¼ ì˜ë¯¸
# * `filters=64` â†’ 64ê°œì˜ íŠ¹ì§• ë§µ(feature map)ì„ ìƒì„±
# * ì´ë¯¸ì§€ì˜ ìƒ‰ìƒ ì±„ë„ ìˆ˜ì— ë”°ë¼ ì…ë ¥ ì°¨ì›ì´ ë‹¬ë¼ì§:
# 
#   * í‘ë°±: 1
#   * RGB: 3
#   * CMYK: 4
# 
# ---
# 

# %%
import tensorflow as tf
import numpy as np
from tensorflow.keras.datasets import mnist

(Q_train,A_train),(Q_test,A_test) = mnist.load_data()

print('\n train shape = ',Q_train.shape,
      ', Label shape = ',A_train.shape)
print(' test shape = ',Q_test.shape,
      ', Label shape =', A_test.shape)
print('\n trian Label =', A_train) # í•™ìŠµë°ì´í„° ì •ë‹µ ì¶œë ¥
print(' test Label(A_test)',A_test)

# %% [markdown]
# csv ë¡œ ë°›ì•„ ë³¼ ê²½ìš°ëŠ” ì •ë‹µë°ì´í„° 1ê°œì™€ 0ë¶€í„° 255 ê¹Œì§€ì˜ ìˆ«ìê°€ 784(28*28)ê°€ ì½¤ë§ˆë¡œ ë¶„ë¦¬ë˜ì–´ ì¡´ì¬í•œë‹¤. ë˜í•œ ì¼€ë¼ìŠ¤ì— ë‚´ì¥ëœ ë°ì´í„°ëŠ” ì •ë‹µê³¼ í›ˆë ¨ë°ì´í„°ê°€ ë¶„ë¦¬ë˜ì–´ ìˆë‹¤

# %%
zeroidx_train_data = Q_train[0]

for row in zeroidx_train_data:
  for pixel in row:
    print(f'{pixel:3d}', end=' ')
  print()

# %%
import matplotlib.pyplot as plt

# 25 image print
plt.figure(figsize=(6,6))

for idx in range(25):
  plt.subplot(5,5,idx +1) # 5row,5col
  plt.imshow(Q_train[idx],cmap='gray')
  plt.axis('off')
plt.show()

# %%
plt.imshow(Q_train[9],cmap='gray')
plt.colorbar()
plt.show()

# %%
plt.title('Label distribution(train)')
plt.grid()
plt.xlabel('Label')

plt.hist(A_train, bins =10, rwidth=0.8)
plt.show()

# %% [markdown]
# ë¼ë²¨ ë°ì´í„°(íƒ€ê²Ÿ ë°ì´í„°) ë³„ ë¹ˆë„ìˆ˜ë¥¼
# íˆìŠ¤í† ê·¸ë¨ìœ¼ë¡œ ì‹œê°í™” ì¶œë ¥í•œë‹¤
# MNISTëŠ” ì™„ë²½í•œ ê· ë“± ë¶„í¬ê°€ ì•„ë‹Œ,
# ê° ìˆ«ìë³„ë¡œ ë¹„ìŠ·í•œ ìˆ˜ì¤€ì˜ í‘œë³¸ ìˆ˜ë¥¼ ê°€ì§„
# ë¹„ê· ë“±í•œ ë¶„í¬
# ëª¨ë¸ì´ ìì£¼ ë“±ì¥í•˜ëŠ” ìˆ«ìì— í¸í–¥ë¨
# ì¦‰, â€œìì£¼ ë‚˜ì˜¤ëŠ” í´ë˜ìŠ¤=ì¤‘ìš”í•œ í´ë˜ìŠ¤â€ë¼ê³  ì˜ëª» ë°°ìš¸ ìœ„í—˜
# >> ìƒ˜í”Œë§ ì¡°ì •, ê°€ì¤‘ì¹˜ ì¡°ì •, ë°ì´í„° ì¦ê°• ë“±ìœ¼ë¡œ í•´ê²° ì‹œë„.

# %%
answer_distribution = np.zeros(10)

for i in range(len(A_train)):
  answer = int(A_train[i])
  answer_distribution[answer] = answer_distribution[answer] +1
print(answer_distribution)

# %% [markdown]
# answer_distribution = np.zeros(10)í¬ê¸°ê°€ 10ì¸ 0ìœ¼ë¡œ ì´ˆê¸°í™”ëœ ë°°ì—´ ìƒì„±.
# for i in range(len(A_train)):A_trainì˜ ê¸¸ì´ë§Œí¼ ë°˜ë³µ
#  í›ˆë ¨ ë°ì´í„°ì˜ ë ˆì´ë¸”ì„ í•˜ë‚˜ì”© í™•ì¸í•˜ê¸° ìœ„í•œ ë°˜ë³µë¬¸.
# answer = int(A_train[i]): A_trainì˜ idxë²ˆì§¸ ìš”ì†Œë¥¼ ì •ìˆ˜ë¡œ ë³€í™˜í•˜ì—¬ answer ë³€ìˆ˜ì— ì €ì¥.
# ì´ëŠ” í˜„ì¬ ìˆœíšŒ ì¤‘ì¸ í›ˆë ¨ ë°ì´í„°ì˜ ë ˆì´ë¸”ì„ ë‚˜íƒ€ë‚¸ë‹¤.
# answer_distribution[answer] = answer_distribution[answer] +1: answer ë³€ìˆ˜ì— í•´ë‹¹í•˜ëŠ”
# ì¸ë±ìŠ¤ ìœ„ì¹˜ì˜ answer_distribution ê°’ì„ 1 ì¦ê°€. í˜„ì¬ ë ˆì´ë¸”ì˜ ê°œìˆ˜ë¥¼ 1 ì¦ê°€.

# %%
Q_train = (Q_train - 0.0)/(255.0-0.0)#Min_Max
Q_test = (Q_test - 0.0)/(255.0-0.0)#Min_Max

A_train = tf.keras.utils.to_categorical(A_train,num_classes=10) # ì›í•«ì¸ì½”ë”© ì²˜ë¦¬
A_test = tf.keras.utils.to_categorical(A_test,num_classes=10) # ì›í•«ì¸ì½”ë”© ì²˜ë¦¬

# %% [markdown]
# ì…ë ¥ ë°ì´íŠ¸ ë¯¼ë§¥ìŠ¤ ì²˜ë¦¬
# ë¼ë²¨ë°ì´í„° ì›í•«ì¸ì½”ë”©

# %% [markdown]
# Q_train = (Q_train - 0.0)/(255.0-0.0)
# í›ˆë ¨ ë°ì´í„°ì¸ x_trainì˜ ê°’ì„ 0ì—ì„œ 255 ì‚¬ì´ë¡œ ì •ê·œí™”(normalization).
# ì´ë¥¼ ìœ„í•´ ëª¨ë“  ê°’ì„ 0(min value)ìœ¼ë¡œ ë¹¼ê³ , 255ë¡œ ë‚˜ëˆ„ì–´ì„œ ê° ê°’ì´ 0ê³¼ 1 ì‚¬ì´ì˜ ë²”ìœ„ë¡œ ë§ì¶˜ë‹¤.
# Q_test = (Q_test - 0.0)/(255.0-0.0)#Min_Max
# í…ŒìŠ¤íŠ¸ ë°ì´í„°ì¸ x_testì—ë„ ë™ì¼í•œ ì •ê·œí™” ê³¼ì •ì„ ì ìš©.
# A_train = tf.keras.utils.to_categorical(A_train,num_classes=10)
# í›ˆë ¨ ë°ì´í„°ì˜ ë ˆì´ë¸”ì¸ t_trainì„ ì›-í•« ì¸ì½”ë”©(one-hot encoding) í˜•ì‹ìœ¼ë¡œ ë³€í™˜.
# ì´ë¥¼ ìœ„í•´ to_categorical í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ê³ , ë ˆì´ë¸”ì˜ í´ë˜ìŠ¤ ê°œìˆ˜ì¸ num_classesë¥¼ 10ìœ¼ë¡œ ì§€ì •.
# ì´ ê³¼ì •ì„ í†µí•´ ê° ë ˆì´ë¸”ì€ 10ì°¨ì›ì˜ ë²¡í„°ë¡œ ë³€í™˜ë˜ë©°,
# í•´ë‹¹í•˜ëŠ” ì¸ë±ìŠ¤ë§Œ 1ì´ê³  ë‚˜ë¨¸ì§€ëŠ” 0ì¸ í˜•íƒœë¡œ í‘œí˜„.
# A_test = tf.keras.utils.to_categorical(A_test,num_classes=10)
# í…ŒìŠ¤íŠ¸ ë°ì´í„°ì˜ ë ˆì´ë¸”ì—ë„ ë™ì¼í•œ ì›-í•« ì¸ì½”ë”© ê³¼ì •ì„ ì ìš©.
# * ì˜¤ë¥˜ ì£¼ì˜ : ì˜¤íƒ€ë“±ìœ¼ë¡œ ì¬ì‹¤í–‰ì‹œ ì›í•« ì¸ì½”ë”©ì´ ë‘ë²ˆ ì‹¤í–‰ë  ìˆ˜ ìˆë‹¤.
# ì²˜ìŒ: ì •ìˆ˜ ë ˆì´ë¸” (ì˜ˆ: 3) â†’ [0,0,0,1,0,0,0,0,0,0] (shape: (batch_size, 10))
# ë‹¤ì‹œ í•œ ë²ˆ ì ìš©í•˜ë©´ â†’ one-hot ì¸ì½”ë”©ëœ ê²ƒì„ ë˜ ì¸ì½”ë”© â†’ (batch_size, 10, 10)

# %% [markdown]
# í›ˆë ¨ ë°ì´í„°ì˜ ë ˆì´ë¸”ì¸ t_trainì„ ì›-í•« ì¸ì½”ë”©(one-hot encoding) í˜•ì‹ìœ¼ë¡œ ë³€í™˜.
# ì´ë¥¼ ìœ„í•´ to_categorical í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ê³ , ë ˆì´ë¸”ì˜ í´ë˜ìŠ¤ ê°œìˆ˜ì¸ num_classesë¥¼ 10ìœ¼ë¡œ ì§€ì •.
# ì´ ê³¼ì •ì„ í†µí•´ ê° ë ˆì´ë¸”ì€ 10ì°¨ì›ì˜ ë²¡í„°ë¡œ ë³€í™˜ë˜ë©°,
# í•´ë‹¹í•˜ëŠ” ì¸ë±ìŠ¤ë§Œ 1ì´ê³  ë‚˜ë¨¸ì§€ëŠ” 0ì¸ í˜•íƒœë¡œ í‘œ

# %%
model = tf.keras.Sequential()
model.add(tf.keras.layers.Flatten(input_shape=(28,28)))

model.add(tf.keras.layers.Dense(100,activation='relu'))
model.add(tf.keras.layers.Dense(10,activation='softmax'))

# %% [markdown]
# ë ¥(28*28=784) : 2ì°¨ì›ì˜ ì´ë¯¸ì§€ ë°ì´í„°ë¥¼ 1ì°¨ì›ìœ¼ë¡œ í‰íƒ„í™”.
#  ì…ë ¥ ì´ë¯¸ì§€ì˜ í¬ê¸°ê°€ (28, 28)ì´ë¯€ë¡œ input_shapeë¥¼ (28, 28)ë¡œ ì§€ì •
# ì…ë ¥ 1ê°œë‹¹ 100ê°œì˜ ë…¸ë“œì™€ bias 100ê°œê°€ ì„ì˜ì˜ ì´ˆê¸°ê°’ìœ¼ë¡œ ì¶”ê°€
#  (fully connected,íŒŒë¼ë¯¸í„° 78,500 = 784(input)*100(node) + 100(bias) )
# ì…ë ¥ 100ì—ì„œ ì¶œë ¥ 10ê°œì™€ bias 10ê°œ ì¶”ê°€ ë…¸ë“œ(1010 = 100(input) * 10(node) + 10(bias))

# %% [markdown]
# model = tf.keras.Sequential()
# Sequential ëª¨ë¸ì€ ë ˆì´ì–´ë¥¼ ì„ í˜•ìœ¼ë¡œ ìŒ“ì•„ êµ¬ì„±í•˜ëŠ” ê°€ì¥ ê°„ë‹¨í•œ í˜•íƒœì˜ ëª¨ë¸.
# model.add(tf.keras.layers.Flatten(input_shape=(28,28)))
# Flatten ë ˆì´ì–´ 2ì°¨ì›ì˜ ì´ë¯¸ì§€ ë°ì´í„°ë¥¼ 1ì°¨ì›ìœ¼ë¡œ í‰íƒ„í™”.
#  ì…ë ¥ ì´ë¯¸ì§€ì˜ í¬ê¸°ê°€ (28, 28)ì´ë¯€ë¡œ input_shapeë¥¼ (28, 28)ë¡œ ì§€ì •.
#  model.add(tf.keras.layers.Dense(100,activation='relu'))
# Dense ë ˆì´ì–´ë¥¼ ì¶”ê°€. ì´ ë ˆì´ì–´ëŠ” 100ê°œì˜ ë‰´ëŸ°ì„ ê°€ì§€ë©°, í™œì„±í™” í•¨ìˆ˜ë¡œ ReLUë¥¼ ì‚¬ìš©.
# ì´ ë ˆì´ì–´ëŠ” ì…ë ¥ê³¼ ëª¨ë“  ë‰´ëŸ° ì‚¬ì´ì— ì—°ê²°ì´ ì¡´ì¬í•˜ëŠ” ì™„ì „ ì—°ê²°ì¸µ(Fully Connected Layer)
# model.add(tf.keras.layers.Dense(10,activation='softmax')).
# ë§ˆì§€ë§‰ìœ¼ë¡œ Dense ë ˆì´ì–´ë¥¼ ì¶”ê°€. ì´ ë ˆì´ì–´ëŠ” 10ê°œì˜ ë‰´ëŸ°ì„ ê°€ì§€ë©°, í™œì„±í™” í•¨ìˆ˜ë¡œ softmaxë¥¼ ì‚¬ìš©.
# ì´ ë ˆì´ì–´ëŠ” ìµœì¢… ì¶œë ¥ì„ ê° í´ë˜ìŠ¤(0ë¶€í„° 9ê¹Œì§€ì˜ ìˆ«ì)ì— ëŒ€í•œ í™•ë¥ ë¡œ ë³€í™˜í•˜ëŠ” ì—­í• .

# %% [markdown]
# ì™„ì „ ì—°ê²°ì¸µ
# ì¸ê³µ ì‹ ê²½ë§ì—ì„œ ê°€ì¥ ê¸°ë³¸ì ì¸ ì¸µ, ëª¨ë“  ì…ë ¥ ë‰´ëŸ°ê³¼ ì¶œë ¥ ë‰´ëŸ°ì´ ì„œë¡œ ì—°ê²°ë˜ì–´ ìˆëŠ” ì¸µ.
# ê° ì…ë ¥ ë‰´ëŸ°ì€ ì¶œë ¥ ë‰´ëŸ°ê³¼ ê°€ì¤‘ì¹˜ë¡œ ì—°ê²°ë˜ì–´ ìˆìœ¼ë©°, ì´ ê°€ì¤‘ì¹˜ëŠ” í•™ìŠµ ê³¼ì •ì—ì„œ ì¡°ì •.

# %%

model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),
loss='categorical_crossentropy',
metrics=['accuracy'])
model.summary()

# %% [markdown]
# 
# model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),
# loss='categorical_crossent ropy',
# metrics=['accuracÑƒ'])ëª¨ë¸ì„ ì»´íŒŒì¼. ì´ë•Œ ì˜µí‹°ë§ˆì´ì €ë¡œëŠ” Adamì„ ì‚¬ìš©í•˜ë©°, í•™ìŠµë¥ ì€ 1e-3(0.001)ë¡œ ì„¤ì •.
# ì†ì‹¤ í•¨ìˆ˜ë¡œëŠ” categorical_crossentropyë¥¼ ì‚¬ìš©í•˜ë©°,
# í‰ê°€ ì§€í‘œë¡œëŠ” ì •í™•ë„(accuracy)ë¥¼ ì‚¬ìš©.
# model.summary()ëª¨ë¸ì˜ êµ¬ì¡°ë¥¼ ìš”ì•½í•˜ì—¬ ì¶œë ¥.
# ê° ë ˆì´ì–´ì˜ ì´ë¦„, ì¶œë ¥ í¬ê¸°, íŒŒë¼ë¯¸í„° ê°œìˆ˜ ë“±ì˜ ì •ë³´ë¥¼ í™•ì¸.
# ì´ë¥¼ í†µí•´ ëª¨ë¸ì˜ êµ¬ì¡°ì™€ íŒŒë¼ë¯¸í„° ê°œìˆ˜ë¥¼ ì‰½ê²Œ íŒŒì•…í•  ìˆ˜ ìˆë‹¤.

# %%
ModelFit = model.fit(Q_train, A_train, epochs=30, validation_split=0.3)

# %% [markdown]
# ModelFit = model.fit(Q_train, A_train, epochs=30, validation_split=0.3)
# ì£¼ì–´ì§„ í•™ìŠµ ë°ì´í„° x_trainê³¼ ì •ë‹µ ë°ì´í„° t_trainì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ í•™ìŠµ.
# epochs=30ì€ ì „ì²´ ë°ì´í„°ì…‹ì„ 30ë²ˆ ë°˜ë³µí•˜ì—¬ í•™ìŠµí•œë‹¤.
# validation_split=0.3ì€ í•™ìŠµ ì¤‘ì— 30%ì˜ ë°ì´í„°ë¥¼ ê²€ì¦(validation) ë°ì´í„°ë¡œ ì‚¬ìš©í•œë‹¤ëŠ” ì˜ë¯¸.
# í•™ìŠµ ê²°ê³¼ëŠ” ì €ì¥.
# ì£¼ì–´ì§„ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ í•™ìŠµí•˜ê³ ,
# ê° ì—í­(epoch)ë§ˆë‹¤ í•™ìŠµ ì†ì‹¤(loss)ê³¼ ì •í™•ë„(accuracy),
# ê²€ì¦ ì†ì‹¤(validation loss)ê³¼ ê²€ì¦ ì •í™•ë„(validation accuracy) ì¶œë ¥.
# í•™ìŠµ ì™„ë£Œ í›„ ê°ì²´ì—ëŠ” í•™ìŠµ ì´ë ¥(history) ì •ë³´ê°€ ì €ì¥ë˜ì–´ ìˆì–´ì„œ
# í•™ìŠµ ê³¼ì •ì„ ì‹œê°í™”í•˜ê±°ë‚˜ í‰ê°€ ë“±ì— í™œìš©í•  ìˆ˜ ìˆë‹¤.

# %%
test_loss,test_accuracy = model.evaluate(Q_test,A_test)

print('test loss :',test_loss)
print('test accuracy : ',test_accuracy)

# %% [markdown]
# odel.evaluate(Q_test,A_test)
# ì£¼ì–´ì§„ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì™€ ì •ë‹µ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ í‰ê°€
# ê²°ê³¼ accuracy: 0.9717 - loss: 0.1537
# ì‹¤í–‰ ê²°ê³¼ë¡œëŠ” í‰ê°€ ì§€í‘œë“¤ì´ ì¶œë ¥.
# ëŠ” ì •í™•ë„ë¥¼ ë‚˜íƒ€ë‚´ë©°, ëª¨ë¸ì´ ì…ë ¥ ë°ì´í„°ë¥¼ ì–¼ë§ˆë‚˜ ì •í™•í•˜ê²Œ ì˜ˆì¸¡í•˜ëŠ”ì§€ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤.
# ì •í™•ë„ëŠ” ì˜ˆì¸¡ ê²°ê³¼ê°€ ì •ë‹µê³¼ ì¼ì¹˜í•˜ëŠ” ìƒ˜í”Œì˜ ë¹„ìœ¨ë¡œ ê³„ì‚°.
# ì†ì‹¤ ê°’ìœ¼ë¡œ, ëª¨ë¸ì´ ì£¼ì–´ì§„ ì…ë ¥ ë°ì´í„°ì— ëŒ€í•´ ì–¼ë§ˆë‚˜ ì˜ ì˜ˆì¸¡í•˜ëŠ”ì§€ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤.
# ì£¼ë¡œ ì˜¤ì°¨ í•¨ìˆ˜(ì†ì‹¤ í•¨ìˆ˜)ì— ì˜í•´ ê³„ì‚°

# %%
plt.title('Loss')
plt.xlabel('epochs')
plt.ylabel('loss')
plt.grid()

plt.plot(ModelFit.history['loss'],label='train loss')
plt.plot(ModelFit.history['val_loss'],label='valid load')

plt.legend(loc='best')
plt.show()

# %%
plt.title('Loss')
plt.xlabel('epochs')
plt.ylabel('accuracy')
plt.grid()

plt.plot(ModelFit.history['accuracy'],label='train accuracy')
plt.plot(ModelFit.history['val_accuracy'],label='valid accuracy')

plt.legend(loc='best')
plt.show()

# %% [markdown]
# ìƒí™© ëŒ€ì‘ ë°©ë²•
# ê³¼ì í•©ì´ ì‹œì‘ë¨ early stopping, dropout,
# í•™ìŠµ íšŸìˆ˜ ì¤„ì´ê¸°
# ë°ì´í„° ë¶€ì¡±
# regularization ì‚¬ìš© ê³ ë ¤
# í˜„ì¬ë¡œì„  6~7 epoch ì •ë„ì—ì„œ early stop í•˜ëŠ” ê²ƒì´ ì¢‹ì„ ìˆ˜ ìˆìŒ
# data augmentation ë˜ëŠ” í•™ìŠµ ë°ì´í„° í™•ì¥ ê³ ë ¤

# %%
from sklearn.metrics import confusion_matrix
import seaborn as sns

plt.figure(figsize=(6, 6))

predicted_value = model.predict(Q_test) # 784ê°œì˜ ì…ë ¥ x_testê°€ 10ê°œì˜ ì¶œë ¥ê°’ìœ¼ë¡œ ë‚˜ì˜¨ë‹¤

Cmatrix = confusion_matrix(np.argmax(A_test, axis=-1), #ì›í•«ì¸ì½”ë”© ì²˜ë¦¬ëœ L_testì—ì„œ ì œì¼ í°ê°’ì„ ê°–ëŠ”
np.argmax (predicted_value, axis=-1)) # ì›í•«ì¸ì½”ë”© ì²˜ë¦¬ëœ ê°’ ì¤‘ ì œì¼ í°ê°’ì„ ê°–ëŠ” ì¸

sns.heatmap (Cmatrix, annot=True, fmt='d')
plt.show()

# %% [markdown]
# Cmatrix = confusion_matrix(np.argmax(t_test, axis=-1),  
#                            np.argmax(predicted_value, axis=-1))
# 
# ì‹¤ì œ ë ˆì´ë¸”(t_test)ê³¼ ì˜ˆì¸¡ëœ ê°’(predicted_value)ì„ ì‚¬ìš©í•˜ì—¬ í˜¼ë™ í–‰ë ¬(confusion matrix)ì„ ìƒì„±  
# â€» np.argmax(t_test, axis=-1): ì‹¤ì œ ë ˆì´ë¸”(t_test) ì›-í•« ì¸ì½”ë”©ëœ í˜•íƒœì—ì„œ ê°€ì¥ ê°’ì„ ê°–ëŠ” ì¸ë±ìŠ¤ë¥¼ ì¶”ì¶œ. ì‹¤ì œ í´ë˜ìŠ¤ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì •ìˆ˜ê°’ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ê³¼ì •.  
# â€» np.argmax(predicted_value, axis=-1): ì˜ˆì¸¡ëœ ê°’(predicted_value) ì›-í•« ì¸ì½”ë”© í˜•íƒœì—ì„œ ê°€ì¥ í° ê°’ì„ ê°–ëŠ” ì¸ë±ìŠ¤ë¥¼ ì¶”ì¶œ. ì˜ˆì¸¡ í´ë˜ìŠ¤ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì •ìˆ˜ê°’ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ê³¼ì •.
# 
# - í˜¼ë™ í–‰ë ¬ì€ í´ë˜ìŠ¤ ë³„ë¡œ ì‹¤ì œ í´ë˜ìŠ¤ì™€ ì˜ˆì¸¡ í´ë˜ìŠ¤ì˜ ì¡°í•©ì— ë”°ë¼ ì¹´ìš´íŠ¸ëœ ê°’ìœ¼ë¡œ êµ¬ì„±.  
#   ì˜ˆë¥¼ ë“¤ì–´, cm[i][j]ëŠ” ì‹¤ì œë¡œëŠ” í´ë˜ìŠ¤ iì— ì†í•˜ì§€ë§Œ ëª¨ë¸ì´ í´ë˜ìŠ¤ jë¡œ ì˜ˆì¸¡í•œ ë°ì´í„° í¬ì¸íŠ¸ì˜ ê°œìˆ˜ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤.
# 
# sns.heatmap(cm, annot=True, fmt='d')
# 
# cmì„ íˆíŠ¸ë§µìœ¼ë¡œ í‘œì‹œ.  
# - annot=True: ê° ì…€ì— ìˆ«ì ê°’ì„ í‘œì‹œí•˜ë„ë¡ ì§€ì •,  
# - fmt=â€˜dâ€™: ìˆ«ì í˜•ì‹ì„ ì •ìˆ˜ë¡œ ì§€ì •.  
# - íˆíŠ¸ë§µì€ ì…€ì˜ ìƒ‰ìƒì„ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°ì˜ ìƒëŒ€ì ì¸ ê°’ì„ ì‹œê°í™”í•˜ëŠ” ë° ì‚¬ìš©.
# II. Deep Learning
# 
# axis=-1ì€ NumPy ë°°ì—´ì—ì„œ ë§ˆì§€ë§‰ ì¶•(axis)ì„ ë‚˜íƒ€ë‚¸ë‹¤.
# 
# NumPyì˜ ë‹¤ì°¨ì› ë°°ì—´ì€ ì—¬ëŸ¬ ê°œì˜ ì¶•ì„ ê°€ì§ˆ ìˆ˜ ìˆìœ¼ë©°, ê° ì¶•ì€ í•´ë‹¹ ì¶•ì„ ë”°ë¼ ë°°ì—´ì˜ ì°¨ì›ì´ ê²°ì •.  
# ì˜ˆë¥¼ ë“¤ì–´, 3ì°¨ì› ë°°ì—´ì€ ì—´, í–‰, ê¹Šì´ì˜ 3ê°œì˜ ì¶•ì„ ê°€ì§€ë©°,  
# ì´ëŠ” ë°°ì—´ì˜ ì°¨ì›ì— ì˜ì¡´í•˜ë©° í•­ìƒ ë§ˆì§€ë§‰ ì¶•ì„ ì„ íƒí•œë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸.
# 
# ì˜ˆë¥¼ ë“¤ì–´ (3, 4, 5) ëª¨ì–‘ì˜ 3ì°¨ì› ë°°ì—´ì—ì„œ axis=-1ì€ ë§ˆì§€ë§‰ ì¶•ì¸ í¬ê¸° 5ì˜ ì¶•ì„ ì„ íƒ.
# 
# â€» Softmax, argmaxì™€ ê°™ì€ í•¨ìˆ˜ëŠ” ë§ˆì§€ë§‰ ì¶•ì„ ê¸°ì¤€ìœ¼ë¡œ ì—°ì‚°ì„ ìˆ˜í–‰í•˜ëŠ” ê²½ìš°ê°€ ë§ë‹¤.  
# íŠ¹íˆ, ì‹ ê²½ë§ì˜ ì¶œë ¥ ë²¡í„°ëŠ” ë§ˆì§€ë§‰ ì¶•ì„ í´ë˜ìŠ¤ ë ˆì´ë¸”ì´ ìˆëŠ” ì¶•ìœ¼ë¡œ ê°„ì£¼í•˜ê³  ìˆê¸° ë•Œë¬¸ì—,  
# axis=-1ì„ ì‚¬ìš©í•˜ì—¬ ì†ì‰½ê²Œ í´ë˜ìŠ¤ì— ëŒ€í•œ ì—°ì‚°ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆë‹¤.
# 
# 

# %%
predicted_value =np.round(predicted_value)
predicted_value = predicted_value.astype(int)
print(predicted_value)

# %%
print(Cmatrix)
print()

for i in range(10):
  print(('label = %d\t)%d/%d)#taccuracy = %.3f') %
        (i, np.max(Cmatrix[i]), np.sum(Cmatrix[i]),
        np.max(Cmatrix[i])/np.sum(Cmatrix[i])))

# %%
model.save('my_model.keras', include_optimizer=False)

# %%
from PIL import Image
import numpy as n
model=tf.keras.models. load_model('/content/my_model.keras')

# %%
# ì‹¤ì œ ì´ë¯¸ì§€ íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°
img = Image.open('/content/SAM.png').convert('L') # image.pngëŠ” ì‹¤ì œì´ë¯¸ì§€ ê²½ë¡œ
img = Image.open('/content/four.png').convert('L') # image.pngëŠ” ì‹¤ì œì´ë¯¸ì§€ ê²½ë¡œ
img = img.resize((28,28)) # mnist ë°ì´í„°ì™€ ë™ì¼í•œ í¬ê¸°ì¸ 28x28ë¡œ ì´ë¯¸ì§€ í¬ê¸° ì¡°ì •
# ì´ë¯¸ì§€ ë°ì´í„°ë¥¼ ë„˜íŒŒì´ ë°°ì—´ë¡œ ë³€í™˜
input_data = np.array(img)
# ë°ì´í„° ì „ì²˜ë¦¬ (ìƒ‰ìƒ ë°˜ì „ ë° ì •ê·œí™”)
input_data = (255 - input_data)/ 255.0

# ë°°ì¹˜ ì°¨ì› ì¶”ê°€
input_data = np.expand_dims(input_data, axis=0)

# %% [markdown]
# PIL(Python Imaging Library)ì—ì„œ ì´ë¯¸ì§€ë¥¼ ì²˜ë¦¬í•  ë•Œ 'L'ì€ "luminance"(íœ˜ë„), ì´ë¯¸ì§€ë¥¼ í‘ë°±(ê·¸ë ˆì´ìŠ¤ì¼€ì¼)ìœ¼ë¡œ ë³€í™˜
# input_data = (255 - input_data)/255.0
# ë¹„ì£¼ì–¼ í‘œí˜„ ë³€ê²½: ì›ë˜ì˜ í‘ë°± ì´ë¯¸ì§€ì—ì„œ, í°ìƒ‰ì€ ë†’ì€ í”½ì…€ ê°’(255ì— ê°€ê¹Œì›€)ì„ ê°€ì§€ë©°, ê²€ì€ìƒ‰ì€
# ë‚®ì€ í”½ì…€ ê°’(0ì— ê°€ê¹Œì›€)ì„ ê°–ëŠ”. (255 - input_data) ì—°ì‚°ì„ ì‚¬ìš©í•˜ë©´ ìˆœì„œê°€ ë’¤ë°”ë€Œì–´, í°ìƒ‰ì´ ë‚®ì€
# ê°’(0ì— ê°€ê¹Œì›€)ì„, ê²€ì€ìƒ‰ì´ ë†’ì€ ê°’(255ì— ê°€ê¹Œì›€)ì„ ê°€ì§€ê²Œ ë¨.
# input_data = np.expand_dims(input_data, axis=0):
# ë°°ì—´ì— ì¶”ê°€ì ì¸ ì°¨ì›ì„ ì¶”ê°€
# ì›ë³¸ input_data ë°°ì—´ì˜ shapeì´ (28, 28)ì´ì—ˆë‹¤ë©´, ì´ ì½”ë“œë¥¼ ì‹¤í–‰í•œ í›„ì—ëŠ” shapeì´ (1, 28, 28)ë¡œ ë³€í™˜
# ëŒ€ë¶€ë¶„ì˜ ë”¥ ëŸ¬ë‹ ëª¨ë¸ì´ ì—¬ëŸ¬ ì´ë¯¸ì§€ë¥¼ í•œ ë²ˆì— ì²˜ë¦¬í•˜ë„ë¡ ì„¤ê³„ë˜ì–´ ìˆì–´ì„œ, ë‹¨ì¼ ì´ë¯¸ì§€ë¥¼
# ì…ë ¥í•˜ë”ë¼ë„ ê·¸ ì´ë¯¸ì§€ê°€ 3ì°¨ì› ë°°ì—´ë¡œ ì œê³µë˜ì–´ì•¼ í•œë‹¤. ì´ ê²½ìš° ì²« ë²ˆì§¸ ì°¨ì›ì€ 'ë°°ì¹˜' ì°¨ì›ìœ¼ë¡œ,
# ì²˜ë¦¬í•  ì´ë¯¸ì§€ì˜ ìˆ˜ë¥¼ ë‚˜íƒ€ëƒ„.

# %%
prediction = model.predict(input_data)
print('Predicted digit:',np.argmax(prediction))

# %%
Q_train = Q_train /255.0
Q_test=Q_test/255.0

# cnn ì…ë ¥ìš© reshape
Q_train = Q_train.reshape(-1,28,28,1) # cnnìœ¼ë¡œ ë³€ê²½
Q_test= Q_test.reshape(-1,28,28,1) # cnnìœ¼ë¡œ ë³€ê²½

# ì›-í•« ì¸ì½”ë”©
A_train = tf.keras.utils.to_categorical(A_train,num_classes = 10)
A_test = tf.keras.utils.to_categorical(A_test,num_classes=10)

# model config
model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Conv2D(32,(3,3),activation='relu',input_shape=(28,28,1))) # CNNìœ¼ë¡œë³€ê²½
model.add(tf.keras.layers.MaxPooling2D(2,2))

model.add(tf.keras.layers.Conv2D(64,(3,3),activation='relu'))
model.add(tf.keras.layers.MaxPooling2D((2,2)))
model.add(tf.keras.layers.Flatten())
model.add(tf.keras.layers.Dense(64,activation='relu'))
model.add(tf.keras.layers.Dense(10,activation='relu'))

model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

# %% [markdown]
# ë°°ì¹˜ ë…¸ë©€ë¼ì´ì¬ì´ì…˜ ì¶”ê°€:ê° ì¸µ ì‚¬ì´ì— ë°°ì¹˜ ì •ê·œí™”ë¥¼ ì¶”ê°€í•˜ì—¬ í•™ìŠµ ì•ˆì •ì„±ì„ ë†’ì´ê³ , ë”
# ê¹Šì€ ë„¤íŠ¸ì›Œí¬ êµ¬ì¡°ê°€ ê°€ëŠ¥í•˜ë„ë¡ í•©ë‹ˆë‹¤.
# ë“œëì•„ì›ƒ ì¶”ê°€: ê³¼ëŒ€ì í•©ì„ ë°©ì§€í•˜ê¸° ìœ„í•´ ë“œë¡­ì•„ì›ƒì„ ì¶”ê°€í•©ë‹ˆë‹¤.
# ë´ìŠ¤ ë ˆì´ì–´ ê°œìˆ˜ ë° ë…¸ë“œ ìˆ˜ ì¡°ì •: Dense ì¸µì˜ ê°œìˆ˜ ë° í¬ê¸°ë¥¼ ì¡°ì •í•˜ì—¬ ëª¨ë¸ì˜ ë³µì¡ì„±ì„
# ì¤„ì´ê³ , ë” ì ì ˆí•œ í”¼ì²˜ í‘œí˜„ì„ í•™ìŠµí•˜ë„ë¡ í•©ë‹ˆë‹¤.
# ë°ì´í„° ì¦ê°•:ì´ë¯¸ì§€ë¥¼ íšŒì „, ì´ë™, í™•ëŒ€ ë“±ì˜ ë°©ì‹ìœ¼ë¡œ ë°ì´í„°ì…‹ì„
# ì¦ê°•í•˜ì—¬ ëª¨ë¸ì´ ë‹¤ì–‘í•œ íŒ¨í„´ì„ í•™ìŠµí•˜ë„ë¡ ìœ ë„í•©ë‹ˆë‹¤.
# ì–¼ë¦¬ ìŠ¤íƒ‘í•‘:ê³¼ëŒ€ì í•©ì„ ë°©ì§€í•˜ê¸° ìœ„í•´ early stoppingì„ ì‚¬ìš©í•˜ì—¬ ë” ì´ìƒ ì„±ëŠ¥ì´ ê°œì„ ë˜ì§€
# ì•Šìœ¼ë©´ í•™ìŠµì„ ì¤‘ë‹¨í•˜ë„ë¡ í•©ë‹ˆë‹¤

# %% [markdown]
# ì•„í‚¤í…ì²˜ ì¡°ì •:ë‹¤ì–‘í•œ ëª¨ë¸ êµ¬ì¡° ì‹¤í—˜: VGG, ResNetê³¼ ê°™ì€ ë³´ë‹¤ ê¹Šì€ ë„¤íŠ¸ì›Œí¬ ì•„í‚¤í…ì²˜ë¥¼ ì‚¬ìš©í•´ ë³¼ ìˆ˜
# ìˆìŠµë‹ˆë‹¤.
# ì „ì´ í•™ìŠµ:ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ì„ ì‚¬ìš©í•˜ê³ , ë§ˆì§€ë§‰ ë ˆì´ì–´ë§Œ ì¡°ì •í•˜ëŠ” ë°©ë²•ì„
# ê³ ë ¤í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
# í•˜ì´í¼ íŒŒë¼ë¯¸í„° íŠœíŒ…í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§: í•™ìŠµ ì¤‘ì— í•™ìŠµë¥ ì„ ì¡°ì •í•˜ì—¬ ë” ë‚˜ì€ ìˆ˜ë ´ì„ ìœ ë„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
# ë‹¤ì–‘í•œ ì†ì‹¤ í•¨ìˆ˜ ì‹œë„MNISTëŠ” ë¶„ë¥˜ ë¬¸ì œì´ë¯€ë¡œ, ë‹¤ë¥¸ ì†ì‹¤ í•¨ìˆ˜(ì˜ˆ: Focal Loss)ë¥¼ ì‹œë„í•´ë³´ëŠ” ê²ƒë„ ì¢‹ì€
# ë°©ë²•ì…ë‹ˆë‹¤. ì´ëŠ” í´ë˜ìŠ¤ ë¶ˆê· í˜• ë¬¸ì œë¥¼ ì™„í™”í•˜ëŠ” ë° ë„ì›€ì´ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
# ì •ê·œí™” ê¸°ë²•:L2 ì •ê·œí™”ì™€ ê°™ì€ ì •ê·œí™” ê¸°ë²•ì„ ë„ì…í•˜ì—¬ ëª¨ë¸ì˜ ì¼ë°˜í™” ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
# ì•™ìƒë¸” ë°©ë²•:ì—¬ëŸ¬ ëª¨ë¸ì„ í›ˆë ¨ì‹œì¼œ ì´ë“¤ì˜ ì˜ˆì¸¡ì„ ê²°í•©í•˜ì—¬ ìµœì¢… ì˜ˆì¸¡ì„ ê°œì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

# %%
# GPU ì‚¬ìš© ì—¬ë¶€ í™•ì¸
import torch
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f'using device : {device}')

# %%
import torch.nn as nn
import torch.optim as optim
import torchvision
from torchvision import datasets, transforms
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt


# %%
# ë°ì´í„° ì „ì²˜ë¦¬
transfrom = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,),(0.5,))])

train_dataset = datasets.MNIST(root='./data', train=True,download=True,transform=transfrom)
test_dataset = datasets.MNIST(root='./data',train=False,download=True,transform=transfrom)

train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)
test_loader = DataLoader(dataset=test_dataset,batch_size=64,shuffle=False)

# %%
# CNN ëª¨ë¸ ì •ì˜
class CNN(nn.Module):
  def __init__(self):
    super(CNN,self).__init__()
    self.conv1 = nn.Conv2d(1,32,kernel_size = 3)
    self.conv2 = nn.Conv2d(32, 64, kernel_size=3)
    self.pool = nn.MaxPool2d(2,2)
    self.fc1 = nn.Linear(64*5*5,128)
    self.fc2 = nn.Linear(128,10)
  def forward(self,x):
    x = self.pool(torch.relu(self.conv1(x)))
    x = self.pool(torch.relu(self.conv2(x)))
    x = x.view(x.size(0),-1)
    x = torch.relu(self.fc1(x))
    x = self.fc2(x)
    return x

# %%
# model ì†ì‹¤ í•¨ìˆ˜ ë° ì˜µí‹°ë§ˆì´ì € ì •ì˜
model = CNN().to(device) # ëª¨ë¸ì„ ë””ë°”ì´ìŠ¤ ì •ì˜ì— ë§ê²Œ ì‹¤í–‰
loss_fn = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001) # lr=1-e3

# %%
# í•™ìŠµ í•¨ìˆ˜
def train(model, train_loader, optimizerm, loss_fn, device):
  model.train()
  running_loss = 0.0
  correct = 0
  total = 0
  for images, labels in train_loader:
    images, labels = images.to(device),labels.to(device)

    # Forward pass
    outputs = model(images)
    loss = loss_fn(outputs, labels)
    # Backward pass and optimization
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    # Loss ë° Accuracy ê³„ì‚°
    running_loss  += loss.item()
    _,predicted = torch.max(outputs,1)
    correct += (predicted == labels).sum().item()
    total += labels.size(0)
  avg_loss = running_loss / len(train_loader)
  accuracy = correct/ total * 100
  return avg_loss,accuracy

# %%
# 5. í‰ê°€ í•¨ìˆ˜
def evaluate(model, test_loader, loss_fn, device):
    model.eval()
    test_loss = 0.0
    correct = 0
    total = 0

    with torch.no_grad():
        for images, labels in test_loader:
            images, labels = images.to(device), labels.to(device)

            outputs = model(images)
            loss = loss_fn(outputs, labels)
            test_loss += loss.item()

            _, predicted = torch.max(outputs, 1)
            correct += (predicted == labels).sum().item()
            total += labels.size(0)

    avg_loss = test_loss / len(test_loader)
    accuracy = correct / total * 100

    return avg_loss, accuracy

# %%
# í•™ìŠµ ë° í‰ê°€
num_epochs = 30
train_losses, train_accuracies = [],[]
test_losses, test_accuracies = [], []

best_test_acc = 0.0 # ê°€ì¥ ì¢‹ì€ í…ŒìŠ¤íŠ¸ ì •í™•ë„ë¥¼ ê¸°ë¡í•˜ê¸° ìœ„í•œ ë³€ìˆ˜(ì´ ë¶€ë¶„ì„ í•™ìŠµ ë£¨í”„ ì™¸ë¶€ì— ì •ì˜)

for epoch in range(num_epochs):
    train_loss, train_acc = train(model,train_loader, optimizer, loss_fn,device)
    test_loss, test_acc = evaluate(model,test_loader,loss_fn,device)

    train_losses.append(train_loss)
    train_accuracies.append(train_acc)
    test_losses.append(test_loss)
    test_accuracies.append(test_acc)

    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%,'
          f'Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%')

    if test_acc > best_test_acc:
      best_test_acc = test_acc
      torch.save(model.state_dict(), f'best_model_epoch_{epoch+1}.pth') # ëª¨ë¸ ì €ì¥
      print(f'Best model saved with Test Accuracy: {test_acc:.2f}% at Epoch {epoch+1}')

# %%
plt.figure(figsize=(12,5))
plt.subplot(1,2,1)
plt.plot(train_losses,label='Train Loss')
plt.plot(test_losses, label='Test Loss')
plt.title('Loss')
plt.legend()
plt.subplot(1,2,2)
plt.plot(train_accuracies,label='Train Accuracy')
plt.plot(test_accuracies,label='Accuracy')
plt.title('Accuracy')
plt.legend()

plt.show()

# %%
import torch
from torchvision import transforms
from PIL import Image
import numpy as np

# ìƒˆë¡œìš´ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
model = CNN()

# ì €ì¥ëœ ëª¨ë¸ ê°€ì¤‘ì¹˜ë§Œ ë¡œë”©
# ì €ì¥ëœ ëª¨ë¸ íŒŒì¼ ê²½ë¡œ ì ì ˆíˆ ìˆ˜ì •
model.load_state_dict(torch.load('best_model_epoch_30.pth'))  # weights_only ì œê±°
model.eval()

# ì´ë¯¸ì§€ ì „ì²˜ë¦¬ í•¨ìˆ˜(ë°°ê²½ì´ í°ìƒ‰ì´ë©´ ë°˜ì „ í•„ìš”)
def preprocess_image(image_path):
  # ì´ë¯¸ì§€ ë¡œë“œ ë° í‘ë°±ìœ¼ë¡œ ë³€í™˜
    image = Image.open(image_path).convert('L')  # í‘ë°±ìœ¼ë¡œ ë³€í™˜ Lì€ ê·¸ë ˆì´ ìŠ¤ì¼€ì¼ ëª¨ë“œ

    # ìƒ‰ìƒ ë°˜ì „ (í° ë°°ê²½ â†’ ê²€ì • ë°°ê²½)
    image = np.array(image)
    image = Image.fromarray(255 - image)

    # ì „ì²˜ë¦¬: ë¦¬ì‚¬ì´ì¦ˆ, í…ì„œ ë³€í™˜, ì •ê·œí™” (28x28)
    transform = transforms.Compose([
        transforms.Resize((28, 28)),
        transforms.ToTensor(),
        transforms.Normalize((0.5,), (0.5,)) # MNISTì™€ ë¹„ìŠ·í•˜ê²Œ ì •ê·œí™”
    ])

    # ì´ë¯¸ì§€ í…ì„œ ë³€í™˜ í›„ ë°°ì¹˜ ì°¨ì› ì¶”ê°€(1,28,28)
    image = transform(image).unsqueeze(0)  # ì˜¤íƒ€ ìˆ˜ì •: unsqieeze â†’ unsqueeze
    return image


# %%
image_path = '/content/SAM.png'
# image_path = '/content/four.png'

image = preprocess_image(image_path)
with torch.no_grad():
  output = model(image)
  _,predicted = torch.max(output,1)
  print(f'ëª¨ë¸ì´ ì˜ˆì¸¡í•œ ìˆ«ì: {predicted.item()}')

plt.imshow(Image.open(image_path).convert('L'), cmap='gray')
plt.title(f'predicted number of model : {predicted.item()}')
plt.show()


