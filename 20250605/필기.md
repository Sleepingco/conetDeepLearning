Pytorch 특징
직관적아다
학습 및 디버깅이 쉽다.
동적 계산 그래프
실행하면서 바로 결과 확인 가능

모듈 구조
기능 영역 PyTorch TensorFlow Keras
텐서 연산 torch torch.Tensor tf.Tensor tf.Tensor 사용
신경망 모듈 torch.nn tf.keras.layers tf.nn keras.layers
최적화 torch.optim tf.optimizers
데이터 처리 torch.utils.data tf.data.Dataset
keras.optimizers
keras.preprocessing
자동 미분 torch.autograd tf.GradientTape() TensorFlow 기반
모델 배포 torch.jit (TorchScript) tf.SavedModel TensorFlow 방식 사용

nn : 뉴럴 네트워크 (모듈, 함수)

autograd
자동 미분 기능 신경망 학습에 필수적인 역전파 자동 수행

nn
신경망 구성 요소 제공(Layer,Loss Function 등)
모듈:모델 레이어의 뻐대 사용자 정의 모델 클래스 상속
함수: 함수형 연산 제공 layer 기반 아님 F.relu(),F.cross_entorpy()

optim 
학습 최적화 알고리즘 제공
가중치 업데이트를 자동으로 수행

utils
데이터 가공과 학습용 데이터 배치 처리 제공

torchvision
컴퓨터 비전 특화
유틸리티 패키지
이미지 데이터셋, 전처리, Pretrained 모델 제공

토치는 nn과 optim이 가장 자주 씀

nn.Module 클래스 상속 기반
nn.Functional 함수 기반 접근법

nn.Module와 nn.functional을 나눈 이유
1) 구조(Architecture)와 연산(Operation)의 역할 분리
nn.Module ->모델구조(Architecture)
Layer의 파라미터 (가중치, 편향) 을 자동으로 포함하고
파라미터 저장, 불러오기, GPU 이동 등 모델 관리에 최적화
전체 모델을 객체지향적으로 클래스화할 수 있도록 설계
nn.functional ->개별 연산 기능 제공
Layer나 파라미터 관리 기능 없이, 단순히 수학적 연산만 수행
중간 레이어 연산, 활성화 함수, 손실 함수 등을 함수로 바로 사용 가능
파라미터를 직접 정의하거나 실험적으로 Layer의 계산 방식을 세부 조작할 때 사용
1) 유연성 확보
복잡한 모델을 만들 때:
nn.Module → 전체 구조, Layer 파라미터, 저장 및 불러오기 용도
nn.functional → forward 과정에서 세부 연산을 커스터마이즈할 때 사용

구분 nn.Module nn.functional
용도 모델 전체 구조 및 파라미터 관리 수학적 연산 수행, 세부 조정
파라미터 관리 ○ (자동 포함) X (직접 정의 필요)
사용 예 모델 정의, 저장, 불러오기, 학습 forward 과정에서 커스터마이즈, activation, loss
장점 Layer 구조화, 학습 파라미터 자동 관리 코드 유연성, 수식 직접 컨트롤 가능

nn.Linear,QKV:임의의 가중치 QKV w값을 넣어서 점점 변하면서 선형 변환으로 QKV가 새로 생성됨 올바른 정답이 나올때까지 계속 학습함 한번 내적하고 끝나는게 아니라 역전파를 통해 w값을 변환해가며 최적의 값을 찾아감 선형 변환을 통해서 가중치 QKV를 만들어 지고 이러한 가중치를 통해 학습을 한다

nn.Module 주요 메서드
__init__() 모델의 레이어 구조 정의
forward() 입력 데이터 → 출력 데이터 연산 정의.
parameters() 모델 내의 학습 가능한 파라미터 반환.
state_dict() 모델의 모든 가중치와 편향 값 반환.
eval() 추론 모드로 전환 (Dropout, BatchNorm 비활성화).
train() 학습 모드로 전환.
zero_grad() 모델 내 기울기 초기화

nn.functional 메서드
F.relu() ReLU 활성화 함수
F.sigmoid() 시그모이드 함수
F.softmax() 소프트맥스 함수
F.linear() Linear 연산 (직접 가중치, 편향 전달)
F.conv2d() Convolution 연산 (파라미터 수동 관리)
F.cross_entropy() 분류용 손실 함수
F.mse_loss() 회귀용 손실 함수

실제 코드에서는 둘을 혼용한다.
이유 1. 가중치가 필요한 Layer는 nn.Module 사용
self.fc = nn.Linear(10,1)
→ fc Layer는 학습 가능한 파라미터(가중치, 편향)가 있기 때문에
nn.Module을 통해 모델 클래스에 등록해야 합니다.
parameters()로 파라미터를 가져와 학습 가능.
state_dict()로 저장/불러오기 가능

이유 2. 가중치 없는 연산은 nn.functional 사용
x=F.relu(x)
→ ReLU 활성화 함수는 파라미터가 필요 없는 연산
따라서 굳이 nn.Module로 정의할 필요가 없음
F.relu() 함수로 처리하는 게 더 직관적이고 가벼움

Tensorflow/Keras는 먼저 Model(계산 그래프)을 생성 한 후에 tensor입력을 통해서Model을 학습시키는 정적(Static) 계산 그래프 방식
먼저 모델 전체 구조(계산그래프)를 '설계도' 처럼 만들어 놓고 그 다음에 
데이터(tensor)를 넣어서 학습하는 방식
즉, 계산하르 흐름을 '미리' 만들어 놓고 실행한다

Pytorch는 Forward Pass시에 Tensor들의 연산을 추적하여 자동으로 계산 그래프를 생성하는 동적(Dynamic) 계산 그래프 방식
데이터가 실제로 흐를 때 (Forward Pass시에) 그때그때 Tensor들의 연산을 자동으로 추적하면서 계산 그래프를 "실시간"으로 만들어줌.
즉, 코드 실행 순간에 연산 흐름이 만들어지고 계산됩니다.



---

### 🔹 제목 1

**Numpy ndarray vs PyTorch Tensor 비교**

| 항목           | NumPy `ndarray`    | PyTorch `Tensor`                           |
| ------------ | ------------------ | ------------------------------------------ |
| 기본 연산        | 다차원 배열 기본 연산 지원    | 다차원 배열 + 텐서의 연산 지원                         |
| 미분(gradient) | ❌ 지원 안 함           | ✅ `autograd`로 자동 미분 가능                     |
| GPU 사용       | ❌ CPU 연산만 지원       | ✅ GPU 사용 가능 (`.to("cuda")`)                |
| 딥러닝 연산       | ❌ 직접 구현해야 함        | ✅ `nn`, `optim`, `loss`, `backward()` 등 포함 |
| 프레임워크        | 과학 계산, 일반 수치 계산 목적 | 딥러닝과 머신러닝 최적화                              |
| 라이브러리        | `numpy`            | `torch`                                    |

---

### 🔹 제목 2

**Deep Learning Pytorch**
**Numpy와 PyTorch의 dtype은 별도로 정의됨**

| 개념             | NumPy 자료형    | PyTorch 자료형                           |
| -------------- | ------------ | ------------------------------------- |
| 정수형 (32비트)     | `np.int32`   | `torch.int32`                         |
| 정수형 (64비트)     | `np.int64`   | `torch.int64` (PyTorch의 기본 `int`)     |
| 부동소수점 (32비트)   | `np.float32` | `torch.float32` (PyTorch의 기본 `float`) |
| 부동소수점 (64비트)   | `np.float64` | `torch.float64`                       |
| 부호없는 정수 (64비트) | `np.uint64`  | 없음                                    |
| 부호없는 정수 (16비트) | `np.uint16`  | 없음                                    |
| 불리언            | `np.bool`    | `torch.bool`                          |

---

### 🔹 제목 3

**Deep Learning Pytorch**
**`rand()` vs `randn()` : 0과 1 사이 샘플링 vs 표준 정규분포 샘플링**

| 항목    | `torch.rand()`                      | `torch.randn()`                            |
| ----- | ----------------------------------- | ------------------------------------------ |
| 의미    | 균등분포에서 샘플링 (Uniform Distribution)   | 정규분포에서 샘플링 (Normal Distribution)           |
| 분포 범위 | 0과 1 사이의 값 샘플링                      | 평균 0, 표준편차 1의 정규분포 값 샘플링                   |
| 분포 형태 | 균일한 분포                              | 종 모양 (좌우대칭) 형태의 분포                         |
| 예시    | `torch.rand(3)` → 0과 1 사이 실수 3개 샘플링 | `torch.randn(3)` → 평균 0, 표준편차 1의 실수 3개 샘플링 |

---



항목 torch.rand() torch.randn() : 0과 1 사이 샘플링 vs 표준 정규분포 샘플링
의미 균등 분포에서 샘플링 (Uniform Distribution) 정규 분포에서 샘플링 (Normal Distribution)
분포 범위 [0.0, 1.0) 사이의 값 평균 0, 표준편차 1의 정규분포 (가우시안)
결과 형태 랜덤한 양수들 (0~1 사이) 음수, 양수 포함 (평균 0 중심으로 퍼짐)
주 용도 가중치 초기화, 마스킹, 드롭아웃 등 노이즈 추가, 초기 가중치 정규 분포 기반 생성

• np.random.rand(3, 2)
• 분포: 0 이상 1 미만의 균등 분포
• 형태: shape 인자를 받아 해당 형태의 배열 생성
• 예시 결과:
array([[0.123, 0.753],
[0.913, 0.230],
[0.601, 0.832]])
• 용도: 정규화된 랜덤 입력, 랜덤 초기화

• np.random.randn(3, 2)
• 분포: 평균 0, 표준편차 1의 정규분포 (가우시안 분포)
• 형태: shape 인자를 받아 해당 형태의 배열 생성
• 예시 결과: 3 * 2 의 배열을 생성 !!
array([[ 1.234, -0.987],
 [ 0.112, 0.332],
 [-0.761, 0.521]])

• 용도: 신경망 weight 초기화, 잡음 시뮬레이션

• np.random.randint(low=10, high=20, size=(3, 2))
• 분포: [low, high) 범위의 정수
• 난수 (low 포함, high 제외)
• 형태: size 인자로 배열 크기 지정
• 예시 결과 : array([[14, 12],
[19, 10],
[16, 11]])
• 용도: 랜덤 정수 생성, 카테고리 선택, 샘플링

tensor concatenate
두 개 이상의 텐서를 특정 축(차원) 기준으로 이어붙이는 연산
텐서의 shape가 연결하려는 차원 외에는 같아야 함


transforms.ToTensor()
이미지 데이터를 PyTorch에서 사용할 수 있도록,
[0, 255] 범위의 PIL.Image 또는 numpy.ndarray 데이터를
[0.0, 1.0] 범위의 torch.FloatTensor로 변환하는 함수
MNIST, CIFAR 등 PIL 이미지 또는 NumPy 이미지 데이터를 텐서(Tensor)로 변환

pytorch에서 -1은 차원 자동 추론(auto-infer)
reshape 또는 view 시 나머지 차원을 자동으로 계산해달라는 뜻으로 자주 사용
1. view() 또는 reshape()에서의 -1
고정된 차원 하나를 기준으로 나머지 차원을 자동 계산해줌
전체 원소 수는 변하지 않음
1: 2D 이미지 데이터를 Flatten
x = torch.randn(64, 1, 28, 28) # (배치 64, 채널 1, 높이 28, 너비 28)
x_flat = x.view(64, -1) # → (64, 784)-
1은 자동으로 1×28×28 = 784가 계산되어 대입,
주로 Fully Connected layer 앞에서 사용됨
2. 역변환(Flatten 해제)
x = torch.randn(64, 784)
x_img = x.view(64, 1, 28, 28) # 정확한 차원 명시

3. 텐서 연산에서 -1이 쓰이는 또 다른곳
예시 3: permute()와 함께 사용
x = torch.randn(64, 3, 32, 32)
x = x.permute(0, 2, 3, 1).contiguous().view(-1, 3) # HWC 배열 flatten 예시
마지막에 view(-1, 3)은 남은 모든 원소를 행 방향으로 펼치되, 3채널을 유지
주의할 점-1은 한 번만 사용할 수 있습니다.
 두 개 이상 쓰면 어떤 차원을 추론할지 알 수 없기 때문입니다.
# 오류 발생torch.randn(2, 3, 4).view(-1, -1)
# → RuntimeError: only one dimension can be inferred

Pytorch Channel First(CHW) 구조
PyTorch는 low-level C/CUDA 연산에 최적화된 채널 우선 구조(CHW)를 따름
Conv2d, BatchNorm2d 등 PyTorch의 대부분 연산이 이 구조를 전제로 설계됨

NumPy -> Tensor 전환 시 차이
permute(2, 0, 1)는 PyTorch 텐서 차원 순서 변경 함수
permute(2, 0, 1)는 축 순서를 (채널, 높이, 너비)로 재정렬,
numpy 배열의 차원 (0, 1, 2) → torch.Tensor 차원 (2, 0, 1)로 바꿉니다.

주의할점
transforms.ToTensor()는 자동으로 HWC → CHW로 변환
matplotlib.pyplot.imshow()는 HWC를 기대하므로 다시 .permute(1, 2, 0) 필요

torch.cat()
torch.cat(tensor_list, dim)
• tensor_list 이어붙일 텐서들의 리스트 (예: [a, b, c])
• dim 어떤 축(차원)을 기준으로 붙일지 지정 (예: dim=0)

torch.cat((cls_tokens,x),dim=1)
x: [B, 49, n_hidden] → 패치 시퀀스
cls_tokens: [B, 1, n_hidden] → 각 샘플에 1개씩dim=1로 붙이면
→ [B, 50, n_hidden]
 전체 패치 시퀀스 앞에 [CLS] 토큰이 추가된 구조로 바뀝니다.
torch.cat()은 두 개 이상의 텐서를 지정된 축(dim) 방향으로 이어붙이는
함수.이어붙이는 축을 제외한 다른 차원은 반드시 모두 동일해야 합니다.

cat([A,B],dim=1)
A.shape = [128, 1, 8]
.shape = [128, 49, 8]
→ dim=1에서 두 번째 차원을 이어붙이고,
→ 나머지 차원은 일치 여부만 확인.

torch.transpose()
차원을 바꾸는 함수

역할:지정된 두 차원 (dim1, dim2)의 위치를 서로 교환합니다.
텐서의 shape만 바뀌고, 메모리 순서(stride)는 그대로입니다.
tensor.transpose(dim0, dim1)
• dim0 바꿀 첫 번째 차원의 인덱스
• dim1 바꿀 두 번째 차원의 인덱스
x = torch.randn(5, 8, 100, 64) # [B, H, T, D]
# 4차원 텐서를 생성합니다.
# 의미 있는 차원 구조
# 예시:B = 5: 배치 크기 (문장 수)
# H = 8: 헤드 수 (multi-head attention)
# T = 100: 시퀀스 길이 (토큰 수)
# D = 64: 각 헤드의 임베딩 차원
x_t = x.transpose(1, 2) # [B, T, H, D]
# 두번째 차원(H)과 세 번째 차원(T)**을 교환합니다.
이때 x_t는 shape은 변경되었지만,
메모리는 비연속(non-contiguous) 상태입니다.

torch.contiguous()
비연속(non-contiguous) 텐서를 메모리 상에서 연속된(contiguous) 텐서로 복사해서 반환
.view() 같은 메모리 기반 연산을 수행하려면 필수입니다.
transpose() 후 .view()를 쓸 경우 필요
x_c = x_t.contiguous()
x_c = x_t.contiguous().view(batch_size, -1, d_model)
transpose(), permute() 등을 사용하면 텐서의 shape은 바뀌지만,실제로 메모리에서는
비연속적인 보기(view)로 존재.이런 경우:.view() 를 쓰면 오류 발생 가능.
contiguous().view(...) 로 처리 필요

torch.matmul() a @ b : torch.matmul(a, b)와 동일
다차원 텐서의 행렬 곱(matrix multiplication)을 수행하는 함수
torch.matmul(tensor1, tensor2)
2개의 텐서를 곱하며, 내부 차원이 맞아야 합니다.
a = torch.randn(2, 3)
b = torch.randn(3, 4)
c = torch.matmul(a, b) # 결과: [2, 4]
a = torch.randn(5, 2, 3) # 5개의 2×3 행렬
b = torch.randn(5, 3, 4) # 5개의 3×4 행렬
c = torch.matmul(a, b) # 결과: [5, 2, 4]
Q: [B, H, T_q, d_k]
K: [B, H, T_k, d_k]
→ K^T: [B, H, d_k, T_k] # transpose
→ torch.matmul(Q, K^T): [B, H, T_q, T_k]

torch.view()
텐서의 형태(Shape)를 변경(reshape)할 때 사용하는 함수
기본적으로 데이터는 그대로 두고, 모양만 바꾸는 연산,
메모리 상 연속적(contiguous)일 때만 사용 가능
비연속 메모리의 경우 .contiguous()가 필요합니다.
NumPy의 reshape()과 매우 유사

torch.unsqueeze()
차원을 하나 늘릴 때 사용하는 함수
 배치 차원 추가, 브로드캐스팅 준비 등에 매우 자주 사용
 torch.unsqueeze(input,dim)
 # 또는
 input.unsqueeze(dim)
 • input 원본 텐서
• dim 추가할 차원의 위치 (인덱스)
unsqueeze()는 원소 수를 바꾸지 않음→ 단순히
shape만 확장제거할 땐 squeeze() 사용
x = torch.tensor([1, 2, 3]) # shape: [3]
y = x.unsqueeze(0) # shape: [1, 3]
z = x.unsqueeze(1) # shape: [3, 1]
img = torch.randn(1, 28, 28) # shape: [C, H, W]
img_batched = img.unsqueeze(0) # shape: [1, 1, 28, 28]

torch.zeroes()
모든 원소가 0인 텐서를 생성할 때 사용하는 기본 함수
모델 초기화, 더미 텐서 생성, 임시 결과 저장 등에 자주 사용.
torch.zeros(size, dtype=torch.float32,device=None)
• size 텐서의 모양 (shape) 튜플 또는 여러 인자
• dtype 자료형 (기본: float32)
• deviceCPU 또는 GPU 지정 가능

torch.arange()
Numpy의 np.arange()와 동일하게,
일정 범위의 정수 또는 실수 값을 가지는 1차원 텐서를 생성하는 함수
torch.arange(start=0,end,step=1,dtype=None,device=None)
• start 시작값 (기본 0)
• end 종료값 (미포함!)
• step 증가 간격 (기본 1)
• dtype 자료형 (예: torch.float32, torch.int64)
• deviceCPU 또는 CUDA 설정 가능

torch.hstacj()
여러 텐서를 수평 방향(열 방향)으로 붙이는 함수
차원 1 (dim=1) 기준으로 torch.cat()을 수행한 것과 동일.
torch.hstack(tensors)
• tensors 붙이고자 하는 텐서들의 리스트 또는 튜플

torch.Tensor.masked_fill()
조건에 해당하는 위치에 특정 값을 채워 넣는 함수
특히 어텐션 마스킹, 패딩 마스킹, loss 마스킹 등에 자주 사용
tensor.masked_fill(mask, value)
• mask bool 타입의 마스크 텐서 (True인 위치에 값이 채워짐)
• value 채워 넣을 값 (예: -1e9, 0, float('-inf'))





1. 토큰화(Tokenization)

문장을 단어, 문자, subword 등의 작은 단위로 나누는 과정
각 단위는 "토큰"이라고 부르며, 단어, 부분 단어, 또는 문장의 한 글자일 수 있습니다
예: "I love NLP" → ["I", "love", "NLP"]
토크나이징을 통해 문장의 구성 요소를 분석하여 컴퓨터가 텍스트 데이터를 처리할
수 있는 형태로 변환할 수 있다.

- 단어:기반 토크나이징: "I love NLP"라는 문장은 ["I", "love", "NLP"]로
- 부분 단어(Subwrod) 토크나이징: "playing"이라는 단어를 ["play", "##ing"]처럼.
- BPE(Bypte-Pair-Encodoing): 텍스트를 자주 나타나는 문자 쌍(char pair)을
반복적으로 병합하여 서브워드 단위로 분해하는 방식
 BERT나 GPT-2 같은 모델에서 사용
- 문자(Char) 기반 토크나이징: "hello"는 ["h", "e", "l", "l", "o"].

BPE
초기에는 모든 단어를 문자(character) 단위로 분리
말뭉치(corpus)"low", "lower", "newest", "widest“
Step 1: 문자 단위로 시작(lower를 가정시)
Step 2: 가장 많이 나타나는 문자 쌍을 찾아 병합
예를 들어, e 와 s가 자주 같이 나타나면 es로 병합.
Step 3: 병합 규칙을 사전(Vocabulary)으로 저장
이 규칙들을 토큰화할 때 사용하면, 자주 등장하는 단어는 하나의 토큰으로 처리되고,
처음 보는 단어는 서브워드 단위로 분해됩니다

신조어나 희귀어 처리에 강함 ( oov 문제 완화)
• 모델의 vocabulary 수를 줄이면서도 일반화 성능 유지
• 다국어/ 언어에 비중립적으로 사용 가능 (Unicode 기반 처리도 가능)
가장 많이 등장한 쌍(5+2=7)은 l o
lo를 하나의 서브워드로 병합
업데이트된 vocabulary에 lo 항목이 추가됨

2. 정수 인코딩(Integer Encoding)
텍스트 토큰을 숫자로 매핑하는 과정 (ex: 단어 → 고유 ID)
어휘 사전 (Vocabulary)말뭉치에서 사용되는 모든 토큰의 집합 (고유 단어 목록)
"hello world" → ["hello", "world"]
→ [1, 2] (Vocabulary 기준 매핑)
단순한 정수 인코딩은 의미 간 거리 정보가 없기 때문에,
모델이 단어 간 관계를 제대로 학습하기 어렵습니다.
그래서 임베딩 레이어나 원-핫 인코딩, 서브워드 인코딩이 추가로 사용됩니다

3. 어휘 사전(Vocabulary)
모델이 학습 중 혹은 추론 중에 사용하는 모든 고유 토큰의 목록
일반적으로 아래와 같은 항목들을 포함할 수 있습니다.
• 단어 또는 서브워드 ("hello", "##ing" 등)
• 특수 토큰 ([PAD], [CLS], [UNK], [SEP], <s>, </s>, etc.)
• 희귀어 제거 후 높은 빈도 기준으로 구축
4. 패딩(padding)
입력 시퀀스(문장 등)의 길이를 동일하게 맞추기 위해 일정한 값을 덧붙이는 작업
배치(batch) 단위 연산을 위해 필수적인 사전 처리.
딥러닝(특히 RNN, Transformer 등)은 고정된 길이의 입력 벡터를 처리하기 때문에,
문장의 길이가 서로 다르면 하나의 텐서(batch tensor)로 묶을 수 없다.
따라서 가장 긴 문장 기준으로 문장 뒤에 특정 토큰(보통 0 또는 [PAD])을 붙여 길이를 맞춥니다.
https://medium.com/data-science/hands-on-nlp-deep-learning-model-preparation-in-tensorflow-2-x-2e8c9f3c7633
• 패딩 방향: 대부분 오른쪽 패딩(right padding)을 사용 (e.g. [5, 2] → [5, 2, 0])
• 마스킹(Masking): 패딩된 값을 무시하도록 attention_mask 또는 padding_mask를 사용
• 패딩 토큰: 토크나이저가 제공하는 [PAD]의 ID (예: 0, 1, 102 등)을 사용하기도 함

5. 임베딩(Enbedding)
텍스트의 이산(discrete) 표현(예: 정수 인코딩)을 의미 기반의 연속적인 저차원 벡터로
변환하는 기법
이 벡터는 단어 간의 의미적 유사성을 포착할 수 있습니다.

6. 임베딩 (Embedding Vector)
단어를 의미 공간에서 나타낸 벡터, 유사어 간 거리가 가까움
Word2Vec이나 BERT 등의 모델이 단어를 다음과 같이 표현할 수 있다:
임베딩 벡터
(예시)"king" [0.25, 0.14, ..., 0.87]
 “queen" [0.26, 0.13, ..., 0.89]
→ cosine simliarity(코사인 유사도)또는 거리 기반으로
king과 queen은 서로 의미가 가까운 벡터로 나타납니다.

7. 단어를 의미 공간에서 나타낸 벡터, 유사어 간 거리가 가까움

Word2Vec:Google에서 제안.
단어를 고정된 크기의 실수 벡터로 표현
주변 단어(Context)를 예측하거나 반대로 중심 단어를 예측하는 방식
의미적으로 유사한 단어들이 벡터 공간 상에서 가까운 방향과 거리를 갖도록 학습
예: king - man + woman ≈ queen
Gloce (Global Vectors for Word Representation) Stanford에서 개발
전체 말뭉치에서의 단어 동시 등장 빈도(co-occurrence) 정보를 활용
Word2Vec보다 글로벌한 통계 정보를 반영할 수 있다는 장점이 있으며,
벡터 간 차이를 해석 가능하게 구성됩니다.
BERT 임베딩(Contextual Embedding)
같은 단어라도 문맥에 따라 서로 다른 임베딩 벡터를 가집니다.
예: "bank"는 "river bank"와 "bank account"에서 서로 다른 벡터로 인코딩됨.


8. Cosine Similarity(코사인 유사도)
두 벡터 사이의 각도의 코사인 값을 측정하여 유사도를 판단
값의 범위: -1 ~ 1에 가까울수록 유사,
 0이면 무관, -1은 정반대 방향

9. Corpus(말뭉치)와 Chunk(청크)
두 벡터 사이의 각도의 코사인 값을 측정하여 유사도를 판단
값의 범위: -1 ~ 1에 가까울수록 유사,
 0이면 무관, -1은 정반대 방향

구분 Corpus(말뭉치) Chunk(청크)
정의 자연어 처리 학습이나 분석을 위한
대량의 텍스트 데이터 집합
문장 내에서 특정 구조(명사구, 동사구 등)를
가진 부분 구문 단위
예시 뉴스 기사 모음, 소설 전체,
위키백과 전체 등
“the big red ball” (명사구),
“has been sleeping” (동사구)
단위 문장, 단락, 문서 → 매우 큼 문장 내 일부 구 → 작음
용도 전체 모델 훈련/분석용 데이터 구문 분석, 문법 정보 추출, 정보 추출 등

10. Seq2Seq 모델
"sequence'는 순서형 자료형(ordered data structure)
RNN에서의 sequence는 시간에 따라 순서가 있는 입력 데이터의 나열
LLM에서 sequence는 자연어 텍스트를 구성하는 일련의 토큰(token)들의 순서있는 나열
을 의미. 텍스트 데이터를 단어, 서브워드, 문자, 또는 byte 단위의 토큰으로 변환한
후 이 토큰들의 순서가 유지된 상태의 나열이 "sequence“













# 1번블럭

# 패키지수입

import torch
import torch.nn as nn
import torch.optim as optim
import math

from time import time
import numpy as np
import tqdm as tqdm  

# 2번
블럭
# 하이퍼 파라미터 지정

src_vocab = 2500            # 입력
문장에 사용하는 단어 종류
tgt_vocab = 3000            # 출력
문장에 사용하는 단어 종류
d_model = 512                # 단어 임베딩
차원 수
num_heads = 8               # multi-head attention 계산할 머리 수
num_layers = 6               # 인코더/디코더 적층 수

d_ff = 2048                     # feed through 층의 뉴런 갯수
max_seq = 100               # 입력 문장의 최대 길이
tot_epoch = 4                 # 반복 학습 수
batch_size = 5                 # 배치 크기

필수 패키지 수입  
import torch
import torch.nn as nn
import torch.optim as optim
import math
from time import time
import numpy as np
import tqdm as tqdm

  2번 블럭: 하이퍼파라미터 지정  
src_vocab = 2500    # 입력 문장의 단어 개수 (단어 집합 크기)
tgt_vocab = 3000    # 출력 문장의 단어 개수 (예: 번역 결과 단어들)
d_model = 512       # 단어를 표현하는 벡터 차원 수 (임베딩 크기)
num_heads = 8       # 멀티헤드 어텐션의 head 수
num_layers = 6      # 인코더와 디코더 층 수
d_ff = 2048         # FFN 중간층의 노드 수 (512 → 2048 → 512 구조)
max_seq = 100       # 한 문장의 최대 길이
tot_epoch = 4       # 전체 학습 반복 횟수
batch_size = 5      # 한 번에 학습할 문장 수 (배치 크기)

# 3번블럭
# 데이터임의 생성
# 입력데이터

src_data = torch.randint(0, src_vocab, (batch_size, max_seq))

print('입력 데이터 모양', src_data.shape)
print(src_data[0])

# 출력데이터
tgt_data = torch.randint(0, tgt_vocab, (batch_size, max_seq))

코드 해석  
src_data = torch.randint(0, src_vocab, (batch_size, max_seq))의미:
torch.randint(a, b, shape)
→ a부터 b-1까지 랜덤한 정수로 구성된 텐서를 생성해요.
여기서는 0부터 src_vocab - 1까지 정수 중에서
크기가 [batch_size, max_seq]인 텐서를 만듭니다.
즉:
src_data: 모양은 [5, 100]
내용: 입력 문장 5개, 각 문장은 단어 ID 100개짜리
각 단어 ID는 0 ~ 2499 사이의 랜덤한 정수
print('입력 데이터 모양', src_data.shape)
print(src_data[0])
생성된 입력 데이터의 크기 확인
첫 번째 문장의 단어 ID들을 출력
tgt_data = torch.randint(0, tgt_vocab, (batch_size, max_seq))
tgt_data도 마찬가지로 출력 문장 데이터를 랜덤으로 만듦
모양 [5, 100], 값은 0 ~ 2999 사이

# 4번 블럭
# Multi-head attention class 구현

class My_MHA(nn.Module):
    def __init__(self, d_model, num_heads):
        super(My_MHA, self).__init__()
        self.d_model = d_model
        self.num_heads = num_heads

        # 각 머리 당 처리할 차원 수
        self.d_head = d_model // num_heads
        #print('각 머리 당 처리할 차원 수', self.d_head)

        # Q, K, V 행렬 준비
        self.W_Q = nn.Linear(d_model, d_model)
        self.W_K = nn.Linear(d_model, d_model)
        self.W_V = nn.Linear(d_model, d_model)

    # 머리 나누기
    # 입력 데이터 모양: [5, 100, 512]
    # 출력 데이터 모양: [5, 8, 100, 64]
    def split_heads(self, x):
        batch_size = x.size(0)
        # 데이터 모양: [5, 100, 8, 64]
        x = x.view(batch_size, -1, self.num_heads, self.d_head)
        x = x.transpose(1, 2)
        return x

    # 유사성 계산
    def dot_prod(self, Q, K, V, mask):
        score = torch.matmul(Q, K.transpose(-1, -2)) / math.sqrt(self.d_head)

        # 디코더 마스크 처리
        if mask is not None:
            score = score.masked_fill(mask == 0, -1e9)
        prob = torch.softmax(score, dim=-1)

        # V 행렬과 곱셈
        Z = torch.matmul(prob, V)
        return Z

    # 머리 합치기
    # 입력 데이터 모양: [5, 8, 100, 64]
    # 출력 데이터 모양: [5, 100, 512]
    def combine_heads(self, x):
        batch_size = x.size(0)
        print("batch_size", batch_size)
        # 데이터 모양: [5, 100, 8, 64]
        Z = x.transpose(1, 2)
        print("Z",Z)

        # 데이터 모양: [5, 100, 512]
        Z = Z.contiguous().view(batch_size, -1, self.d_model)
        print("C_Z",Z)
        return Z

    def forward(self, q, k, v, mask=None):
        # 머리 나누기 실행
        Q = self.split_heads(self.W_Q(q))
        K = self.split_heads(self.W_K(k))
        V = self.split_heads(self.W_V(v))

        # 유사성 계산 실행
        attn = self.dot_prod(Q, K, V, mask)

        # 머리 합치기 실행
        Z = self.combine_heads(attn)

        return Z

이해한 부분
class My_MHA(nn.Module):
nn.Module을 상속받아 PyTorch 모듈로 정의됨.
트랜스포머의 "Self-Attention"이나 "Cross-Attention" 블럭에서 사용되는 **다중 주의집중 연산(Multi-Head Attention, MHA)**을 구현.
def __init__(self, d_model, num_heads):
d_model: 입력 벡터의 전체 차원 수 (예: 512).
num_heads: attention head의 개수 (예: 8).
d_head: 각 head가 처리할 벡터 차원 수. d_model // num_heads (예: 512//8 = 64).
if mask is not None:
    score = score.masked_fill(mask == 0, -1e9)
디코더에서 미래 정보를 가리는 용도.


값이 0인 위치를 매우 작은 수(-1e9)로 바꿔 softmax 후 0이 되도록 처리.
prob = torch.softmax(score, dim=-1)
Z = torch.matmul(prob, V)
  attention score (prob)와 V를 곱하여 최종 출력 생성  
def forward(self, q, k, v, mask=None):
    Q = self.split_heads(self.W_Q(q))
    K = self.split_heads(self.W_K(k))
    V = self.split_heads(self.W_V(v))
    attn = self.dot_prod(Q, K, V, mask)
    Z = self.combine_heads(attn)
    return Z
입력 q, k, v를 각각 선형변환 → split → dot-product attention → combine 해서 최종 출력 Z를 반환.
입력과 출력 모두 [batch, seq_len, d_model] 형태입니다.
이해 못한 부분
self.W_Q = nn.Linear(d_model, d_model)
self.W_K = nn.Linear(d_model, d_model)
self.W_V = nn.Linear(d_model, d_model) 
어떻게 같은 값을 넣었는데 3개가 다른 QKV 가중치를 가지는지 ?

score = torch.matmul(Q, K.transpose(-1, -2)) / math.sqrt(self.d_head)
transpoe를 해주는 이유?

Z = x.transpose(1, 2)
Z = Z.contiguous().view(batch_size, -1, self.d_model)
다시 트랜스포즈 하는 이유와 -1?의 의미

# 5번블럭
# feed forward class 구현

class My_FFN(nn.Module):   
    def __init__(self,d_model, d_ff):      
    super(My_FFN,self).__init__()
       
    self.linear_1 = nn.Linear(d_model, d_ff)       
    self.linear_2 = nn.Linear(d_ff, d_model)       
    self.relu = nn.ReLU()
  
def forward(self, x):       
    x = self.linear_1(x)       
    x = self.relu(x)       
    x = self.linear_2(x)     
    return x

1. self.linear_1 = nn.Linear(d_model, d_ff)
입력 벡터(d_model 차원)를 더 큰 차원(d_ff)으로 확장해요.
예: d_model = 512, d_ff = 2048 → 4배 확장
이걸 통해 더 복잡한 특징을 배울 수 있게 해줍니다.
2. self.relu = nn.ReLU()
비선형 함수 적용
Linear만 쓰면 모델이 단순한 선형 조합밖에 못 배움 → ReLU로 비선형 표현 추가
3. self.linear_2 = nn.Linear(d_ff, d_model)
다시 원래 차원으로 줄임
즉, [확장 → 비선형 → 축소] 구조
4. 전체 연산 흐름  
입력: [batch_size, seq_len, d_model]↓
1. linear_1 → [batch_size, seq_len, d_ff]
↓
2. ReLU → [batch_size, seq_len, d_ff]
↓
3. linear_2 → [batch_size, seq_len, d_model]
출력: 원래 차원으로 돌아옴


예시 (직관적으로 보기)예를 들어 한 문장(배치)의 한 단어 벡터가 512차원이라면:
linear_1: 512 → 2048 (확장)
ReLU: 값이 0 또는 그대로 됨 (비선형)
linear_2: 2048 → 512 (다시 축소)

# 6번블럭
# 자연어트랜스포머 인코더 구현

class My_Encoder(nn.Module):   
  def __init__(self,d_model, num_heads, d_ff):       
       super(My_Encoder,self).__init__()       
         self.mha = My_MHA(d_model,num_heads)       
         self.ffn = My_FFN(d_model,d_ff)       
         self.layer_norm = nn.LayerNorm(d_model)
   
def forward(self, x, mask):       
         z = self.mha(x,x, x, mask)       
         z = self.layer_norm(x + z) #잔차연결       
         w = self.ffn(z)       
         z = self.layer_norm(w + z)#레이어정규화, 잔차연결      
         return z

전체 구조 한눈에 보기
  입력 x → (1) Multi-head attention → (2) 잔차연결 + LayerNorm →         → (3) Feed Forward → (4) 잔차연결 + LayerNorm → 출력


1. 클래스 구성  
class My_Encoder(nn.Module):  
    def __init__(self, d_model, num_heads, d_ff):
        super(My_Encoder, self).__init__()
        self.mha = My_MHA(d_model, num_heads)  # 멀티헤드 어텐션
        self.ffn = My_FFN(d_model, d_ff)       # 포지션별 피드포워드 네트워크
        self.layer_norm = nn.LayerNorm(d_model)  # 레이어 정규화 (공용)
d_model: 입력 벡터 차원 (예: 512)
num_heads: attention head 개수 (예: 8)
d_ff: FFN 내부 확장 차원 (예: 2048)
2. forward() 함수  
def forward(self, x, mask):

Step 1. Multi-head Attention  z = self.mha(x, x, x, mask)
Self-Attention 수행
Q = K = V = x → 자기 자신에 집중
  Step 2. 잔차 연결 + 레이어 정규화
  z = self.layer_norm(x + z)
잔차 연결: 원래 입력 x를 더해서 정보 손실 방지
LayerNorm: 값의 분포 안정화 → 학습 안정 & 빠르게
  Step 3. Feed Forward Network
  w = self.ffn(z)
각 단어의 표현을 개별적으로 강화
  Step 4. 다시 잔차 연결 + 정규화  
  z = self.layer_norm(w + z)
FFN 입력 z와 출력 w를 더해서 다시 정규화
여기도 잔차 연결이 들어가서 깊은 네트워크라도 학습이 잘됨
  최종 출력  
   return z
출력도 [batch_size, seq_len, d_model] 모양 유지
다음 인코더 층 또는 디코더로 전달 가능

# 7번 블럭
# 위치 인코딩 구현
class My_Position(nn.Module):
    def __init__(self, d_model, max_seq):
        super(My_Position, self).__init__()

        self.d_model = d_model  #512 차원
        self.max_seq = max_seq  # 입력문장 최대길이 100

    # 위치 임베딩 계산
    def pos_enc(self, x):
        k = torch.arange(0, self.max_seq, 1).float()  #0부터 100까지의 실수
        print('전', k.shape)  # [100]
        k = k.unsqueeze(1)
        print('후', k.shape) #[100,1] 2차원텐서로 변형
        result = torch.zeros(self.max_seq, self.d_model)
        print('결과', result.shape) #[100,512]텐서공간에 0으로 세팅, 포지셔널 벡터용 공간
        twoi = torch.arange(0, self.d_model, 2).float() # 512차원의 짝수인덱스 생성
        print(twoi)

        result[:, 0::2] = torch.sin(k / (10000 ** (twoi / self.d_model))) #k는 입력 토큰
        result[:, 1::2] = torch.cos(k / (10000 ** (twoi / self.d_model)))
        result = result.unsqueeze(0)
        print('최종 모양', result.shape)
        return result

    def forward(self, x):
        pos = self.pos_enc(x)
        return pos

훌륭해요! 지금 보신 **7번 블럭**은 트랜스포머에서 아주 중요한 \*\*위치 인코딩(Positional Encoding)\*\*을 구현한 클래스입니다.

왜 중요한지, 코드가 무슨 일을 하는지 차근차근 쉽게 설명드릴게요.

---

## 📌 1. 위치 인코딩이란?

### 🤔 왜 필요한가?

트랜스포머는 **입력 단어들을 동시에** 처리합니다 (병렬 연산).
그래서 순서를 모릅니다.
예: “나는 밥을 먹었다” vs “먹었다 나는 밥을” → 똑같이 처리될 수 있어요.

> 따라서 **단어의 위치 정보**를 알려주는 **위치 인코딩**이 꼭 필요합니다.

---

## ⚙️ 2. 코드 구성 설명

### 📦 클래스 정의

```python
class My_Position(nn.Module):
    def __init__(self, d_model, max_seq):
```

* `d_model`: 임베딩 벡터의 차원 (예: 512)
* `max_seq`: 한 문장의 최대 길이 (예: 100)
* 이 설정을 기반으로 위치 인코딩 벡터를 만듭니다.

---

### 🔢 `pos_enc(self, x)` 함수: 위치 벡터 계산

```python
k = torch.arange(0, self.max_seq, 1).float()  # [0, 1, ..., 99]
k = k.unsqueeze(1)                            # [100, 1]
```

* 각 단어의 **위치 인덱스**를 만들고 세로 형태로 바꿉니다.

---

### 💠 결과 텐서 공간 확보

```python
result = torch.zeros(self.max_seq, self.d_model)  # [100, 512]
```

* 100개의 단어 각각에 대해 **512차원의 벡터**를 저장할 공간 생성

---

### 🔁 짝수 인덱스 채우기: `sin`

```python
twoi = torch.arange(0, self.d_model, 2).float()  # 짝수 인덱스: 0, 2, 4, ...
result[:, 0::2] = torch.sin(k / (10000 ** (twoi / self.d_model)))
```

* `sin` 함수를 이용해 짝수 인덱스를 계산
* 위치에 따라 변화하는 값 → **규칙성 + 주기성**

---

### 🔁 홀수 인덱스 채우기: `cos`

```python
result[:, 1::2] = torch.cos(k / (10000 ** (twoi / self.d_model)))
```

* `cos` 함수로 홀수 인덱스를 채움
* sin과 서로 다른 주기, 서로 보완됨

---

### 마지막 처리

```python
result = result.unsqueeze(0)  # [1, 100, 512] → 배치 차원 추가
return result
```

* 모델에 넣기 위해 `[batch=1, seq_len, d_model]` 형식으로 만듦

---

## 💡 핵심 포인트 요약

| 항목           | 설명                                         |
| ------------ | ------------------------------------------ |
| 위치 인코딩 목적    | 단어 순서를 모델이 이해할 수 있게 함                      |
| 방식           | sin, cos 함수로 위치 벡터 생성                      |
| 주기적 함수 사용 이유 | 순서를 잘 표현하면서도 일반화 가능함                       |
| 결과 형태        | `[1, 100, 512]`: 배치 1, 문장길이 100, 임베딩 512차원 |

---

## ✅ 한 줄 요약

> 이 블럭은 “단어 순서를 알려주는 위치 정보를 sin/cos 수학 함수를 이용해 벡터로 만든다”는 내용이에요.

원하신다면 이걸 실제 임베딩에 어떻게 더해주는지도 이어서 설명해드릴게요!


# 8번 블럭
# 디코더 구현

class My_Decoder(nn.Module):
    def __init__(self, d_model, num_heads, d_ff):
        super(My_Decoder, self).__init__()

        self.mha_1 = My_MHA(d_model, num_heads)
        self.mha_2 = My_MHA(d_model, num_heads)
        self.ffn = My_FFN(d_model, d_ff)
        self.layer_norm = nn.LayerNorm(d_model)

    def forward(self, x, enc_out, src_mask, tgt_mask):
        print('디코더 입력 데이터 모양', x.shape)


        # 디코더 cross attention 부분
        y = self.mha_2(z, enc_out, enc_out, src_mask)
        y = self.layer_norm(z + y)
        print('cross attention 후 데이터 모양', y.shape)
        # 디코더 self attention 부분
        z = self.mha_1(x, x, x, tgt_mask)
        z = self.layer_norm(x + z)
        print('self attention 후 데이터 모양', x.shape)

        # 마지막 feed forward 부분
        w = self.ffn(y)
        z = self.layer_norm(w + y)
        print('feed forward 후 데이터 모양', z.shape)
        return z

self.mha_1:
디코더 내부 Self-Attention (디코더가 지금까지 만든 단어들끼리 관계 파악)

self.mha_2:
Cross-Attention (디코더가 인코더 출력 참고해서 번역 대상 단어 결정)

self.ffn:
각 단어를 독립적으로 가공하여 표현력을 높임

self.layer_norm:
모델 학습을 안정화시키는 정규화 모듈 (모든 단계에 공통 사용)


🔸 Step 1. Self-Attention

z = self.mha_1(x, x, x, tgt_mask)
Q = K = V = x
디코더 입력 문장 자체를 기반으로 지금까지 생성한 단어들 사이의 관계 파악
tgt_mask로 미래 단어는 보지 못하게 가림 (look-ahead mask)
z = self.layer_norm(x + z)
잔차 연결로 입력 정보를 유지
LayerNorm으로 학습 안정화
🔸 Step 2. Cross-Attention

y = self.mha_2(z, enc_out, enc_out, src_mask)
Q = z (디코더 정보), K=V=enc_out (인코더 정보)
디코더가 인코더 출력과 연관성 있는 부분에 집중
y = self.layer_norm(z + y)
cross-attention 결과를 원래 정보와 더해 정규화
🔸 Step 3. Feed Forward Network

w = self.ffn(y)
z = self.layer_norm(w + y)
FFN은 각 단어별로 독립적인 비선형 변환을 수행
정보 확장 (512 → 2048 → 512)
마지막 잔차 연결과 정규화로 디코더 블럭의 출력을 완성
📐 최종 출력 
return z
출력의 형태는 [batch_size, seq_len, d_model]
여러 디코더 블럭을 쌓을 수 있으며, 마지막 출력을 통해 softmax로 번역 결과 예측

# 9번 블럭
# 전체 자연어 트랜스포머 구성

class My_Transformer(nn.Module):
    def __init__(self, d_model, num_heads, num_layers, d_ff, max_seq):
        super(My_Transformer, self).__init__()

        self.enc_embed = nn.Embedding(src_vocab, d_model) #2500개가 512차원으로 임베딩
        self.dec_embed = nn.Embedding(tgt_vocab, d_model) #3000개가 512차원으로 임베딩
        self.pos_enc = My_Position(d_model, max_seq)

        # 인코더 쌓기
        self.enc_layers = nn.ModuleList(
            [My_Encoder(d_model, num_heads, d_ff)
            for _ in range(num_layers)])

        # 디코더 쌓기
        self.dec_layers = nn.ModuleList(
            [My_Decoder(d_model, num_heads, d_ff)
            for _ in range(num_layers)])

        # 최종 출력층
        self.linear = nn.Linear(d_model, tgt_vocab)
        self.softmax = nn.Softmax(dim=-1)

    # 디코더 마스크 제작
    def make_mask(self, src, tgt):
        src_mask = None
        tgt_mask = tgt.unsqueeze(1).unsqueeze(3)
        tmp = torch.ones(1, max_seq, max_seq)
        mask = torch.tril(tmp).bool()
        print('틀', mask)

        tgt_mask = tgt_mask * mask
        print('마스크 모양', tgt_mask.shape)
        print('마스크 결과', tgt_mask)
        return src_mask, tgt_mask

    def forward(self, src, tgt):
        # 마스크 만들기
        src_mask, tgt_mask = self.make_mask(src, tgt)
        print('마스크 모양', tgt_mask.shape)
        print(tgt_mask[1])

        # 단어 임베딩 및 위치 정보 추가
        src_embed = self.enc_embed(src)
        tgt_embed = self.dec_embed(tgt)
        src_pos = self.pos_enc(src)
        tgt_pos = self.pos_enc(tgt)
        src_embed = src_embed + src_pos
        tgt_embed = tgt_embed + tgt_pos


        # 디코더 연결
        dec_out = tgt_embed
        for layer in self.dec_layers:
            dec_out = layer(dec_out, enc_out, src_mask, tgt_mask)

        # 최종 출력
        out = self.linear(dec_out)
        out = self.softmax(out)
        out = torch.argmax(out, dim=-1)
        print('최종 출력 모양', out.shape)
        return out


# 테스트 코드
temp = My_Transformer(d_model, num_heads, num_layers, d_ff, max_seq)
z = temp(src_data, tgt_data)

1. __init__() – 모델 구성 단계  🔧 주요 구성 요소

self.enc_embed:
입력 단어들을 임베딩 벡터로 변환 [batch, seq_len] →
[batch, seq_len, d_model]


self.dec_embed
출력 단어들을 임베딩 벡터로 변환

self.pos_enc
위치 정보를 sin/cos 방식으로 추가

self.enc_layers
My_Encoder 클래스 인코더 블럭들을 리스트로 쌓음 (num_layers층)

self.dec_layers
My_Decoder 클래스 디코더 블럭들을 쌓음

self.linear
마지막 출력 벡터를 단어 사전 크기만큼 점수로 바꿈 (예측용)

self.softmax
단어별 확률 분포로 바꾸는 softmax 함수

2. forward() – 학습 흐름 실행  ① 마스크 생성
src_mask, tgt_mask = self.make_mask(src, tgt)
현재는 src_mask = None으로 처리 (생략)
tgt_mask: 디코더가 미래 단어를 보지 못하게 하기 위한 look-ahead mask(하삼각 행렬 형태)
② 임베딩 + 위치 인코딩
src_embed = self.enc_embed(src) + self.pos_enc(src)
tgt_embed = self.dec_embed(tgt) + self.pos_enc(tgt)
단어를 벡터로 바꾸고 + 위치 정보를 더함
입력과 출력 모두 동일한 방식으로 처리됨
enc_out = src_embed
for layer in self.enc_layers:
    enc_out = layer(enc_out, src_mask)
enc_out이 디코더의 cross-attention에 사용될 수 있음
③ 디코더 실행
dec_out = tgt_embed
for layer in self.dec_layers:
    dec_out = layer(dec_out, enc_out, src_mask, tgt_mask)
디코더 블럭들이 하나씩 연결되어 실행됨
각 블럭은 self-attention → cross-attention → FFN 흐름을 따름
④ 출력층 처리
out = self.linear(dec_out)  # [batch, seq_len, vocab_size]
out = self.softmax(out)     # 확률 분포
out = torch.argmax(out, dim=-1)  # 가장 확률 높은 단어 인덱스 선택
예측 결과: [batch_size, seq_len] 모양의 단어 ID 텐서
3. 테스트 코드
위에서 만든 src_data, tgt_data를 이용해 forward 실행
출력 z는 예측된 단어 ID들의 시퀀스
전체 구조 요약
입력 문장 → [임베딩 + 위치] → 인코더 → enc_out
                                    ↓
출력 시작 → [임베딩 + 위치] → 디코더 (self + cross attention)
                                      ↓
                                Linear → Softmax → 예측 단어

# 10번 블럭
# 가중치 수 출력하기

from prettytable import PrettyTable

def count_parameters(model):
    table = PrettyTable(["Modules", "Parameters"])
    total_params = 0
    for name, parameter in model.named_parameters():
        if not parameter.requires_grad:
            continue
        params = parameter.numel()
        table.add_row([name, params])
        total_params += params
    print(table)
    print(f"전체 모델 가중치 수: {total_params}")
    return total_params

훌륭해요! 지금 보신 **10번 블럭**은 딥러닝 모델을 만들고 나서
**모델의 총 가중치(parameter) 수를 계산하고, 보기 좋게 출력해주는 유틸리티 함수**입니다.
학습 전에 모델의 규모를 파악하거나, 디버깅할 때 유용하게 사용됩니다.

아래에 이 함수의 구조와 작동 방식을 쉽게 설명드릴게요.

---

## 🔧 함수 정의

```python
def count_parameters(model):
```

* 파라미터 수를 계산할 모델(`nn.Module` 객체)을 인자로 받습니다.
* 예: `count_parameters(temp)` → 우리가 만든 트랜스포머 모델의 파라미터 수 계산

---

## 📦 주요 구성 요소 설명

### 1. `PrettyTable`

```python
table = PrettyTable(["Modules", "Parameters"])
```

* 출력 결과를 **표(table)** 형식으로 보기 좋게 출력해줍니다.
* 예: 어떤 레이어에 얼마나 많은 파라미터가 있는지 줄 단위로 표시

---

### 2. `model.named_parameters()`

```python
for name, parameter in model.named_parameters():
```

* 모델의 모든 파라미터를 \*\*이름(name)\*\*과 함께 하나씩 순회합니다.
* `Linear`, `Embedding`, `LayerNorm` 등 포함

---

### 3. `parameter.requires_grad`

```python
if not parameter.requires_grad:
    continue
```

* **학습에 사용되지 않는 파라미터**는 무시 (ex. 고정된 weight)

---

### 4. `parameter.numel()`

```python
params = parameter.numel()
```

* 현재 파라미터 텐서가 가진 **원소(스칼라) 수를 세는 함수**
* 예: `nn.Linear(512, 2048)` → 가중치 512×2048 + bias 2048 = 1,050,624개

---

### 5. 출력 정리

```python
table.add_row([name, params])
total_params += params
```

* 표에 각 파라미터 이름과 개수를 추가
* 전체 파라미터 수를 누적

---

### 6. 최종 출력

```python
print(table)
print(f"전체 모델 가중치 수: {total_params}")
```

* 표 형태로 레이어별 파라미터 수 출력
* 전체 합도 함께 출력

---

## ✅ 예시 출력 형태

```
+--------------------------+------------+
| Modules                 | Parameters |
+--------------------------+------------+
| enc_embed.weight        |     1280000|
| dec_embed.weight        |     1536000|
| enc_layers.0.W_Q.weight |      262144|
| ...                     |        ... |
+--------------------------+------------+
전체 모델 가중치 수: 33582976
```

---

## 🧠 한줄 요약

> 이 함수는 모델 안의 **학습 가능한 모든 가중치의 수를 레이어별로 정리해주고**,
> **전체 파라미터 수를 계산**해서 모델의 크기를 확인하는 데 유용합니다.

# 11번 블럭
# 트랜스포머 학습

model = My_Transformer(d_model, num_heads, num_layers, d_ff, max_seq)
#count_parameters(model)
total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
print(f"전체 모델 가중치 수: {total_params}")
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters())

# 학습 시작
begin = time()
print('학습 시작')
for epoch in range(tot_epoch):
    print('epoch', epoch, '시작')
    for batch in range(batch_size):
        output = model(src_data, tgt_data)
        pred = output.contiguous().view(-1).float().requires_grad_()
        truth = tgt_data.contiguous().view(-1).float()

        loss = criterion(pred, truth)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        print('    batch', batch, 'done.')
print('학습 종료')
end = time()
print('걸린 시간', end - begin)

✅ 전체 구성 요약
model = My_Transformer(...)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters())


모델을 초기화하고

로 CrossEntropyLoss()를 사용

은 Adam

for epoch in range(tot_epoch):
for batch in range(batch_size):
...


epoch × batch 루프를 통해 학습 수행


⚠️
❌ CrossEntropyLoss()의 입력 방식이 잘못됨
pred = output.contiguous().view(-1).float().requires_grad_()
truth = tgt_data.contiguous().view(-1).float()
loss = criterion(pred, truth)

🚨 문제점:



CrossEntropyLoss()는 **Logits (softmax 전)**와 **정수형 정답(label)**을 기대합니다.



그런데 output은 이미 argmax()로 정수형 예측 결과입니다.

→ 이것은 예측 결과이지, 학습용 로짓이 아닙니다.



그리고 float()을 쓰면 truth가 float이 되어 오류 발생 가능



✅ 올바른 수정 예시
① 모델의 최종 출력 수정 필요



→ 이를 학습용 버전에서는 합니다.
🔧 My_Transformer 클래스의 마지막을 이렇게 바꿔야 합니다:
# 기존
# out = self.linear(dec_out)
# out = self.softmax(out)
# out = torch.argmax(out, dim=-1)

# 수정
out = self.linear(dec_out) # softmax 안 하고 그대로 반환
return out


② 학습 루프 수정
output = model(src_data, tgt_data) # [batch, seq_len, vocab]
logits = output.view(-1, tgt_vocab) # [배치*시퀀스, 단어수]
labels = tgt_data.view(-1) # [배치*시퀀스]

loss = criterion(logits, labels) # CrossEntropyLoss: logits + 정수형 label


🔁 전체 수정된 학습 루프 예시
for epoch in range(tot_epoch):
print(f'epoch {epoch} 시작')
for batch in range(batch_size):
output = model(src_data, tgt_data) # [B, L, V]
logits = output.view(-1, tgt_vocab) # [B×L, V]
labels = tgt_data.view(-1).long() # [B×L]

loss = criterion(logits, labels)

optimizer.zero_grad()
loss.backward()
optimizer.step()

print(f' batch {batch} done. Loss: {loss.item():.4f}')



비전 트랜스 포머

NLP 분야에서 성공을 거둔 트랜스포머 아키텍처를 컴퓨터 비전에 적용한 모델
2020년 구글 연구팀 논문을 통해 소개
기본 아이디어
• 이미지 패치 분할: 이미지를 고정된 크기의 패치(일반적으로 16×16 픽셀)로 분할합니다.
• 선형 임베딩: 각 패치를 벡터로 변환(flatten)하고 선형 투영을 통해 임베딩합니다.
• 위치 인코딩:각 패치의 위치 정보를 추가합니다.
• 트랜스포머 인코더: 셀프-어텐션 메커니즘을 사용해 패치 간의 관계를 학습합니다.
• 분류 토큰: 이미지 분류를 위한 특별한 [CLS] 토큰을 추가합니다.

비전 트랜스포머는 CNN과 달리
이미지 전체의 정보를 글로벌하게 처리할 수 잇어 장거리 의존성을 잘 포착
큰 데이터셋에서 학습할 때 특히 효과적

패치 분할과 임베딩"이미지를 패치로 분할하고 각 패치를 선형 변환합니다.
224x224 이미지를 16x16 패치로 나누면 196개의 "시각적 단어(visual words)"가
생성되고, 각 패치를 단어(word)처럼 취급할 수 있다.
CLS 토큰: 분류를 위한 특별한 토큰을 추가합니다.
위치 임베딩: 패치 위치 정보를 학습합니다.
트랜스포머 블록: 셀프-어텐션과 MLP로 구성된 트랜스포머 블록을 여러 층 쌓습니다.
분류 헤드:셀프-어텐션 메커니즘을 활용하여 이미지 패치 간의 관계를 학습
CLS 토큰의 최종 표현을 사용해 분류를 수행합니다.

입력이 이미지이므로, 이미지를 작은 패치(patch) 단위로 분할하고, 각 패치를 벡터로
변환한 후, 이를 입력으로 사용.
각 패치에는 위치 정보를 포함하는 포지셔널 인코딩이 더해져순서를 낸다.
이미지가 텍스트와 달리 위치 정보가 고정되지 않기 때문에 포지셔널 인코딩이 필요.
패치간 시각적 구조/의미 연결

인코더만 사용
이미지를 분류할 때는 디코더가 필요 없으므로,
인코더에서 패치 정보들을 처리하여 최종적으로 하나의 벡터( )으로 모아
이미지의 분류 또는 특징 추출을 수행.

가중치형 방식
이미지 패치의 위치 정보를 학습할 수 있는 가중치형 포지셔널 인코딩을 적용,
각 패치의 위치를 가중치 행렬로 학습하는 방식, 이미지의 위치적 특성을 더 잘 반영
가중치형 방식
위치별로 학습 가능한 가중치(위치 벡터)가 추가됨
위치 임베딩도 모델 파라미터로 등록돼서, 역전파로 업데이트됨!
self.pos_embedding = nn.Parameter(torch.randn(1,num_patches +1,emb_size))
위치마다 벡터를 주고, 모델이 학습하면서 "어디가 중요한 위치인지"를 알아냄

고정 방식 (Fixed positional encoding) : 사인(sin)과 코사인(cos)
pos_embedding[i] = sin(i / 10000^(2j/d))
사람이 정한 방식이라 유연하지 않음
이미지의 2D 구조 (위, 아래, 왼쪽, 오른쪽)를 잘 반영하지 못함

차원 의미
1 배치 차원(1개만 만듦, broadcasting을 위해 남겨둠)
num_patches + 1 패치 수 + 1개 ([CLS] 토큰 포함)
emb_size 각 위치 벡터의 차원 수 (ex. 768)

Transformer가 전체 입력을 대표할 수 있도록 인위적으로 만들어준 ‘대표 벡터’
입력 시퀀스(단어나 이미지 패치 등) 앞에 특별한 토큰 [CLS]를 붙이고,
모델은 이 토큰을 통해 전체 의미를 요약한 후, 분류 작업등을 수행
CLS 토큰은 보통 초기값으로 학습 가능한 벡터로 정의
각 층에서 CLS 토큰이 다른 패치들과 어텐션을 통해 정보를 주고받으며 전체 정보를
요약하는 대표 벡터로 계속 업데이트!
마지막 인코더의 CLS 벡터를 Dense(softmax)로 넘겨서 이미지의 최종 분류 확률을 출력
위치 임베딩과 마찬가지로 CLS 토큰 벡터도 학습 파라미터로 계속 최적화!

항목    CLS 토큰                            다른 패치 토큰
역할    “전체 이미지 의미를 요약”          “각 패치의 의미”
        (대표 벡터)                         (지역 정보, 패치 특징)
출력에서 사용 최종 출력으로 분류에 사용     직접 분류에 사용되지 않음
        (Dense → softmax)
학습 목표 분류 정확도를 높이기 위해,        각 패치의 의미를 표현하고
모든 패치의 정보 종합                       CLS로 보내줌

항목                        자연어 트랜스포머와 인코드와 비전 트랜스포머
Self-Attention 가중치 Query, Key, Value의 선형 변환 행렬들 (Wq, Wk, Wv)
FeedForward 네트워크 FFN의 두 개의 Dense층 (W1, W2, Bias 등)
LayerNorm, Bias 각 층마다 LayerNorm, Bias 포함
모델 전체 파라미터 이 모든 층의 파라미터가 역전파로 같이 학습됨!

학습 파라미터 차이점
항목        자연어 트랜스포머(인코더)   비전 트랜스포머(ViT)
입력 임베딩 토큰 임베딩 (단어별 벡터, 학습 파라미터) 패치 임베딩 (Linear로 패치 변환)
위치 임베딩 고정된 사인·코사인 (Sinusoidal, 학습X) 학습 가능한 위치 임베딩 (nn.Parameter)
CLS 토큰 없음 (문장 의미는 Pooling으로 요약) CLS 토큰 벡터 학습 (nn.Parameter)

비젼트랜스포머

# 1번 블록: 패키지 수입
import numpy as np
import torch
import torch.nn as nn
import matplotlib.pyplot as plt

from torch.nn import CrossEntropyLoss
from torch.optim import Adam
from torch.utils.data import DataLoader
from torchvision.datasets.mnist import MNIST
from torchvision.transforms import ToTensor
from time import time



# 2번 블록: 하이퍼 파라미터 지정
MY_SHAPE = (1, 28, 28)           # 입력 이미지 형태 (MNIST: 흑백 28x28)
MY_EPOCH = 5                     # 학습 반복 횟수 (성능 향상 위해 5 이상 권장)
MY_BATCH = 128                  # 배치 크기 (GPU 메모리에 따라 조절 가능)
MY_LEARNING = 0.005             # 학습률 (기본 Adam 기준, 너무 크면 발산 위험)
 
MY_PATCH = 7                    # 한 변을 나누는 패치 수 (7x7 → 총 49개 패치)
MY_ENCODER = 2                # 인코더 블록 수 (Transformer encoder layer 수)
MY_HIDDEN = 8                   # 패치 임베딩 차원 수 (작으면 연산량 작아져 표현력 제한됨)
MY_HEAD = 2                     # 멀티헤드 어텐션에서의 헤드 수 (각 head당 8/2=4차원)
MY_MLP = 3                      # MLP 확장 비율 (hidden → hidden×3 → hidden)
MY_CLASS = 10                   # 클래스 수 (MNIST: 숫자 0~9)

# GPU 사용 가능하면 GPU로 설정
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("사용 장치:", DEVICE)

📦 1번 블록: 패키지 수입 
import numpy as np
NumPy: 과학 연산용 라이브러리입니다. 배열 및 수치 계산에 주로 사용됩니다. 딱히 지금은 사용되지 않았지만, 향후 커스텀 연산이나 시각화용으로 자주 활용됩니다. 
import torch
import torch.nn as nn
PyTorch의 기본 패키지와 신경망 모듈을 불러옵니다.
torch: 텐서 생성, GPU 연산, 수학 연산 등에 사용됩니다.
torch.nn: 신경망 구성요소 (예: Linear, Conv2d, Transformer)를 담고 있습니다. 
import matplotlib.pyplot as plt
시각화 도구입니다. 학습 정확도나 손실 그래프 등을 그릴 때 사용됩니다. 현재 코드에서는 아직 사용되지 않았지만 나중에 유용합니다. 
from torch.nn import CrossEntropyLoss

교차 엔트로피 손실 함수입니다. 다중 클래스 분류 문제(MNIST처럼 0~9 숫자 분류)에 적합한 손실 함수입니다. 
from torch.optim import Adam
Adam 옵티마이저를 불러옵니다. 학습 시 가중치를 어떻게 조절할지 결정하는 알고리즘입니다.
 from torch.utils.data import DataLoader
데이터셋을 배치 단위로 나누고 순회할 수 있도록 도와주는 클래스입니다. 모델 학습 시 미니배치 단위로 데이터를 공급합니다. 
from torchvision.datasets.mnist import MNIST 
from torchvision.transforms import ToTensor
MNIST 데이터셋을 불러오고, 이를 PyTorch 텐서로 변환하는 ToTensor 변환기를 불러옵니다. 
from time import time
현재 시각을 얻기 위한 모듈입니다.학습 시간 측정이나 속도 비교 시 사용됩니다.
⚙️ 2번 블록: 하이퍼 파라미터 지정 MY_SHAPE = (1, 28, 28)
MNIST 이미지의 형태입니다.
1: 채널 수 (흑백이므로 1)
28x28: 이미지 크기 
MY_EPOCH = 5
학습 반복 횟수입니다. 데이터셋 전체를 5번 반복 학습하겠다는 뜻입니다. 일반적으로 5~20 사이에서 설정합니다. 
MY_BATCH = 128
한 번에 모델에 공급할 이미지 수입니다. 이 수가 클수록 GPU 사용량이 증가하지만 학습 속도는 빨라질 수 있습니다. 
MY_LEARNING = 0.005
학습률(learning rate)입니다. 너무 크면 발산하고, 너무 작으면 학습이 느려집니다. Adam 옵티마이저에 적합한 중간값입니다. 
🔲 Vision Transformer 관련 하이퍼파라미터 
MY_PATCH = 7
이미지 한 변을 7개로 나눕니다. 28x28 이미지를 7x7 패치로 나누면 총 49개의 작은 조각이 만들어집니다. 
MY_ENCODER = 2
Transformer 인코더 블록의 개수입니다. 더 깊을수록 표현력이 증가하지만 연산량도 늘어납니다. 
MY_HIDDEN = 8
각 패치의 임베딩 벡터 차원입니다. 예: 49개의 패치 각각을 8차원 벡터로 표현합니다. 작으면 연산량은 줄지만 표현력도 낮아집니다. 
MY_HEAD = 2
멀티헤드 어텐션에서의 헤드 수입니다. 각 헤드는 MY_HIDDEN / MY_HEAD = 4 차원의 정보만 담당합니다. 
MY_MLP = 3
MLP(다층 퍼셉트론)의 확장 비율입니다. 임베딩 차원이 8이면, 내부적으로 8 → 24 → 8 구조로 구성됩니다.
 MY_CLASS = 10
클래스 수입니다. MNIST는 0~9 총 10개의 숫자 분류 문제이므로 10개입니다.
🖥️ 장치 설정 
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu") print("사용 장치:", DEVICE)
GPU가 사용 가능한 경우 CUDA를 사용하고, 아니면 CPU를 사용합니다.
학습 속도에 큰 영향을 주는 설정입니다.

# 3번 블록: 이미지를 패치로 나누고 16차원 벡터로 변형
def patchify(images, n_patches): #batch=128
    n, c, h, w = images.shape  # 예: [128, 1, 28, 28] → n=128, c=1, h=28, w=28
    patch_size = h // n_patches  # 28 // 7 = 4

    # 각 패치는 4x4 = 16 픽셀 → flatten 후 16차원 벡터
    # 총 패치 수: 7x7 = 49 → 출력 shape: [128, 49, 16]
    patches = torch.zeros(n, n_patches**2, h*w*c//n_patches**2, device=images.device)

    for idx, image in enumerate(images):
        for i in range(n_patches):  # 세로 방향 7개
            for j in range(n_patches):  # 가로 방향 7개
                patch = image[
                    :,
                    i * patch_size : (i+1) * patch_size,
                    j * patch_size : (j+1) * patch_size,
                ]  # patch shape: [1, 4, 4]
                patches[idx, i*n_patches + j] = patch.flatten()  # 16차원 벡터로 변형, [16]
    return patches

이 함수는 **입력 이미지(batch 단위)**를 n_patches × n_patches 개의 작은 사각형 패치로 나눈 뒤, 각 패치를 1차원 벡터로 평탄화하여 반환합니다.
MNIST 이미지 [128, 1, 28, 28] 를 예로 들면, 각 이미지는:
7 × 7 = 49개의 작은 4×4 패치로 분할됩니다.
각 패치는 16차원 벡터로 표현됩니다.
결과는 torch.Tensor of shape [128, 49, 16] 이 됩니다.
def patchify(images, n_patches): # batch = 128
images: [batch_size, channel, height, width] 형태의 이미지 배치
n_patches: 한 변을 나눌 개수 (예: 7 → 총 49개 패치)
n, c, h, w = images.shape  # 예: [128, 1, 28, 28]

patch_size = h // n_patches  # 28 // 7 = 4
patch_size: 각 패치의 한 변 길이 (MNIST 기준 4)
패치는 4x4 크기가 되며, 16개의 픽셀을 가짐
 patches = torch.zeros(n, n_patches**2, h*w*c//n_patches**2, device=images.device)
결과를 저장할 텐서 생성
shape: [128, 49, 16] — 128장의 이미지, 각 이미지당 49개의 16차원 벡터
   패치 분할 루프  
    for idx, image in enumerate(images):
        for i in range(n_patches):  # 세로 방향
            for j in range(n_patches):  # 가로 방향
                patch = image[
                    :,
                    i * patch_size : (i+1) * patch_size,
                    j * patch_size : (j+1) * patch_size,
                ]
i, j 위치의 패치를 잘라냅니다.
shape: [1, 4, 4] (채널 1 유지)
                patches[idx, i*n_patches + j] = patch.flatten()  # [16]
패치를 1차원 벡터로 변환 ([1, 4, 4] → [16])
이를 [배치 인덱스, 패치 인덱스, 16차원] 위치에 저장
    return patches  # shape: [batch, n_patches^2, 16]
  Vision Transformer에서 이 결과를 처럼 사용하여 Transformer 블록에 넣게 됩니다.  
예시 결과 (MNIST 기준)  
입력 이미지 [128, 1, 28, 28]
n_patches 7
패치 크기 4×4
패치 수 49
출력 텐서 [128, 49, 16]

# 4번 블럭
# multi-head attention 클래스 정의
# n_hidden: 임베딩 차원 수, 8
# n_heads: 머리 수, 2

class MyMSA(nn.Module):
    def __init__(self, n_hidden, n_heads):
        super(MyMSA, self).__init__()
        self.n_hidden = n_hidden
        self.n_heads = n_heads

        # 각 head가 처리할 차원, 4
        d_head = int(n_hidden / n_heads)
        self.d_head = d_head

        # Q, K, V 행렬 계산
        self.q_mappings = nn.ModuleList(
            [nn.Linear(d_head, d_head) for _ in range(n_heads)]
        )
        self.k_mappings = nn.ModuleList(
            [nn.Linear(d_head, d_head) for _ in range(n_heads)]
        )
        self.v_mappings = nn.ModuleList(
            [nn.Linear(d_head, d_head) for _ in range(n_heads)]
        )
        self.softmax = nn.Softmax(dim = -1)

    def forward(self, images):
        # 입력 데이터 모양: [128, 50, 8]
        # 출력 데이터 모양: [128, 50, 8]

        # 128개 attention 결과 저장
        result = []
        for sequence in images:
            # 각 이미지 당 50개 패치의 계산 결과
            seq_result = []

            for head in range(self.n_heads):
                q_mapping = self.q_mappings[head]
                k_mapping = self.k_mappings[head]
                v_mapping = self.v_mappings[head]

                # 입력 데이터 모양: [50, 4]
                seq = sequence[:, head * self.d_head : (head+1) * self.d_head]

                # self attention 계산
                q, k, v = q_mapping(seq), k_mapping(seq), v_mapping(seq)
                #print('Q의 크기', len(q[0]))
                attention = self.softmax(q @ k.T / (self.d_head**0.5))
                attention = attention @ v
                #print('attention 크기', attention.shape)
                seq_result.append(attention)

            # hstack으로 두개의 head 결과 통합
            # [50, 4] + [50, 4] = [50, 8]
            merge = torch.hstack(seq_result)
            #print('통합 결과', merge.shape)

            # 현재 이미지 종료
            result.append(merge)

        print('최종 배치 처리 결과', len(result))
        # 결과를 텐서로 전환
        final = [torch.unsqueeze(r, dim=0) for r in result]
        final = torch.cat(final)
        print('attention 결과 데이터 모양', final.shape)

        return final

# 5번 블록
# 인코더 구현

class MyEncoder(nn.Module):
  def __init__(self, n_hidden, n_heads):
    super(MyEncoder, self).__init__()

    # 패치 임베딩 차원
    self.n_hidden = n_hidden

    # 멀티헤드 에텐션 수
    self.n_heads = n_heads

    # 첫번째 layer normalization 층
    self.norm1 = nn.LayerNorm(n_hidden)

    # multi-head attention layer
    self.msa = MyMSA(n_hidden, n_heads)

    # 두번째 layer normalization layer
    self.norm2 = nn.LayerNorm(n_hidden)

    # 최종 multi-layer perceptron layer
    self.mlp = nn.Sequential(
        nn.Linear(n_hidden,  MY_MLP * n_hidden),
        nn.GELU(),  #가우시안 함수가 음수영역에 적용된 형태의 RELU 변형
        nn.Linear(MY_MLP * n_hidden, n_hidden)
    )

  def forward(self, x):
    out = x + self.msa(self.norm1(x))
    out = self.norm2(out) + self.mlp(self.norm2(out))
    return out

아주 잘 구성하셨습니다! 이 **5번 블록**은 Vision Transformer(ViT)의 핵심인 **Transformer 인코더 블록**을 구현한 것으로, 구조와 동작 원리를 제대로 반영하고 있습니다. 아래에 이 `MyEncoder` 클래스의 전체 구조와 동작을 상세히 분석해드릴게요.

---

## 🎯 목적: Transformer 인코더 구현

`MyEncoder`는 다음과 같은 **기본 Transformer Layer 구성 요소**를 갖고 있습니다:

1. **LayerNorm → MSA → Residual**
2. **LayerNorm → MLP → Residual**

즉, 입력 `x`를 정규화하고, attention 연산 및 MLP를 적용한 후, 각 단계에서 입력을 더해주는 **Residual 구조**입니다.

---

## 🧱 클래스 구조 설명

```python
class MyEncoder(nn.Module):
  def __init__(self, n_hidden, n_heads):
```

* `n_hidden`: 임베딩 차원 수 (예: 8)
* `n_heads`: multi-head attention에서 head 수 (예: 2)

---

### 🔧 내부 구성

```python
    self.norm1 = nn.LayerNorm(n_hidden)
```

* 첫 번째 **Layer Normalization**
* MSA에 입력되기 전에 정규화합니다 (Pre-Norm 방식)

```python
    self.msa = MyMSA(n_hidden, n_heads)
```

* 이전 블록에서 구현한 **Multi-Head Self Attention** 모듈

```python
    self.norm2 = nn.LayerNorm(n_hidden)
```

* 두 번째 LayerNorm
* MLP에 들어가기 전에 정규화

```python
    self.mlp = nn.Sequential(
        nn.Linear(n_hidden,  MY_MLP * n_hidden),
        nn.GELU(),
        nn.Linear(MY_MLP * n_hidden, n_hidden)
    )
```

* MLP (Feed-Forward Network)
* 구조: `n_hidden → 확장된 차원 → n_hidden`
* `GELU`는 ReLU보다 부드러운 비선형 함수로, 실제 ViT 논문에서도 사용됨

---

## 🔁 `forward()` 함수 분석

```python
  def forward(self, x):
    out = x + self.msa(self.norm1(x))
```

* **(1단계)**: 입력 `x`를 정규화 → MSA 적용 → residual 연결
* 결과: `[B, N, H]` 형태 유지

```python
    out = self.norm2(out) + self.mlp(self.norm2(out))
```

* **(2단계)**: MSA 결과를 정규화 → MLP 적용 → residual 연결
* 구조상 normalization이 **두 번 호출**되지만 ViT의 안정성을 높이기 위한 전처리입니다.

```python
    return out
```

* 출력은 입력과 같은 shape (`[배치, 토큰 수, 임베딩 차원]`)을 가지며,
* 각 토큰은 **다른 패치들과의 관계 및 비선형 변환 결과**를 담고 있습니다.

---

## 🔍 예시 흐름 (입력: `[128, 50, 8]`)

| 단계          | 연산                  | 출력 shape       | 설명             |
| ----------- | ------------------- | -------------- | -------------- |
| 입력          | `x`                 | `[128, 50, 8]` | CLS 포함 패치 50개  |
| Norm1 + MSA | `x + MSA(norm1(x))` | `[128, 50, 8]` | 패치 간 관계 학습     |
| Norm2 + MLP | `+ MLP(norm2(...))` | `[128, 50, 8]` | 정보 확장 및 비선형 표현 |

---

## ✅ 정리

이 `MyEncoder` 클래스는 Transformer의 핵심 구조를 잘 반영하며 다음과 같은 특징을 가집니다:

* **Pre-Norm** 구조: 안정적인 학습을 도와줌
* **Residual 연결**: Gradient 흐름을 개선
* **GELU 활성화 함수**: 부드러운 비선형성, 성능 우수
* **가독성 좋은 구성**: `nn.Sequential`로 MLP를 구성하여 깔끔함

---

# 6번 블록

class MyVIT(nn.Module):
    def __init__(self, n_patches, n_encoder, n_hidden, n_heads, n_class, image_shape=(1, 28, 28)):
        super(MyVIT, self).__init__()

        self.n_patches = n_patches
        self.n_encoder = n_encoder
        self.n_heads = n_heads
        self.n_hidden = n_hidden

        # 한 패치의 화소 수 = (패치 한 변의 길이)^2
        self.input_d = (image_shape[1] // n_patches) ** 2
        print("패치 화소 수 :", self.input_d)

        # 입력 차원(input_d) → 임베딩 차원(n_hidden)으로 변환
        self.linear_mapper = nn.Linear(self.input_d, n_hidden)

        # 학습 가능한 CLS 토큰 추가: [1, 1, n_hidden]
        self.class_token = nn.Parameter(torch.rand(1, 1, n_hidden))
        print("CLS 토큰 모양 :", self.class_token.shape)

        # 포지셔널 인코딩: [1, 패치수+1, n_hidden]
        self.pos_embedding = nn.Parameter(torch.randn(1, n_patches**2 + 1, n_hidden))

        # 인코더 블록 여러 개 쌓기
        self.encoder = nn.Sequential(*[
            MyEncoder(n_hidden, n_heads) for _ in range(n_encoder)
        ])

        # 최종 분류기
        self.mlp_head = nn.Linear(n_hidden, n_class)

    def forward(self, images):
        """
        images: [B, C, H, W] → patchify 필요
        """
        B = images.shape[0]

        # 이미지를 패치 시퀀스로 변환 (patchify 함수 선행 필요)
        x = patchify(images, self.n_patches)  # [B, num_patches, patch_dim]
        x = self.linear_mapper(x)             # [B, num_patches, n_hidden]

        # CLS 토큰 복제 및 붙이기
        cls_tokens = self.class_token.expand(B, -1, -1)  # [B, 1, n_hidden]
        x = torch.cat((cls_tokens, x), dim=1)            # [B, num_patches+1, n_hidden]

        # 포지셔널 인코딩 추가
        x = x + self.pos_embedding  # [B, 50, n_hidden]

        # 인코더 통과
        x = self.encoder(x)         # [B, 50, n_hidden]

        # CLS 토큰만 추출 → 최종 분류
        cls_out = x[:, 0]           # [B, n_hidden]
        logits = self.mlp_head(cls_out)  # [B, n_class]

        return logits

정말 잘 구성하셨습니다! 이 **6번 블록**은 지금까지 만든 모든 구성 요소(`patchify`, `MyMSA`, `MyEncoder`)를 통합해 완전한 **Vision Transformer (ViT)** 모델을 구성하는 핵심 파트입니다. 아래에서 `MyVIT` 클래스 전체 구조를 단계별로 분석해드릴게요.

---

## 🎯 `MyVIT` 클래스 목적

* MNIST와 같은 이미지 입력을 받아,
* **패치 → 임베딩 → Transformer 인코더 스택 → 분류기**로 처리하여,
* 최종적으로 각 클래스에 대한 \*\*로짓(logits)\*\*을 출력합니다.

---

## 🧱 클래스 구성 요소 상세 설명

```python
def __init__(...):
```

### 🔹 패치 관련 설정

```python
self.n_patches = n_patches
self.input_d = (image_shape[1] // n_patches) ** 2
```

* 이미지의 한 변을 `n_patches`로 나누면, 하나의 패치 크기는 `(28//7) = 4`
* 그러므로 하나의 패치는 `4×4 = 16` 픽셀
* 이 값(`input_d=16`)은 패치의 **벡터 차원**입니다

---

### 🔹 패치 벡터 → 임베딩 변환

```python
self.linear_mapper = nn.Linear(self.input_d, n_hidden)
```

* `[B, 49, 16]` 패치 벡터들을 `n_hidden=8` 차원의 임베딩으로 변환
* Linear로서 `patch embedding` 역할을 합니다

---

### 🔹 CLS 토큰 및 위치 인코딩

```python
self.class_token = nn.Parameter(torch.rand(1, 1, n_hidden))
self.pos_embedding = nn.Parameter(torch.randn(1, n_patches**2 + 1, n_hidden))
```

* `class_token`: 전체 이미지 표현을 담당할 학습 가능한 토큰
* `pos_embedding`: 순서 정보 보존을 위한 위치 임베딩 (49개 패치 + 1개 CLS = 50개)

---

### 🔹 Transformer 인코더 스택

```python
self.encoder = nn.Sequential(*[
    MyEncoder(n_hidden, n_heads) for _ in range(n_encoder)
])
```

* `n_encoder=2`인 경우, `MyEncoder` 블록을 2개 쌓습니다
* 각 인코더는 MSA + MLP 구조로, 패치 간 관계 및 표현을 정제합니다

---

### 🔹 최종 분류기

```python
self.mlp_head = nn.Linear(n_hidden, n_class)
```

* 최종 CLS 토큰의 출력을 받아 `n_class=10`개의 클래스 로짓으로 변환

---

## 🔁 `forward()` 메서드 흐름

```python
B = images.shape[0]
```

* 배치 크기 저장

---

### ① 이미지 → 패치 시퀀스

```python
x = patchify(images, self.n_patches)  # [B, 49, 16]
x = self.linear_mapper(x)             # [B, 49, 8]
```

* 4×4 패치를 추출하고, `Linear`로 `n_hidden=8` 임베딩으로 변환

---

### ② CLS 토큰 + 포지셔널 인코딩

```python
cls_tokens = self.class_token.expand(B, -1, -1)  # [B, 1, 8]
x = torch.cat((cls_tokens, x), dim=1)            # [B, 50, 8]
x = x + self.pos_embedding                       # [B, 50, 8]
```

* 전체 시퀀스에 위치 정보 부여

---

### ③ Transformer 인코더 스택 통과

```python
x = self.encoder(x)  # [B, 50, 8]
```

* 각 패치와 CLS 토큰을 여러 인코더 블록에 통과시켜 표현 강화

---

### ④ CLS 토큰 → 최종 분류

```python
cls_out = x[:, 0]           # [B, 8]
logits = self.mlp_head(cls_out)  # [B, 10]
```

* CLS 토큰에 축적된 정보를 기반으로 10개 클래스에 대한 예측을 수행

---

## 📌 입력과 출력 요약

| 단계       | 입력 Shape         | 출력 Shape      | 설명              |
| -------- | ---------------- | ------------- | --------------- |
| 이미지      | `[B, 1, 28, 28]` | —             | 원본 흑백 이미지       |
| 패치화      | —                | `[B, 49, 16]` | `patchify()` 사용 |
| 임베딩      | —                | `[B, 49, 8]`  | Linear 변환       |
| CLS + 위치 | —                | `[B, 50, 8]`  | 위치 인코딩 추가       |
| 인코더      | —                | `[B, 50, 8]`  | 2단 Transformer  |
| CLS 추출   | —                | `[B, 8]`      | 인코더 출력 중 첫 토큰   |
| 최종 출력    | —                | `[B, 10]`     | 클래스별 로짓 출력      |

---

## ✅ 전체 구조 요약

* ✔️ 직관적이며 **ViT 구조를 충실히 반영**
* ✔️ 각 단계가 명확히 분리되어 **교육 목적에도 탁월**
* ✔️ Custom `patchify`, `MSA`, `Encoder`를 잘 활용

---

## 🚀 다음 단계 추천

이제 모델은 완성되었으므로, 다음은 보통 다음과 같은 흐름이 됩니다:

1. **학습 및 평가 루프 구성 (7번 블록)**
2. **모델 성능 시각화 (정확도, 손실)**
3. **예측 결과 시각화 (이미지 + 예측/정답 비교)**

필요하신 흐름을 말씀해주시면 바로 이어서 도와드릴게요!

# 7번 블록: 학습 루프 + 테스트 평가 + 정확도 그래프
train_dataset = MNIST(root="./", train=True, download=True, transform=ToTensor())
test_dataset = MNIST(root="./", train=False, download=True, transform=ToTensor())
train_loader = DataLoader(train_dataset, batch_size=MY_BATCH, shuffle=True)  # batch_size = 128
test_loader = DataLoader(test_dataset, batch_size=MY_BATCH)

model = MyVIT(
    n_patches=MY_PATCH,        # 7
    n_encoder=MY_ENCODER,      # 2
    n_hidden=MY_HIDDEN,        # 8
    n_heads=MY_HEAD,           # 2
    n_class=MY_CLASS,          # 10
    image_shape=MY_SHAPE       # (1, 28, 28)
).to(DEVICE)

criterion = CrossEntropyLoss()
optimizer = Adam(model.parameters(), lr=MY_LEARNING)  # 학습률: 0.005

# 학습 기록용 리스트
train_accuracies = []

model.train()
start = time()

MY_EPOCH = 5

for epoch in range(MY_EPOCH):  # 총 5 에폭
    epoch_loss = 0
    correct = 0
    total = 0

    for batch_idx, (images, labels) in enumerate(train_loader):
        images, labels = images.to(DEVICE), labels.to(DEVICE)  # GPU로 전송
        outputs = model(images)  # 출력 shape: [128, 10]
        loss = criterion(outputs, labels)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        epoch_loss += loss.item()
        preds = outputs.argmax(dim=1)
        correct += (preds == labels).sum().item()
        total += labels.size(0)

    accuracy = 100 * correct / total
    train_accuracies.append(accuracy)
    print(f"[Epoch {epoch+1}] Loss: {epoch_loss:.4f} | Accuracy: {accuracy:.2f}%")

print("총 학습 시간:", round(time() - start, 2), "초")

# 정확도 그래프 출력
plt.plot(range(1, MY_EPOCH + 1), train_accuracies, marker='o')
plt.title("Training Accuracy over Epochs")
plt.xlabel("Epoch")
plt.ylabel("Accuracy (%)")
plt.grid(True)
plt.show()

# 테스트 평가
model.eval()
correct = 0
total = 0

with torch.no_grad():
    for images, labels in test_loader:
        images, labels = images.to(DEVICE), labels.to(DEVICE)
        outputs = model(images)
        preds = outputs.argmax(dim=1)
        correct += (preds == labels).sum().item()
        total += labels.size(0)

print(f"[Test Accuracy] {100 * correct / total:.2f}%")
탁월하게 정리하셨습니다! 이 **7번 블록**은 지금까지 구축한 Vision Transformer(`MyVIT`) 모델을 **MNIST 데이터셋에 대해 학습하고 평가**하며, **정확도 그래프까지 시각화**하는 전체 학습 파이프라인입니다. 아래에서 **각 단계별 기능**, **실행 흐름**, 그리고 **보완할 수 있는 팁**까지 차근차근 설명드리겠습니다.

---

## 🧱 전체 구조 요약

### 구성 요소:

1. **데이터 준비**: MNIST, `DataLoader`
2. **모델 정의 및 초기화**
3. **손실함수 + 옵티마이저**
4. **학습 루프**
5. **학습 정확도 기록 및 시각화**
6. **테스트 평가**

---

## ① 📥 데이터 준비

```python
train_dataset = MNIST(..., transform=ToTensor())
train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)

test_dataset = MNIST(..., transform=ToTensor())
test_loader = DataLoader(test_dataset, batch_size=128)
```

* `ToTensor()`로 `[0, 255] → [0.0, 1.0]` 정규화됨
* `shuffle=True`로 학습 데이터 순서 무작위
* `batch_size=128`: 128개씩 처리 (하드웨어 상황에 따라 조정 가능)

---

## ② 🧠 모델 생성

```python
model = MyVIT(...).to(DEVICE)
```

* 앞서 정의한 `MyVIT` 클래스 인스턴스를 GPU/CPU에 올림
* 하이퍼파라미터는 2번 블록과 일관되게 가져옴

---

## ③ 🎯 손실 함수 & 옵티마이저

```python
criterion = CrossEntropyLoss()
optimizer = Adam(model.parameters(), lr=0.005)
```

* `CrossEntropyLoss`: 다중 클래스 분류에 적합
* `Adam`: 자동으로 학습률과 모멘텀 조정
* 학습률 `0.005`는 약간 큰 편 → 학습 안정성 모니터링 필요

---

## ④ 🔁 학습 루프

```python
for epoch in range(MY_EPOCH):
    ...
    for batch_idx, (images, labels) in enumerate(train_loader):
        ...
```

* **입력과 정답**을 GPU로 전송
* `model(images)`로 예측값 출력
* 손실 계산 → 역전파 → 파라미터 업데이트
* `accuracy = correct / total`로 정확도 계산

📌 정확도와 손실 출력:

```python
print(f"[Epoch {epoch+1}] Loss: {epoch_loss:.4f} | Accuracy: {accuracy:.2f}%")
```

📝 `train_accuracies`에 각 epoch의 정확도를 저장해 시각화에 활용

---

## ⑤ 📊 정확도 시각화

```python
plt.plot(...)
plt.title("Training Accuracy over Epochs")
...
plt.show()
```

* 각 에폭마다 모델이 얼마나 잘 학습되었는지 시각적으로 확인 가능
* 학습이 안정되었다면 정확도가 점점 증가하는 추세를 보여야 함

---

## ⑥ 🧪 테스트 평가

```python
model.eval()
with torch.no_grad():
    for images, labels in test_loader:
        ...
```

* `eval()` 모드로 전환 → Dropout/BatchNorm 등 비활성화
* `torch.no_grad()`는 불필요한 그래디언트 계산 방지 → 더 빠르고 메모리 절약
* 최종 테스트 정확도 출력:

```python
print(f"[Test Accuracy] {100 * correct / total:.2f}%")
```

---

## ✅ 전체 실행 흐름 요약

```text
이미지 → 패치화 → 임베딩 → 인코더 스택 → CLS 토큰 → 분류기 → 예측
```

| 단계     | 내용                       |
| ------ | ------------------------ |
| 학습 데이터 | MNIST 6만 개               |
| 평가 데이터 | MNIST 1만 개               |
| 결과     | 에폭별 학습 정확도 + 최종 테스트 정확도  |
| 출력     | 학습 로그 + 정확도 그래프 + 테스트 성능 |

---

## 💡 향후 확장 팁

1. **Validation set 분리**:

   * 현재는 테스트셋을 직접 평가용으로 사용 → 실무에서는 별도의 검증셋을 사용하는 것이 일반적입니다.

2. **손실 시각화**:

   * 정확도 뿐만 아니라 `loss`도 리스트로 저장해 같이 그려보면 학습 안정성을 더 잘 파악할 수 있습니다.

3. **Confusion Matrix 출력**:

   ```python
   from sklearn.metrics import confusion_matrix
   import seaborn as sns
   sns.heatmap(...)
   ```

4. **데이터 증강 추가** (예: `RandomRotation`, `RandomCrop`) → `transforms.Compose([...])`

5. **모델 저장**:

   ```python
   torch.save(model.state_dict(), "vit_mnist.pth")
   ```

---

## 🏁 결론

이제 전체 Vision Transformer 학습 파이프라인이 완성되었습니다! 매우 잘 구성되어 있으며, 아래와 같은 목적에 모두 충실합니다:

* Transformer 구조 학습
* 직접 구현을 통한 원리 이해
* 학습 및 평가 루프 구성
* 시각화를 통한 학습 추이 확인

이제 추가적으로 실험 결과 정리, 비교 실험, 성능 개선 등을 원하신다면 도와드릴 수 있습니다. 필요하신 다음 방향을 알려주세요!
