CNN: 특징 분류
transfer Learning : 기존에 학습에서 다른걸 학습하는거
DETR: 매타에서 만듬

벡터(Vector): 벡터는 크기와 방향을 가지는 양을 나타내는 개념.
크기는 양수로 표현.
벡터의 방향은 양의 방향과 음의 방향을 가질 수 있다
벡터는 특정 좌표계에서의 위치를 나타내며,
각 좌표는 벡터의 성분
일반적으로 2차원 또는 3차원 좌표계에서 벡터는 (x, y) 또는 (x, y, z)와 같이
좌표로 표현. 좌표는 벡터의 방향과 크기를 결정
벡터는 열벡터(1*N)/행벡터(N*1)의 형태.
스칼라, 행렬과 구분된다.
행렬과 구분되며 열벡터, 행벡터로 표현
전치: 행 벡터를 열벡터로 열벡터를 행벡터로 바꾸는 연산
벡터r의 크기/길이는 |r|로 표기. 피타고라스 정리로 구한다.

행렬(Matrix): 행렬은 n 차원 배열로 구성된 숫자의 집합.
행렬은 데이터의 표현과 변환을 위해 널리 사용.
벡터의 차원은 요소의 개수에 의해 결정,
행렬의 차원은 행과 열의 수에 의해 결정.
행렬은 n 차원 배열로 구성된 숫자의 집합.
•행의 개수: 데이터의 개수를 의미.
행의 개수는 데이터셋의 크기를 나타내는 지표.
•열의 개수: 데이터 포인트의 특성 또는 속성(기하학: 그래프의 축/차원)
단위행렬(곱해진 임의의 행렬, 자신이 나오게 한다)
역행렬(단위 행렬이 나오게 하는 역행렬)
전치행렬(행과 열을 서로 맞바꾼 행렬)

어텐션: 토큰애 대해 3개의 임의변수 QKV를 발생시킨다?
매우 중요한 질문입니다. "어텐션(attention)" 메커니즘에서 **Q, K, V (Query, Key, Value)** 벡터를 생성한다는 말은 맞습니다. 하지만 이들을 "임의의 변수"라고 표현하기보다는 **학습 가능한 파라미터를 통해 생성된 벡터**라고 이해하는 것이 정확합니다. 아래에 개념을 체계적으로 설명해 드리겠습니다.

---

## 1. 어텐션 메커니즘이란?

어텐션은 입력 시퀀스의 각 요소가 \*\*다른 요소들과의 관계(중요도)\*\*를 동적으로 계산해, 더 중요한 정보에 집중할 수 있게 해주는 메커니즘입니다.

예: 문장에서 "그녀는 사과를 먹었다" → "먹었다"는 "사과"와 높은 연관을 가짐

---

## 2. Q, K, V는 무엇인가?

입력 벡터(= 토큰 임베딩)를 가지고 아래 3개의 벡터를 \*\*선형변환(linear transformation)\*\*을 통해 생성합니다:

* **Q (Query)**: 내가 어떤 정보를 찾고 싶은지 표현
* **K (Key)**: 각 토큰이 어떤 정보인지 표현
* **V (Value)**: 실제 정보 (문맥)

```python
Q = X @ W_Q
K = X @ W_K
V = X @ W_V
```

여기서:

* `X`: 입력 임베딩 (예: 각 단어를 768차원 벡터로 표현)
* `W_Q, W_K, W_V`: 각각의 가중치 행렬 (학습 대상)

즉, Q, K, V는 "임의"로 만들어지는 것이 아니라, **입력으로부터 학습된 가중치에 의해 계산된 결과**입니다.

---

## 3. 어텐션 스코어 계산 방법

어텐션 스코어는 다음과 같이 계산됩니다:

```
Attention(Q, K, V) = softmax(Q × Kᵗ / √d_k) × V
```

* `Q × Kᵗ`: Query와 Key의 유사도 (즉, 얼마나 관련 있는지)
* `softmax`: 관련도를 확률처럼 해석
* 그 확률로 V를 가중합해서 **최종 문맥 벡터 출력**

---

## 4. Q, K, V는 왜 필요한가?

* 문장 속 각 단어가 **다른 단어를 얼마나 참고할지(중요도)** 계산할 때, Q와 K가 비교 대상이 됩니다.
* 참고된 값을 계산할 때는 V를 사용합니다.
* 이 과정은 **자연어의 문맥, 순서, 의미**를 모델이 동적으로 이해할 수 있게 도와줍니다.

---

## 5. 요약 정리

| 항목        | 설명                          |
| --------- | --------------------------- |
| Q (Query) | 현재 토큰이 찾고 싶은 정보             |
| K (Key)   | 각 토큰이 가진 정보의 정체             |
| V (Value) | 실제 정보 (문맥)                  |
| 생성 방법     | 입력 임베딩에 가중치 행렬 곱해서 생성 (학습됨) |
| "임의"인가?   | ❌ 무작위 아님, ⭕ 학습을 통해 생성됨      |

---


행렬(Matrix): 선형 방정식과 연립 방정식을 표현 가능
선형 방정식은 변수들 간의 일차식으로 이루어진 방정식.
선형 방정식을 행렬과 벡터로 나타낼 수 있다.
연립 방정식은 여러 개의 선형 방정식으로 이루어진 방정식의 집합

(행*열) *(행*열) : 앞 행렬의 열과 뒤 행렬의 행의 숫자가 같아야 한다.
(행*열) *(행*열) : 행렬의 곱은 앞 행렬의 행과 뒤 행렬의 열의 크기를 갖는다
=> (5 * 3) * (3*4) = 5 * 4

1. A: 3x2 행렬 (특성: 길이, 너비)
2. B: 2x1 벡터 (특성: 가격)
3. C = A * B
4. 결과: C는 3x1 벡터 (특성: 길이가격, 너비가격)
2.행렬과 행렬 간의 곱:
1. A: 4x3 행렬 (특성: 무게, 길이, 너비)
2. B: 3x2 행렬 (특성: 가격, 수량)
3. C = A * B
4. 결과: C는 4x2 행렬 (특성: 무게가격, 길이가격, 너비가격, 무게수량, 길이수량, 너비수량)
3.행렬과 행렬 간의 곱 (브로드캐스팅 사용):
1. A: 2x2 행렬 (특성: 높이, 너비)
2. B: 1x2 행렬 (특성: 비율1, 비율2)
3. C = A * B
4. 결과: B가 브로드캐스팅되어 2x2 행렬로 확장되어 계산됨,
C는 2x2 행렬 (특성: 높이비율1, 너비비율1, 높이비율2, 너비비율2)

1. A: [[2, 3], [4, 5], [6, 7]] (특성: 길이, 너비)
2. B: [[10], [20]] (특성: 가격)
3. C = A * B
C = [[ 80],
     [140],
     [200]]

1. A: [
    [1, 2, 3], 
    [4, 5, 6], 
    [7, 8, 9], 
    [10, 11, 12]] (특성: 무게, 길이, 너비) 4행 3열

2. B: [
    [2, 3], 
    [4, 5], 
    [6, 7]] (특성: 가격, 수량) 3행 2열
3. C = A * B

C = [[ 28,  34],
     [ 64,  79],
     [100, 124],
     [136, 169]]

1. A: [[3, 4], 
        [5, 6]] (특성: 높이, 너비)
2. B: [
    [0.5, 0.2]] (특성: 비율1, 비율2)
3. C = A * B

Bᵗ = [[0.5],
      [0.2]]
C = [[2.3],
     [3.7]]

항목 /벡터 (Vector) /행렬 (Matrix)/ 텐서 (Tensor)
차원 수 (Dim) /1차원 (1D) 2차원 (2D) /3차원 이상 (3D, 4D, ...)
Shape 예시/ (3,) /(3, 2)/ (2, 3, 4)
구조 형태/ [2, 4, 6] /[[1, 2], [3, 4], [5, 6]]/ [[[...], [...], [...]], ...]
데이터 의미 /1개의 방향성 데이터/ 행과 열로 구성된 표 형식/ 여러 행렬 또는 다차원 데이터 묶음
대표적 예시/ 속도, 가중치 벡터/ 이미지의 픽셀값, 변환 행렬 /배치된 이미지, 문장, 영상 데이터
활용 분야 /선형대수, NLP /회귀분석, CNN 필터 등/ 딥러닝 전반 (CNN, RNN, Transformer)

내적(Dot product): 두 벡터의 유사성을 측정하는 기준
두 벡터의 각도값(cosine)을 곱함으로써
방향에 대한 유사성을 포함하게 된다.
코싸인에 의해서 방향성에 유사성을 측정한다 1이면 유사 90도면 전혀 안맞는다?
코싸인을 써서 두 위치를 고유하게 써서 문장마다 다른 고유한 위치를 가져 패턴을 가지는데 그때 코싸인 싸인을써서 학습을한다?

---

## 1. **코사인 유사도는 방향의 유사성을 측정하는 방법이다**

* 코사인 유사도는 \*\*두 벡터 사이의 각도(방향)\*\*를 비교하여, 얼마나 비슷한 방향을 가리키는지를 나타낸다.
* 계산식은 `cos(θ) = (A · B) / (‖A‖‖B‖)` 이며, 결과값은 -1에서 1 사이이다.

  * 1에 가까울수록 거의 같은 방향 → **매우 유사**
  * 0에 가까울수록 90도 → **전혀 다른 방향**
  * -1은 반대 방향

따라서 **벡터가 얼마나 유사한지를 방향 기준으로 측정**하고 싶을 때 사용된다.

---

## 2. **문장은 벡터로 표현되며, 이 벡터는 고유한 위치 정보를 가진다**

자연어처리(NLP)에서는 문장이나 단어를 **고정된 차원의 벡터로 변환**하는데, 이를 임베딩(embedding)이라고 한다.
예: "나는 학교에 간다" → `[0.2, -0.3, ..., 1.1]` 와 같은 벡터로 변환

* 문장마다 의미가 다르므로 **다른 방향과 크기의 벡터**를 가지게 된다.
* 이때 **문장 간 유사성은 벡터 간 코사인 유사도**로 측정할 수 있다.

  * 예: "나는 밥을 먹었다"와 "나는 점심을 먹었다"는 유사한 의미 → 벡터 방향이 비슷 → 코사인 유사도 ↑

---

## 3. **코사인 유사도를 신경망이 학습하는가?**

* 딥러닝 모델 자체는 **코사인 유사도를 직접적으로 학습하지는 않는다.**
* 대신, 모델은 문장을 벡터로 바꾸는 \*\*패턴(파라미터)\*\*을 학습한다.
* 학습이 끝난 후, 우리가 벡터 간 의미 유사성을 확인할 때 **코사인 유사도**를 주로 사용한다.
* 일부 특수한 모델 (예: 쿼리-문서 검색, 정보 검색, Siamese network)은 코사인 유사도를 \*\*손실 함수(loss)\*\*에 포함해서 **직접 학습**하기도 한다.

---

## 요약 정리

1. **코사인 유사도**는 벡터 방향의 유사성을 측정하는 방법이다.
2. 문장은 고유한 \*\*벡터 위치(임베딩)\*\*로 표현되며, 이는 신경망이 학습을 통해 만든다.
3. 학습 후 **문장 간 의미 비교**에 코사인 유사도를 사용하면, 문장의 의미적 유사성을 측정할 수 있다.
4. 일반 딥러닝에서는 코사인을 직접 학습하지 않지만, **유사도 기반 학습 모델에서는 활용되기도 한다.**

---


Cosine 유사도 사용 사례
1) 문서 유사도 분석 (NLP)
벡터 또는 문장 임베딩 벡터 간 유사도 계산
2) 문장 임베딩 기반 검색
sentence-transformers, BERT로 임베딩한 후 cosine 유사도 계산
사용 예: 유사 질문 찾기, 뉴스 본문 유사도
3) 이미지 검색 / 시각 특징 비교
이미지의 CNN 특징 벡터 사이의 cosine 유사도 계산
4) 추천 시스템 : 사용자와 아이템을 임베딩한 후 유사도 기반으로 추천
5) 클러스터링, 분류, 군집 해석
군집 내 평균 벡터와 개별 벡터 간 cosine 유사도 계산
6) 의미 기반 검색 시스템 (Semantic Search)
문서, 질문, 상품명 등을 벡터로 임베딩한 후 cosine 유사도 순으로 정렬
Pinecone, FAISS 등에서도 cosine distance로 최근접 이웃 검색 수행

Regularization : 정규화 ??, 규제화...
Regularization의 주목적 : 모델의 오버피팅(Overfitting)을 줄이고자 하는 것.
모델의 일반화 성능을 높이는 방법 중의 하나 :
Data augmentation(데이터 증강), Ensemble model(앙상블), Dropout(노드
누락),, BatchNormalization(배치 정규화), Early stopping(조기 종료), MultiTask learning 등
L1, L2 Regularization : Weight 중 일부가 특정 변수에 극단적으로 의존하는 상황
특정 Weight가 과도하게 커지지 않게끔 제한(regularization)을 걸어주는 역할
입력 데이터의 스케일을 조정하거나, 데이터의 분포를 조정하는데 사용.
•예를 들어, 모든 입력 특성을 평균이 0이고 표준편차가 1이 되도록 스케일을 조정
StandardScaler, MinMaxScaler.
• 배치 정규화(Batch Normalization)는 각 레이어의 입력 분포를 정규화하여, 학습
과정을 안정화하고 학습 속도를 개선하는 기법입니다

배치 일괄처리 예) 백만개를 짧게 잘라서 데이터를 넣는다

Normalization :
입력 데이터의 스케일을 조정하거나, 데이터의 분포를 조정하는데 사용.

미분(Differentiation): 미분은 함수의 변화율을 나타내는 개념.
함수의 도함수를 계산하여 어떤 지점에서의 순간 변화율을 구할 수 있다.
머신러닝에서는 손실 함수의 기울기(그래디언트)를 계산하고,
이를 최적화 알고리즘에 활용하여 모델을 학습. x가 변할때 y의 순간 변화율을 기울기로

상수 미분 = 0
자수 미분 = nx^n-1
합성함수 미분: 앞에 미분과 뒤에 미분 더한다
연쇄 법칙: 함수의 x 값이 다음 뉴런에 x 값이 들어가고 최초의 x 값이 계속해서 쌓여가며 값이 바뀐다? 끝에서는 n개의 겹쳐진 끝에 값을 도출해냄 그 값을 미분해서 점점 바꿔봐서 다시한번 밀어넣는다 순정파 역전파

적분(Integration): 적분은 함수의 면적을 구하는 개념.
머신러닝에서는 적분을 사용하여 확률 분포 함수의 면적 계산이나
변수 간의 상관 관계 계산 등에 활용될 수 있다

연쇄 법칙(Chain Rule): 합성함수(composite function)를 미분하는 규칙.
합성함수란, 함수 f와 g가 있을 때, f(g(x))와 같이 하나의 함수 안에
또 다른 함수가 있는 경우.
연쇄 법칙(Chain Rule): 딥러닝에서 오차함수의 최적화를 위해 역전파를 시행할 때
연쇄법칙에 의한 미분값(가중치)를 계산하게 된다
균등분포를 통해 어느정도 가능성이 있는 가중치를 최초에 설정함
연쇄 법칙(Chain Rule): 딥러닝에서 오차함수의 최적화를 위해 역전파를 시행할 때
연쇄법칙에 의한 미분값(가중치)를 계산하게 된다
각 층 = 양파의 껍질: 딥러닝의 각 층은 양파의 껍질과 같이 순차적으로 연결
함수 합성 = 양파 성장: 각 층은 이전 층의 출력을 입력으로 받아 새로운 출력을
만들어내는 함수와 같다.
미분 = 영향력 분석: 최종 결과에 대한 각 층의 영향력을 분석하는 것이 미분의 목표
체인 룰 = 껍질 벗기며 영향력 전파: 가장 바깥쪽 껍질부터 시작하여 안쪽 껍질로
순차적으로 이동하며 각 껍질의 영향력을 계산하고, 이를 곱하여 최종 결과에 대한 각
층의 전체적인 영향력을 파악하는 방식
이는 양파 껍질을 하나씩 벗겨나가며 각 껍질이 전체 크기에 얼마나 기여했는지
분석하는 것과 논리적으로 같습니다.
모델의 예상결과와 실제 결과의 차이(Loss)를 옵티마이저를 통해 최적화하는 과정
옵티마이저: 더쉽고 빠르게 가중치를 찾는 방법
 에포크(epochs) :
준비된 데이터가 한 번씩 순전파(forward pass)와 역전파(backward pass)를 통해
학습되는 과정을 포함.
이 과정을 통해 모델의 가중치가 업데이트되고, 에포크를 반복하여 모델의 성능을 향상.
에포크의 수는 학습의 반복 횟수이며, 학습 시간과 모델의 성능에 큰 영향을 준다

 최적화(Optimization)
목적함수(손실함수)의 최솟값이나 최댓값을 찾는 과정. 머신러닝에서는 모델의
손실 함수를 최소화하는 매개변수 값을 찾기 위해 최적화 알고리즘을 사용.
최적화 알고리즘은 경사하강법(Gradient Descent)과 그 변형들이 주로 사용.

경사하강법(Gradient Descent):
최적화 알고리즘 중 가장 기본적이고 널리 사용되는 방법.
목적 함수의 기울기(그래디언트)를 사용하여 매개변수를 반복적으로 조정하면서
목적 함수의 결과값을 최소화하는 방향으로 진행.

경사하강법은 인공신경망에서 출력값과 정답 사이의 오차를 점점 줄이기 위해 사용하는 최적화 기법이다.
모델은 입력을 받아 순전파(forward pass)를 통해 출력을 계산하고, 그 출력과 정답 사이의 오차(loss)를 측정한다.
이후 역전파(backpropagation)를 통해 이 오차가 각 레이어의 가중치에 얼마나 영향을 받았는지를 계산하고, 그 정보를 바탕으로 가중치를 업데이트해나간다.

이때 미분은 핵심적인 역할을 한다.
미분은 어떤 값(예: 가중치)을 아주 조금 바꿨을 때, 손실 값이 얼마나 변하는지를 계산하는 수학적 도구이다.
즉, 현재 위치에서 어느 방향으로 움직이면 손실이 줄어들지를 알려주는 **기울기(gradient)**를 제공하며,
경사하강법은 이 기울기를 따라 가장 빠르게 손실이 줄어드는 방향으로 파라미터를 조정한다.
이러한 기울기를 구하기 위해 각 레이어의 출력에 대해 손실 함수를 미분하는 과정이 필수적으로 포함된다.

이때 연쇄법칙(Chain Rule)은 각 레이어를 통과한 값들이 서로 어떻게 연결되어 있는지를 수학적으로 미분해주는 방식으로, 역전파에서 핵심적인 역할을 한다.
연쇄법칙 덕분에 출력에 대한 최종 오차가 입력층부터 시작해 모든 레이어를 거슬러 올라가며 분해되어, 각 가중치가 결과에 얼마나 영향을 주었는지 계산할 수 있다.

이러한 과정을 반복하면서 모델은 점점 오차를 줄이고, 최종적으로 손실 함수가 최소가 되는 지점을 찾아간다.
이 과정을 손실 곡면 위를 내려가는 것에 비유할 수 있다.

이 과정은 마치 통신 시스템에서 고주파는 빠르지만 장애물을 통과하기 어렵고, 반대로 저주파는 느리지만 안정적인 것처럼,
모델도 너무 급격한 변화는 위험하고, 적절한 방향과 속도를 조절하면서 최적점을 찾아야 하는 과정이다.
또, 전파가 환경에 따라 영향을 받듯, 역전파에서는 각 가중치가 결과에 미치는 영향을 분석하며 각 지점에서 어떤 변화가 중요한지를 판단한다.
이처럼 신호가 목표에 도달한 뒤, 그 경로를 되짚어가며 어디서 왜 손실이 발생했는지를 따져보는 것이 경사하강법과 연쇄법칙, 미분, 그리고 역전파의 핵심적인 흐름이다.


학습률(learning rate): 보통 0.001 또는 1e-3
학습률은 모델이 학습할 때마다 얼마나 크게 또는 작게 자신의 지식(모델의 파라미터,
즉 가중치와 편향)을 업데이트할지를 조절하는 변수.
산을 내려갈 때 ＇보폭＇과 같다.
너무 큰 보폭은 최저점을 지나칠 수 있고 너무 작으면 시간이 많이 걸린다.
옵티마이저(optimizer)
모델의 가중치와 편향을 최적화하는 역할. 옵티마이저는 손실 함수를 최소화하기 위해
가중치와 편향을 업데이트하는 방식을 결정하며, 학습 속도와 수렴 품질에 영향.

1) 확률적 경사 하강법(SGD : Stochastic Gradient Descent):
• 등산자가 각 단계마다 가장 가파른 경사를 따라 내려오는 것과 비슷합니다.
• 경사면이 가파르기 때문에 빠르게 하산할 수 있지만, 직진하지 않고 좌우로 흔들리거나
노이즈에 민감할 수 있습니다.
2) 모멘텀 최적화(Momentum Optimization):
• 등산자가 이전에 움직인 방향과 속도를 기억하면서 내려오는 것과 비슷합니다.
• 이전에 이동한 방향과 크기를 고려하기 때문에 경사면이 완만한 지역에서도 좀 더 일정한
속도로 내려오며, 지역 최소점에서 빠져나올 수 있습니다.
3) 아담(Adam: Adaptive Moment estimation): * 일반적으로 많이씀
• 등산자가 학습 경로에서 학습률을 조정하면서 내려오는 것과 비슷합니다.
• 등산자는 학습 경로에서 현재 위치에서의 기울기를 계산하고, 이동 평균을 사용하여
학습률을 동적으로 조절합니다.
• 학습 경로의 특성에 따라 학습률을 적절하게 조절하면서 목표 지점으로 향하며,
안정적으로 수렴할 수 있습니다.
4) 애다그라드(Adagrad: Adaptive Gradient Algorithm):
• 등산자가 경사면의 특성에 따라 학습률을 조정하면서 내려오는 것과 비슷합니다.
• 경사면이 높은 지점에서는 학습률을 감소시켜 더 조심스럽게 이동하고, 경사면이 낮은
지점에서는 학습률을 증가시켜 더 빠르게 이동합니다

볼록 최적화(Convex Optimization):
목적 함수가 볼록 함수인 경우에 적용되는 최적화 방법.
볼록 함수는 특정 조건을 만족하며, 이러한 함수에서의 최소값은 전역 최소값. 

활성화 함수(activation function)
딥러닝에서 활성화 함수(Activation Function)는 뉴런의 출력 값을 결정하는 함수
뉴런의 입력에 대해 어떤 값이 출력될지 결정('활성화’)
1) 딥러닝 모델이 비선형 문제를 해결하는 데 중요한 역할
비선형 활성화 함수를 사용함으로써 모델은 더 복잡한 패턴을 학습할 수 있다.(Relu)
2) 출력 범위 제한: 뉴런의 출력값을 특정 범위 (예: 0과 1 사이, -1과 1 사이)로 제한하여
모델의 안정적인 학습을 돕습니다. 예를 들어, 이미지가 특정 클래스에 속할 확률을
나타낼 때 0과 1 사이의 값으로 출력하는 데 유용합니다.(sigmoid, softmax)
3) 의사 결정: 특정 임계값을 기준으로 출력을 조절하여 뉴런이 활성화될지 여부를
결정합니다. 이는 모델이 중요한 특징을 선택하고 무시할 특징을 걸러내는 데 도움을
줍니다.

활성화 함수
1)시그모이드(Sigmoid) 함수: 함수의 출력 값은 0과 1 사이.
이진 분류 문제의 출력층에서 사용. 그래디언트 소실 문제 발생.
2)하이퍼볼릭 탄젠트(tanh) 함수: 출력 범위가 -1에서 1사이
시그모이드 함수보다 더 넓은 출력 범위.
3)렐루(ReLU) 함수: 입력이 양수일 때는 입력 값을 그대로, 음수일 때는 0을 반환.
간단하면서 효과적인 함수로, 현재 가장 널리 사용
4)Leaky ReLU: ReLU의 변형, 음수 입력에 대해 아주 작은 기울기를 가진다
ReLU의 "죽은 뉴런" 문제를 해결할 수 있다.
5) 소프트맥스(Softmax) 함수: 다중 클래스 분류 문제의 출력층에서 사용
각 클래스에 대한 확률을 출력
각 클래스에 대한 출력값들의 합은 1

Relu 함수 : 활성화 함수
가장 많이 사용되는 활성화 함수, 입력값이 0보다 작으면 0으로 출력하고, 0보다 크면 그
값을 그대로 출력. 음수 값을 제거하고 양수 값을 유지하는 역할. 
함수는 비선형성을 추가하여 CNN이 복잡한 패턴과 특징을 학습할 수 있도록 도와준다. 

SoftMax
신경망의 출력층에서 사용되며, 그 결과로 확률 분포를 생성
분류 문제에서 클래스의 확률을 예측하는 데 사용
소프트맥스 함수의 핵심은 입력받은 값(logit)을 확률로 변환
모든 입력 값을 지수 함수를 사용하여 '확률'로 변환, 확률들의 합이 1
각 클래스에 대한 확률 분포를 얻을 수 있다.
1.범위: 소프트맥스 함수의 출력은 항상 0과 1 사이, 모든 출력 값의 합은 항상 1
2.비선형성: 소프트맥스 함수는 비선형 함수.

최적화는 머신러닝과 딥러닝에서 손실 함수의 값을 
최소화하거나 최대화하기 위해 사용하는 과정이다.
 이 과정의 핵심 목적은 모델이 예측한 값과 실제 값 사이의 오차를 줄이는 것이다. 
이를 위해 경사하강법과 같은 최적화 알고리즘이 사용되며, 
현재는 그 변형인 확률적 경사하강법(SGD), Adam, RMSprop 등이 널리 활용된다.

경사하강법은 손실 함수의 기울기, 즉 미분값을 계산해 그 방향으로 파라미터를 
조금씩 조정해 나가는 방식이다. 
미분은 함수의 변화율을 계산하여 오차가 어느 방향으로 줄어드는지를 알려주는 
중요한 수학적 도구이며, 
이 과정에서 연쇄법칙이 함께 사용된다.
 연쇄법칙은 복잡하게 연결된 여러 함수의 미분을 순차적으로 계산할 수 있게 해 주며, 
신경망의 역전파 과정에서 필수적으로 활용된다.

  이와 같은 최적화 알고리즘 중 가장 널리 사용되는 것 중 하나가 Adam이다. 
Adam은 경사하강법의 변형으로, 모멘텀과 적응형 학습률(adaptive learning rate)이라는 
두 가지 개념을 결합하여 더 안정적이고 빠른 학습을 가능하게 한다. 
모멘텀은 이전에 계산된 기울기의 방향을 기억하여 진동을 줄이고 더 빠르게 수렴하도록 돕는다. 
동시에, 각 파라미터마다 개별적으로 학습률을 조정하여, 
변화가 큰 파라미터에는 작은 학습률을, 변화가 적은 파라미터에는 큰 학습률을 적용함으로써 
학습 효율을 높인다.
 이러한 방식 덕분에 Adam은 초기 설정으로도 좋은 성능을 내는 경우가 많아, 
딥러닝 실무에서 매우 널리 사용된다.  


활성화 함수는 인공신경망에서 각 뉴런의 출력을 결정하는 역할을 한다. 
입력된 값이 어느 기준 이상일 때 뉴런이 활성화되도록 하여, 
모델이 중요한 정보를 통과시키고 불필요한 정보를 억제하는 데 도움을 준다.
 또한 비선형성을 모델에 부여함으로써 선형 모델이 해결할 수 없는 복잡한 문제까지도 처리할 수 있도록 해 준다.


자주 사용되는 활성화 함수에는 여러 종류가 있다.
  ReLU 함수는 가장 널리 사용되며, 
입력이 0보다 작으면 0을 출력하고, 0보다 크면 그대로 출력한다. 
연산이 간단하고 빠르며, 많은 모델에서 기본 설정으로 사용된다.
 Leaky ReLU는 ReLU에서 발생하는 ‘죽은 뉴런’ 문제를 해결하기 위해 고안된 함수로, 
음수 영역에서도 아주 작은 기울기를 유지해 학습을 가능하게 한다.
 Sigmoid 함수는 0과 1 사이의 값을 출력하여 이진 분류 문제에서 사용되지만, 
기울기 소실 문제가 있어 최근에는 주로 출력층에서만 쓰인다. 
Tanh 함수는 -1에서 1 사이의 값을 출력하며, Sigmoid보다 중심이 0에 가까워 더 나은 특성을 가진다.
 Softmax 함수는 다중 클래스 분류 문제에서 사용되며,
 각 클래스의 출력 값을 확률처럼 변환한다. 
모든 출력 값의 합은 1이 되며, 각 값은 해당 클래스에 속할 확률을 나타낸다.

완전 연결층(Fully Connected Layer) : FC
인공 신경망에서 가장 기본적인 층, 모든 입력 뉴런과 출력 뉴런이 서로 연결되어 있는 층.
각 입력 뉴런은 출력 뉴런과 가중치로 연결되어 있으며, 이 가중치는 학습 과정에서 조정
각 뉴런 사이의 연결은 선형 연산과 활성화 함수를 거쳐 진행된다.
선형 연산은 입력 뉴런의 값과 해당 연결의 가중치를 곱한 뒤, 모든 연결의 결과를 합산. 그리고 이 합산 결과는 활성화 함수로 전달되어 출력값을 생성.
모든 입력 뉴런과 출력 뉴런 사이의 연결에 대해 반복되며, 출력 뉴런의 최종값이 예측
결과로 사용.
완전 연결층은 입력과 출력 사이의 가중치를 학습하여
입력 특성과 클래스 사이의 복잡한 관계를 모델링.
입력 데이터를 분류하거나 예측하는 작업을 수행

배치 (Batch), 드롭아웃 (Dropout), 조기종료(Early Stopping)
모델의 효과적 학습을 돕는 여러 가지 기술들
배치(Batch): 데이터를 한 번에 조금씩 묶어서 학습시키는 방법.
마치 학생이 문제집을 한 페이지씩 풀어나가는 것.
드롭아웃(Dropout): 학습 과정에서 일부 뉴런을 무작위로 '잠시 꺼서' 모델이 특정
부분에만 너무 의존하는 것을 방지.
조기 종료(Early Stopping): 학습이 더 이상 개선되지 않으면 중간에 멈추는 방법
CNN(Convolution Neural Network) 
Convolution
이미지나 다차원 데이터에 대해 작은 필터를 이용하여 합성곱 연산을 수행하는 것.
필터는 입력 데이터의 일부 영역과 가중치를 곱한 값을 모두 더하여 출력을 생성. 필터를
이동하면서 전체 입력 데이터에 대해 합성곱 연산을 수행하면 특징 맵(Feature Map)이 생성.
Convolution은 입력 데이터의 패턴을 감지하고, 이미지에서 특징을 추출하는 역할을 수행.
또한, 합성곱 연산을 통해 필터의 가중치를 학습하여 이미지의 특징을 감지하는 더 나은
필터를 생성할 수도 있다.
convolution NN, 합성곱신경망
•특히 이미지 및 비디오 처리에 활용
•핵심 요소는 커널(convolution 연산시 입력 관련 기능 추출)이라는 필터
•입력 데이터의 특징을 추출하여 특징들의 패턴 파악하는 구조
•Convolution과정과 Pooling 과정으로 진행

목적 함수(Objective Function):
Cost function 비용함수(sum of loss function),
loss function 손실함수(model complexitry penalty for regularization),
error function 오차함수(for backpropation)

평균 제곱 오차 (Mean Squared Error, MSE):
실제 값과 예측 값 사이의 제곱 오차를 평균한 값. 회귀 문제에서 주로 사용.
큰 오차에 대해 패널티를 부여하고, 작은 오차에 대해 민감하게 반응.
교차 엔트로피 오차 (Cross Entropy Error, CEE):
분류 문제에서 예측 값과 실제 값의 차이를 측정.
이진 분류 : 이진 교차 엔트로피 오차(Binary Cross Entropy)가 사용되고,
다중 분류 : 범주형 교차 엔트로피 오차(Categorical Cross Entropy)가 사용.

딥러닝에서 사용되는 주요 오차함수
sparse categorical crossentropy 희소 범주형 크로스엔트로피
다중 클래스 분류 문제에서 주로 사용되며,
모델이 각 클래스에 대한 확률 분포를 예측하도록 학습할 때 사용
정수 형태의 클래스 레이블을 입력으로 사용
softmax Cross Entropy Loss 소프트맥스 크로스 엔트로피
다중 클래스 분류 문제에서 사용
softmax 활성화 함수와 함께 사용. 각 클래스에 대한 확률 분포
one-hot 인코딩된 클래스 레이블을 입력으로 사용

딥러닝의 손실 함수(loss function)
모델의 예측값과 실제값 사이의 차이를 측정하는 역할.
이를 통해 모델은 실제값과 최대한 일치하는 예측값을 만들어낼 수 있다.
1) 평균 제곱 오차(Mean Squared Error, MSE):
• 예측값과 실제값 사이의 거리를 제곱하여 계산하기 때문에, 오차가 커질수록 패널티가 크게
부여됩니다.(회귀 문제)
2) 교차 엔트로피 손실(Cross-Entropy Loss):
• 실제값의 확률 분포와 예측값의 확률 분포 사이의 교차 엔트로피를 계산.
• 분류 문제 (MNIST)
3) 이진 교차 엔트로피 손실(Binary Cross-Entropy Loss):
• 이진 분류 문제에서 사용되는 손실 함수로, 두 확률 분포 간의 차이를 측정.
• 이메일 스팸 분류
4) 희소 교차 엔트로피 손실(Sparse Cross-Entropy Loss):
• 다중 클래스 분류 문제에서 사용되는 손실 함수.
• 정수형태로 인코딩된 실제 레이블과 모델의 예측 확률 분포의 교차 엔트로피를 계산

1.평균 제곱 오차(Mean Squared Error, MSE): vs MAE/RMSE
1. 회귀 문제에서 자주 사용,
2. 예측값과 실제값 사이의 거리를 제곱하여 계산하기 때문에, 오차가 커질수록 패널티가 크게
부여됩니다.(회귀 문제)
3. 미분 계산의 편의성: 머신 러닝에서 모델의 학습은 주로 미분을 사용하여 손실 함수에
대한 모델의 파라미터의 그래디언트를 계산하는 과정으로 이루어 진다. MSE는 제곱
오차의 평균이므로 미분 계산 시 제곱 함수의 미분과 평균 함수의 미분이 서로
상쇄되는 특성을 가진다
4. 이상치의 영향 유무를 분석하고 오차 측정에서 이상치의 영향이 큰 경우 다른 오차
평가 모델을 선택. 이를 통해 모델의 신뢰성을 높이고 실제 데이터에 대한 적합성을
개선할 수 있다.

2.교차 엔트로피 손실(Cross-Entropy Loss): 이미지 분류, 자연어 처리, 생성 모델 등 다항분류에 사용
1. 엔트로피는 무질서를 나타내는 개념으로, 확률 분포에서의 불확실성이 얼마나 큰지를 측정
2. 교차 엔트로피 손실은 두 확률 분포 간의 유사도를 측정하는 방법 중 하나. 모델의 예측값과
실제값의 확률 분포를 비교하고 두 분포 사이의 차이를 계산
3. 만약 모델의 예측값이 실제값과 일치한다면, 두 분포 사이의 차이가 작아지고 엔트로피가 낮아진다.
이는 모델이 더욱 확실한 예측을 수행하고 더욱 정확한 분류를 이룰 수 있게 된다.
4. 만약 모델의 예측값과 실제값 사이에 큰 차이가 있다면, 두 분포 사이의 차이가 커지고 엔트로피가
높아진다. 모델은 더욱 불확실한 예측을 수행하고 분류 결과가 더욱 오류가 발생할 가능성이 크다
5. 교차 엔트로피 손실을 사용하여 모델을 학습시키는 경우, 모델은 실제값과 예측값의 확률 분포를
최대한 비슷하게 만들도록 학습하여 모델이 더욱 정확하고 확실한 예측을 수행하도록 유도

3.이진 교차 엔트로피 손실(Binary Cross-Entropy Loss):
1. 두 확률 분포 간의 차이를 측정.이메일 스팸 분류
2. "이진"은 두 가지 클래스(예를 들면, 스팸/일반 메일, 양성/음성 진단) 중 하나를 예측하는 문제.
"교차 엔트로피"는 예측값과 실제 레이블 사이의 엔트로피 차이를 계산하는 방법

4.희소 교차 엔트로피 손실(Sparse Cross-Entropy Loss):
1. 다중 클래스 분류 문제에서 사용되는 손실 함수.
2. 정수형태로 인코딩된 실제 레이블과 모델의 예측 확률 분포의 교차 엔트로피를 계산
3. "희소"라는 단어는 실제 레이블의 확률 분포가 주로 하나의 클래스에 집중되어 있을 때를 의미
희소(sparse)란?
정답 라벨을 원-핫 벡터(one-hot vector)로 변환하지 않고,
예: [0], [3]처럼 정수 인덱스로 직접 처리합니다.
→ 계산 속도와 메모리 사용을 줄여줍니다.

손실 함수(loss function)의 적용 프로세스
최종 활성화 함수의 예상결과와 실제 결과의 차이(Loss)를 옵티마이저를 통해
최적화

logit: 모델이 학습하고 특징을 뽑은 값

| 손실 함수                                      | 설명                                                               | 사용 사례                                  | 선택 기준                                                                     |
| ------------------------------------------ | ---------------------------------------------------------------- | -------------------------------------- | ------------------------------------------------------------------------- |
| **교차 엔트로피(Cross-Entropy)**                 | 다중 클래스 분류 문제에서 사용되는 손실 함수.                                       | 이미지 분류, 자연어 처리 등 다중 클래스 분류 문제          | 다중 클래스 분류 문제인지 확인                                                         |
| **이진 교차 엔트로피(Binary Cross-Entropy)**       | 이진 분류 문제에서 사용되는 손실 함수. 두 개의 클래스 간의 확률 분포 차이를 측정.                 | 이미지 스팸 분류, 감정 분석 등 이진 분류 문제            | 이진 분류 문제인지 확인, 각 클래스가 독립적일수록 적합. **클래스가 상호 배타적이지 않은 경우**에 적합              |
| **희소 교차 엔트로피(Sparse Cross-Entropy)**       | 다중 클래스 분류 문제에서 원-핫 인코딩이 아닌 **정수 형태로 레이블이 주어진 경우**에 사용되는 손실 함수    | 단어 분류, 감정 분석 등 자연어 처리 분야의 다중 클래스 분류 문제 | 다중 클래스 분류 문제인지 확인, **클래스 레이블이 정수 형태로 주어진 경우**                             |
| **범주형 교차 엔트로피(Categorical Cross-Entropy)** | 다중 클래스 분류 문제에서 **원-핫 인코딩된 클래스 레이블을 사용**하며 확률 분포 간 차이를 측정하는 손실 함수 | 이미지 분류, 자연어 처리 등 다중 클래스 분류 문제          | 다중 클래스 분류 문제인지 확인, **원-핫 인코딩 사용 여부 확인**, 클래스가 **서로 독립적이고 상호 배타적인 경우**에 적합 |


✅ 손실 함수(Loss Function)란?
딥러닝에서 손실 함수는 모델의 예측값과 실제값 사이의 차이를 정량적으로 계산하는 함수로, 모델이 얼마나 잘못 예측했는지를 측정한다.
손실 함수 값을 최소화하는 방향으로 파라미터를 조정함으로써, 모델은 점점 더 정확한 예측을 하도록 학습된다. 
✅ 주요 손실 함수 종류 및 특징 
1. 평균 제곱 오차 (MSE: Mean Squared Error)
사용 분야: 회귀 문제
정의: 예측값과 실제값의 차이를 제곱한 후 평균을 취함
특징:
오차가 커질수록 패널티가 기하급수적으로 커진다.
이상치에 민감하다.
미분 계산이 간단해 학습에 효율적이다.
이상치가 많은 경우 MSE 대신 MAE(절대 오차)나 RMSE(제곱근 평균 오차)를 고려할 수 있다. 
2. 교차 엔트로피 손실 (Cross-Entropy Loss)
사용 분야: 다중 클래스 분류, 이미지 분류, 자연어 처리, 생성 모델
정의: 실제값과 예측값의 확률 분포 사이의 교차 엔트로피를 계산
특징:
분포 간의 유사도 또는 차이를 정량화한다.
예측이 실제와 일치할수록 손실이 작아진다.
예측이 틀릴수록 손실이 커진다.
모델이 더 확실한 예측을 하도록 학습을 유도한다.
일반적으로 원-핫 인코딩된 레이블과 함께 사용된다. 
3. 이진 교차 엔트로피 손실 (Binary Cross-Entropy Loss)
사용 분야: 이진 분류 문제 (예: 스팸 메일 분류, 양성/음성 판별)
정의: 두 확률 분포(실제 레이블 vs 예측 확률) 간의 교차 엔트로피 계산
특징:
두 클래스 중 하나에 속하는 확률 예측을 학습한다.
출력층 활성화 함수로 sigmoid를 주로 사용한다.
예측이 확신에 찼으면서 틀릴 경우 손실이 크게 발생한다. 
4. 희소 교차 엔트로피 손실 (Sparse Cross-Entropy Loss)
사용 분야: 다중 클래스 분류 문제
정의: 정수형 인덱스 레이블과 예측 확률 분포의 교차 엔트로피를 계산
특징:
정답 레이블을 원-핫 벡터로 변환하지 않고 정수로 처리한다 (예: [0], [3] 등).
메모리 사용이 적고 계산 속도가 빠르다.
클래스 수가 많고 대부분의 값이 0인 경우 유리하다.
예측 확률 분포가 희소한 경우에 적합하다. 
✅ 손실 함수의 적용 과정 요약
모델은 logit이라는 예측 값을 출력한다.
logit은 아직 확률로 변환되지 않은 값이다.
이 logit 값은 **최종 활성화 함수(sigmoid, softmax 등)**를 거쳐 예측 확률로 바뀐다.
이 예측 확률과 실제 정답값을 비교하여 손실 함수가 **오차(loss)**를 계산한다.
계산된 손실은 옵티마이저를 통해 **역전파(backpropagation)**되어, 모델의 가중치가 오차를 줄이는 방향으로 업데이트된다. 
✅ 핵심 용어 정리
Loss (손실): 예측과 실제 값 사이의 오차
Logit: 모델 출력의 미가공 상태, 확률로 변환되기 전 값
Optimizer (최적화기): 손실을 줄이기 위해 가중치를 조정하는 알고리즘
Backpropagation (역전파): 오차를 역으로 전파해 각 가중치에 미치는 영향을 계산

자연어처리 역사(RNN>Transformer)
1. 기본 RNN (Recurrent Neural Network)
시퀀스 데이터를 처리하는 첫 딥러닝 구조
순차적으로 단어를 입력받고, 은닉 상태로 이전 정보를 기억
문제점
장기 의존성 문제: 앞쪽 정보가 뒤로 갈수록 사라짐 (Gradient Vanishing)
병렬화 불가능: 단어를 한 글자씩 순차적으로 처리해야 함
개선 방향 → LSTM / GRU 개발

2. LSTM / GRU
기억 게이트를 도입해 중요 정보는 오래 기억, 덜 중요한 건 잊음
RNN의 장기 의존성 문제 완화
문제점
문장 전체를 하나의 벡터에 담는 구조는 여전히 유지
→ 정보 압축 손실, 여전히 순차 처리만 가능
개선 방향 → Seq2Seq 모델로 확장

3. Seq2Seq (Encoder-Decoder)
Encoder가 전체 입력 시퀀스를 벡터(context)로 압축
Decoder는 그 벡터를 보고 출력 시퀀스를 생성 (예: 번역)
문제점
고정된 context 벡터에 전체 정보를 압축하려다 보니, 긴 문장에서는 성능 급감
출력이 모든 단어에 대해 동일한 context만 참고
개선 방향 → Attention 도입

4. Seq2Seq + Attention
Decoder가 출력 단어를 예측할 때 입력 단어 전체에 다른 가중치를 부여
Q = Decoder 상태, K/V = Encoder 상태 → 중요 단어에 집중 가능
문제점
여전히 순차 처리 기반 (RNN 구조) → 느림
학습 및 병렬화가 제한적
개선 방향 → Self-Attention만 사용하는 구조 개발 → Transformer

5. Transformer (self attention)
Encoder/Decoder 모두 Self-Attention + Feedforward 구성
Positional Encoding으로 순서 정보 보완
장점
병렬 처리 가능 → 학습 속도 비약적 증가
문장 내 모든 단어 간 관계를 한 번에 계산
긴 문장에서도 성능 우수

임베딩: 정해진 차원으로 토큰을 뿌린다 Q,K,V를 가중치에 의해 만듬 Q,K를 전치행렬로 곱하고, 마지막 V를 곱해서 결과를 만듬?

Seq2Seq(Sequence-to-Sequence) vs Attention
Attention은 Seq2Seq 모델의 성능을 보완하기 위해 추가된 메커니즘
Seq2Seq 구조
입력 문장 → Encoder → 고정 벡터 context → Decoder → 출력 문장
인코더는 전체 입력을 하나의 벡터(context)로 압축
디코더는 이 context만을 보고 모든 단어를 생성함
예: "나는 학교에 간다" → “I go to school”
문제점: 긴 문장일수록 context 벡터 하나에 모든 정보를 담기 어려움

구분 Seq2Seq Seq2Seq + Attention
입력 정보 처리 고정된 벡터로 압축 매 시점마다 입력 전체를 재조합
문장 길이 처리 길어질수록 성능 저하 긴 문장에서도 중요한 단어에 집중 가능
병렬화 느림 (RNN 기반) Attention은 빠르게 병렬화 가능 (특히 Transformer)

항목 Seq2Seq Attention
목적 입력 시퀀스를 → 출력 시퀀스로 변환 출력의 각 단어가 입력의 어떤 부분을 얼마
나 중요하게 여길지 계산
구조 Encoder + Decoder Decoder에 추가되는 연산 모듈
핵심 구성 RNN / LSTM 기반 인코더-디코더 Query, Key, Value → 가중합
한계 입력을 고정된 벡터로만 요약 → 정보 손실 입력 전체에 가변적 주의 집중 가능

Attention 추가된 Seq2Seq 구조
입력 문장 → Encoder → 모든 hidden state 유지
→ Decoder가 매 스텝마다 입력 단어들 전체를 다르게 가중합해서 사용
디코더는 매 시점마다 입력 전체를 다르게 바라보고
Q (디코더 state), K/V (인코더 hidden states)로 attention 계산
즉, “출력의 각 단어가 입력의 어디에 집중해야 하는지” 판단

입력: “나는 학교에 간다”
출력: “I go to school”
school을 번역할 때는 “학교”에 집중해야 함 → 이걸 계산해주는 게 Attention
기존 Seq2Seq은 “나는 학교에 간다” 전체를 벡터 하나로 보고 “I go to school”을 생성하지만,
Attention이 추가되면 “school”을 생성할 때 “학교” 쪽에 더 집중하게 됩니다.

Attention:
무엇?: 두 개의 서로 다른 시퀀스(예: 영어 문장 → 프랑스어 문장) 간 관계를 학습.
사용 예: 기계 번역에서 소스 문장(입력)과 타겟 문장(출력)의 단어 관계를 파악.
특징: Query는 타겟에서, Key/Value는 소스에서 가져옴. 주로 인코더-디코더 구조에서 사용.
비유: "영어 문장에서 어떤 단어가 프랑스어 문장에 영향을 미칠까?"를 찾는 과정.
Self-Attention:
무엇?: 같은 시퀀스 내 단어들 간의 관계를 학습.
사용 예: "고양이가 나무 위에 있다"에서 "고양이"와 "나무"의 관계를 이해.
특징: Query, Key, Value 모두 같은 시퀀스에서 생성. 문맥 이해에 강력.
비유: "이 문장에서 단어들이 서로 어떻게 연결될까?"를 알아내는 과정.

Self-Attention의 등장 배경
기존의 문제점: RNN 기반 모델의 한계
• 순차 처리만 가능: 병렬화 불가능 → 학습 속도 느림
• 장기 의존성 문제: 앞뒤 단어가 멀리 떨어지면 관계를 반영하기 어려움
• 정보 손실: 고정된 길이의 벡터로 문장을 요약할 때 정보가 잘림
해결책:
Self-Attention의 등장
모든 단어가 서로를 바라볼 수 있게 함
즉, 문장 안의 모든 단어가 Query, Key, Value가 되어 모든 단어쌍의 관계를 계산
병렬 계산 가능 + 긴 거리의 의존 관계도 반영 가능

Q와 K의 내적이 필요한 이유
1. 내적(dot product)은 유사도(similarity)를 측정하는 간단한 방법
벡터 간 내적은 방향이 얼마나 비슷한가를 나타내는 값
두 벡터가 비슷한 방향을 가리키면 내적은 크고 양수
두 벡터가 서로 수직이면 내적은 0
두 벡터가 반대 방향이면 내적은 음수
즉,Q · Kᵀ가 클수록 → Query가 Key와 더 연관되어 있다는 뜻!
"The cat sat on the mat.“
Q = "cat“
K = ["the", "cat", "sat", "on", "the", "mat"]
Q · Kᵀ는 “cat이 문장 내 어느 단어에 집중할까?”를 계산
만약 Q와 K('mat')의 내적이 크다면 → "cat과 mat은 연관 있다"고 판단하여 attention
weight가 높아지고, 그 단어에 더 집중합니다.

자연어 처리 딥러닝을 위한 필수 개념
토큰화 (Tokenization)
문장을 단어, 문자, subword 등의 작은 단위로 나누는 과정예: "I love NLP" → ["I", "love", "NLP"]
정수 인코딩 (Integer Encoding)
텍스트 토큰을 숫자로 매핑하는 과정 (ex: 단어 → 고유 ID)어휘 사전 (Vocabulary)말뭉치에서
사용되는 모든 토큰의 집합 (고유 단어 목록)
패딩 (Padding)
서로 다른 길이의 문장을 동일한 길이로 맞추기 위한 0 등의 추가 작업예: [5, 2] → [5, 2, 0]
임베딩 (Embedding)
단어를 고차원에서 저차원 연속 벡터로 표현하는 방식(ex: Word2Vec, GloVe, FastText, Embedding
Layer)
임베딩 벡터 (Embedding Vector)
단어를 의미 공간에서 나타낸 벡터, 유사어 간 거리가 가까움

transformer를 이해하기 위한 용어
용어 설명
Token 문장을 잘게 쪼갠 단위 (단어, subword 등)
Embedding Token을 벡터로 변환하는 층
Positional Encoding 위치 정보를 벡터에 추가하는 방식 (Transformer는 순서를 직접 알 수 없기 때문)
Self-Attention 입력 시퀀스 내 단어들이 서로에게 얼마만큼 집중할지 계산
Multi-Head Attention 여러 개의 attention head를 통해 다양한 관계를 동시에 학습
Query (Q) 지금 단어가 어떤 정보를 찾고 싶은지
Key (K) 각 단어가 가진 특징 또는 정체성
Value (V) 각 단어가 가진 실제 정보
Attention Score Q·Kᵀ로 계산한 유사도 (얼마나 집중할지 결정)
Softmax Attention Score를 확률처럼 정규화
Scaled Dot-Product Attention Q·Kᵀ 결과를 √dₖ로 나누어 안정성 향상
Add & Norm Residual connection 후 Layer Normalization
Feedforward Layer Attention 뒤에 붙는 완전 연결층 (비선형 함수 포함)
Layer Normalization 입력 벡터를 정규화하여 학습 안정화
Residual Connection 이전 값을 더해 정보 손실 방지
Encoder Block Self-Attention + Feedforward Layer로 구성된 블록
Decoder Block Masked Self-Attention + Encoder-Decoder Attention + Feedforward
Masked Attention 미래 단어를 가리지 않도록 마스킹 (훈련 시 필수)
Transformer Encoder 전체 입력을 인코딩하는 부분 (BERT는 이 구조만 씀)
Transformer Decoder 인코딩된 정보를 바탕으로 출력 생성 (GPT는 이 구조 기반)