# -*- coding: utf-8 -*-
"""0522DL.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17NtH5o9i36HfpeaLPEjsf_gzyduSSawu
"""

import torch
# 열벡터
v = torch.tensor([[2], [3]]) # shape: (2, 1)
print("열벡터 v:", v, "shape:", v.shape)
# 행벡터
w = torch.tensor([[4, 5]]) # shape: (1, 2)
print("행벡터 w:", w, "shape:", w.shape)

# 행렬 A
A = torch.tensor([[1, 2], [3, 4]]) # shape: (2, 2)
print("행렬 A:", A, "shape:", A.shape)
# 행렬곱 A @ v
y = A @ v
print("행렬곱 A @ v:", y, "shape:", y.shape)

# 내적 w @ v
inner_product = w @ v
print("내적 w @ v:", inner_product, "shape:", inner_product.shape)

# 1차원 배열
u = torch.tensor([2, 3]) # shape: (2,)
print("1차원 배열 u:", u, "shape:", u.shape)
# A @ u (PyTorch가 u를 열벡터로 해석)
y_u = A @ u
print("A @ u:", y_u, "shape:", y_u.shape)
# 명시적 열벡터로 변환
u_column = u.unsqueeze(1) # shape: (2, 1)
print("명시적 열벡터 u_column:", u_column, "shape:", u_column.shape)
y_u_column = A @ u_column
print("A @ u_column:", y_u_column, "shape:", y_u_column.shape)

# w @ A
result_wA = w @ A
print("w @ A:", result_wA, "shape:", result_wA.shape)

"""-1을 reshape()에서 사용
해당 차원의 크기를 자동으로 계산하도록 PyTorch나 NumPy에 지시하기 위함입니다

항목 PyTorch TensorFlow
기본 차원 순서 channels_first (예: [N, C, H, W]) channels_last (예: [N, H, W, C])
행렬곱 연산자 @, torch.matmul() tf.matmul()
브로드캐스팅 NumPy 호환, 유연함 NumPy 호환, 약간 보수적
인덱싱 방식 Python/Numpy 스타일 강력 지원 유사하지만 일부 제한 존재
default tensor type float32 (명시적 변환 자주 필요) float32 (자동 변환 많음)
GPU 처리 tensor.to('cuda')로 명시 자동 처리 또는 device 지정
"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

