# %% [markdown]
# ì²˜ë¦¬ ë‹¨ê³„ ì²˜ ë¦¬ ë‚´ ìš© ìš” ì•½
# ë°ì´í„° ì¤€ë¹„ BBC ë°ì´í„°ì…‹ì„ ë¶ˆëŸ¬ì™€ì„œ ê¸°ì‚¬ ë‚´ìš©ê³¼ ë ˆì´ë¸”ì„ ì¶”ì¶œ.
# ë°ì´í„° ì „ì²˜ë¦¬ ë¶ˆìš©ì–´ë¥¼ ì œê±° í›„ ê¸°ì‚¬ ë‚´ìš©ì„ í† í°í™”í•˜ê³ , ë‹¨ì–´ë¥¼ ìˆ«ìë¡œ ì¸ì½”ë”©í•˜ì—¬ ë²¡í„°ë¡œ ë³€í™˜.
# ê¸°ì‚¬ ë‚´ìš©ì˜ ê¸¸ì´ë¥¼ ë§ì¶”ê¸° ìœ„í•´ íŒ¨ë”©ì„ ì¶”ê°€.
# ë ˆì´ë¸”ì„ ì›-í•« ì¸ì½”ë”©í•˜ì—¬ ë²¡í„°ë¡œ ë³€í™˜.
# ë°ì´í„° ë¶„í•  ì „ì²´ ë°ì´í„°ë¥¼ í•™ìŠµ ë°ì´í„°ì™€ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œë¡œ.
# ëª¨ë¸ êµ¬ì„± RNN(Recurrent Neural Network) ëª¨ë¸ì„ êµ¬ì„±
# ì„ë² ë”© ë ˆì´ì–´ë¥¼ ì¶”ê°€í•˜ì—¬ ë‹¨ì–´ ë²¡í„°ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ëŠ”ë‹¤.
# RNN ì¸µì„ ì¶”ê°€í•˜ì—¬ ì‹œí€€ìŠ¤ ë°ì´í„°ë¥¼ ì²˜ë¦¬
# ì¶œë ¥ ì¸µì„ ì¶”ê°€í•˜ì—¬ ë¶„ë¥˜ ê²°ê³¼ë¥¼ ì˜ˆì¸¡
# ëª¨ë¸ ì»´íŒŒì¼ ì†ì‹¤ í•¨ìˆ˜, ìµœì í™” ì•Œê³ ë¦¬ì¦˜, í‰ê°€ ì§€í‘œë¥¼ ì„¤ì •.
# ëª¨ë¸ í•™ìŠµ í•™ìŠµ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ í•™ìŠµ
# ëª¨ë¸ í‰ê°€ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€.
# ì •í™•ë„, F1 ìŠ¤ì½”ì–´ ë“±ì„ ê³„ì‚°í•˜ì—¬ ë¶„ë¥˜ ì„±ëŠ¥ì„ í™•ì¸.
# ëª¨ë¸ ì˜ˆì¸¡ ìƒˆë¡œìš´ ê¸°ì‚¬ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ë¶„ë¥˜ ê²°ê³¼ë¥¼ ì˜ˆì¸¡
# 
# NLTK(Natural Language Toolkit)ìì—°ì–´ ì²˜ë¦¬ë¥¼ ìœ„í•œ íŒŒì´ì¬ ë¼ì´ë¸ŒëŸ¬ë¦¬
# ì˜ì–´ ë¶ˆìš©ì–´(stopwords)
# ìì—°ì–´ ì²˜ë¦¬ ì‘ì—…ì—ì„œ ì˜ë¯¸ê°€ ì—†ê±°ë‚˜ ë¶„ì„ì— ì˜í–¥ì´ í¬ì§€ ì•ŠëŠ” ì¼ë°˜ì ì¸ ë‹¨ì–´ë“¤
# í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì „ì²˜ë¦¬ í•  ë•Œ ì‚¬ìš©.
# ì£¼ë¡œ ë¬¸ë²•ì ì¸ ì—­í• ì„ í•˜ëŠ” ë‹¨ì–´ë“¤, í…ìŠ¤íŠ¸ ë¶„ì„ì— ìì£¼ ë“±ì¥í•˜ì§€ë§Œ ì¤‘ìš”í•œ ì˜ë¯¸ë¥¼
# ì „ë‹¬í•˜ì§€ ì•ŠëŠ” ê²½ìš°ê°€ ë§ë‹¤.
# ë¶ˆìš©ì–´ë¥¼ ì œê±°í•¨ìœ¼ë¡œì¨ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆë‹¤.
# ì˜ˆ) "a", "an", "the", "is", "of", "and" ë“± ë¬¸ì¥ì˜ ë¬¸ë²•ì ì¸ êµ¬ì¡°ë¥¼ í˜•ì„±í•˜ëŠ” ë° ë„ì›€ì„
# ì£¼ì§€ë§Œ, ë¶„ì„ì—ëŠ” í° ì˜í–¥ì„ ë¯¸ì¹˜ì§€ ì•ŠëŠ”ë‹¤.
# nltk.corpus.stopwords ëª¨ë“ˆì„ í†µí•´ ì˜ì–´ ë¶ˆìš©ì–´ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤. 

# %%
import csv
import numpy as np
import nltk
from time import time

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import Dense, Dropout
from keras.layers import LSTM,Embedding

# %%
MY_VOCAB = 5000 # í•™ìŠµìš©ìœ¼ë¡œ ì‚¬ìš©í•  ë‹¨ì–´ì˜ ì „ì²´ ê°¯ìˆ˜
MY_EMBED = 64 # ë²¡í„°í™”í•  ì„ë² ë”© ê·œëª¨(ì°¨ì›)
MY_HIDDEN = 100 # RNN ì…€ì˜ ê·œëª¨(ìˆ¨ê²¨ì§„ ë ˆì´ì–´ì¸µ)
MY_LEN = 200 # ì—…ë¡œë“œí•  ê¸°ì‚¬ì˜ ì „ì²´ ë‹¨ì–´ì˜ ìˆ«ì

MY_SPLIT = 0.8 # í•™ìŠµìš© ë°ì´í„° ë¹„ìœ¨
MY_SAMPLE = 123 # ìƒ˜í”Œë¡œ ì‚¬ìš©í•  ê¸°ì‚¬ ë²ˆí˜¸
MY_EPOCH = 100 # ë°˜ë³µ í•™ìŠµìˆ˜
original = []
processed = []
labels = []

# %%
# NLTK(Natural Language Toolkit)ì—ì„œ ì˜ì–´ì˜ ë¶ˆìš©ì–´(stop words)ë¥¼ ë‹¤ìš´ë¡œë“œ
nltk.download('stopwords') # ê¸°ì‚¬ ë¶„ë¥˜ì— ì˜ë¯¸ì—†ëŠ” ì˜ì–´ ë‹¨ì–´ ë‹¤ìš´ë¡œë“œ
MY_STOP = set(nltk.corpus.stopwords.words('english'))

# corpus : í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ë¶„ì„/ì²˜ë¦¬ ìœ„í•œ ì—¬ëŸ¬ ë¬¸ì„œë‚˜ ë§ë­‰ì¹˜ë“¤

print('ì˜ì–´ ì œì™¸ì–´',MY_STOP)
print('ì œì™¸ì–´ ê°¯ìˆ˜', len(MY_STOP))
print(type(MY_STOP))
print('the' in MY_STOP)

# %%
import os
print("í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬:", os.getcwd())


# %%
import csv

# íŒŒì¼ ê²½ë¡œ ì„¤ì • (Windowsì—ì„œëŠ” ì´ìŠ¤ì¼€ì´í”„ ì²˜ë¦¬ í•„ìš”)
path = r'.\bbc-text.csv'


# ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”
labels = []
original = []
processed = []

# CSV íŒŒì¼ ì—´ê¸°
with open(path, 'r', encoding='utf-8') as file:
    reader = csv.reader(file)
    header = next(reader)  # í—¤ë” ì½ê¸°
    print("í—¤ë”:", header)

    # ë‚´ìš© í™•ì¸ (ì˜µì…˜)
    for row in reader:
        # print(row)
        labels.append(row[0])      # ì˜ˆ: ì¹´í…Œê³ ë¦¬ì´ë¦„ì„ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
        original.append(row[1])    # ì˜ˆ: ë‰´ìŠ¤ ë³¸ë¬¸ ë‚´ìš©ì„ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
        news = row[1] # ê°í–‰ì˜ ë‘ ë²ˆì§¸ ì—´ê°’ì„ news ë³€ìˆ˜ì— í• ë‹¹

        # print('before:',news)#ì œì™¸ì–´ ì²˜ë¦¬ ì „ ë¬¸ì¥
        for word in MY_STOP:
            mask = ' ' + word+ ' '
            news = news.replace(mask,' ')
        # print('after :', news) # ì œì™¸ì–´ ì²˜ë¦¬ í›„ ë¬¸ì¥
        processed.append(news) # ì²˜ë¦¬ëœ ê²°ê³¼ì¸ newsë¥¼ processed ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
    print('ì œì™¸ì–´ ì²˜ë¦¬ ì „ì²´ ê¸°ì‚¬ ìˆ˜: ',len(processed))
    print('ë ˆì´ë¸” ë°ì´í„° ìˆ˜:', len(labels))
    print('Original ë°ì´í„° ìˆ˜:', len(original))
    print('News ë°ì´í„° ìˆ˜:',len(news))

# %%


## âœ… ì½”ë“œ ë‚´ìš© ì •ë¦¬

```python
for row in reader:
    labels.append(row[0])       # ê° í–‰ì˜ ì¹´í…Œê³ ë¦¬ ì´ë¦„ì„ labels ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
    original.append(row[1])     # ê° í–‰ì˜ ê¸°ì‚¬ ë‚´ìš©ì„ original ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
    news = row[1]               # ê° í–‰ì˜ ë‘ ë²ˆì§¸ ì—´ ê°’ì„ news ë³€ìˆ˜ì— í• ë‹¹
```

ğŸ‘‰ **â†’ `labels`, `original` ë¦¬ìŠ¤íŠ¸ì— ì½ì€ ê¸°ì‚¬ë¥¼ í•œ ì¤„ì”© ì¶”ê°€ í›„ `news`ì˜ ë‚´ìš©ì€ ë³„ë„ ì €ì¥**

---

```python
for word in MY_STOP:
    mask = ' ' + word + ' '
    news = news.replace(mask, ' ')
    processed.append(news)
```

ğŸ‘‰ **â†’ `news`ì˜ ë‚´ìš©ì—ì„œ ë¶ˆìš©ì–´ë¥¼ ì°¾ì•„ì„œ ì–‘ìª½ ëì— ê³µë°±ì„ ë„£ì–´ `mask`ë¡œ ë‹´ì€ í›„,
í•´ë‹¹ `mask`ë¥¼ ê³µë°±ìœ¼ë¡œ ëŒ€ì²´í•´ `news`ì— ì €ì¥í•˜ê³ ,
ë¶ˆìš©ì–´ê°€ ì‚¬ë¼ì§„ `news`ì˜ ë‚´ìš©ì„ `processed`ì— ì €ì¥í•´ ë†“ëŠ”ë‹¤.**

---

## âœ… ë³´ì¶© ì„¤ëª…

> `"and"`ë¼ëŠ” ë¶ˆìš©ì–´ ì œê±° ì‹œ,
> `"sand"`ë¼ëŠ” ë‹¨ì–´ê¹Œì§€ `"s "`ë¡œ ë°”ë€Œì–´ë²„ë¦¬ëŠ” ë¬¸ì œë¥¼ í•´ê²°í•œë‹¤.

* ì¦‰, `mask = ' ' + word + ' '` ë¡œ ê³µë°±ì„ ë„£ì€ ì´ìœ ëŠ”,
  `" and "`ë§Œ ì œê±°í•˜ê³  `"sand"`ëŠ” ê±´ë“œë¦¬ì§€ ì•Šê¸° ìœ„í•¨ì´ë‹¤.

---

## âœ… í•µì‹¬ ìš”ì•½

| ë¦¬ìŠ¤íŠ¸         | ì—­í•                         |
| ----------- | ------------------------- |
| `labels`    | ë‰´ìŠ¤ì˜ ì¹´í…Œê³ ë¦¬(ì˜ˆ: ìŠ¤í¬ì¸ , ì •ì¹˜ ë“±) ì €ì¥ |
| `original`  | ë‰´ìŠ¤ ë³¸ë¬¸ ì›ë¬¸ ì €ì¥               |
| `processed` | ë¶ˆìš©ì–´ ì œê±°ëœ ë‰´ìŠ¤ ë³¸ë¬¸ ì €ì¥          |

| ì „ëµ                        | ì´ìœ                           |
| ------------------------- | --------------------------- |
| `' ' + word + ' '` ì‚¬ìš©     | ì •í™•í•œ ë‹¨ì–´ ë§¤ì¹­ë§Œ ì œê±° (ë‹¨ì–´ ì¼ë¶€ ì œê±° ë°©ì§€) |
| `news.replace(mask, ' ')` | ê³µë°±ìœ¼ë¡œ ì¹˜í™˜í•˜ì—¬ ë‹¨ì–´ ìœ„ì¹˜ ìœ ì§€          |

---


# %%
print('ìƒ˜í”Œ ê¸°ì‚¬ ì›ë¬¸ :',original[MY_SAMPLE])
print('ìƒ˜í”Œ ê¸°ì‚¬ ë¶„ë¥˜ :',labels[MY_SAMPLE])
print('ìƒ˜í”Œ ê¸°ì‚¬ ê¸°ì‚¬ ë°ì´í„°:',type(original[MY_SAMPLE]))
print('ìƒ˜í”Œ ê¸°ì‚¬ì˜ ì  ì²´ ë‹¨ì–´ìˆ˜:',str(len(original[MY_SAMPLE])).split())

print('\nì œì™¸ì–´ë¥¼ ì‚­ì œí•œ ìƒ˜í”Œ ê¸°ì‚¬:',processed[MY_SAMPLE])
print('ì œì™¸ì–´ ì‚­ì œ ìƒ˜í”Œì˜ ë‹¨ì–´ìˆ˜:',str(len(processed[MY_SAMPLE])).split())
# ì •ìˆ˜ì— split() í•¨ìˆ˜ë¥¼ ì ìš©í•  ìˆ˜ ì—†ê¸° ë•Œë¬¸ì—
# ë¬¸ìì—´ì„ ë¶„í• í•˜ê¸° ìœ„í•´ split()í•¨ìˆ˜ë¥¼ ë¬¸ìì—´ì— ì ìš©í•´ì•„

# %%
# í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì‘ì€ ë‹¨ìœ„ë¡œ ë¶„í• í•˜ëŠ” ì²˜ë¦¬(ê°ì²´ìƒì„±>í† í°í™”)
A_token = Tokenizer(num_words= MY_VOCAB,
oov_token='!') # out of vocaburary > "!"
A_token.fit_on_texts(processed)

print('ì´ ê¸°ì‚¬ ìˆ˜:',A_token.document_count)
print('ì´ ë‹¨ì–´ ìˆ˜:',len(A_token.word_counts))
print('ê° ë‹¨ì–´ì˜ ì‚¬ìš© íšŸìˆ˜:',A_token.word_counts)
print('ë‹¨ì–´ë¥¼ ì •ìˆ˜ë¡œ:',A_token.word_index)

# %% [markdown]
# 
# 
# ## âœ… ì½”ë“œ ë‚´ìš©
# 
# ```python
# A_token = Tokenizer(num_words=MY_VOCAB, oov_token='!')
# ```
# 
# ### ğŸ”¹ ì¸ì ì„¤ëª…
# 
# * **`num_words`**:
#   í† í°í™”í•  ë‹¨ì–´ì˜ ìµœëŒ€ ê°œìˆ˜ë¥¼ ì§€ì •.
#   â†’ ê°€ì¥ ë¹ˆë„ê°€ ë†’ì€ ìƒìœ„ `num_words`ê°œì˜ ë‹¨ì–´ë§Œ í† í°í™”ì— ì‚¬ìš©.
# 
# * **`oov_token='!'`**:
#   ì‚¬ì „ì— ì—†ëŠ” ë‹¨ì–´(OOV: Out-Of-Vocabulary)ë¥¼ ë‚˜íƒ€ë‚´ëŠ” í† í°ì„ `'!'`ë¡œ ì§€ì •.
#   â†’ í•™ìŠµí•˜ì§€ ì•Šì€ ë‹¨ì–´ê°€ ì…ë ¥ë˜ë©´ `'!'`ë¡œ ëŒ€ì²´ë¨.
# 
# ---
# 
# ```python
# A_token.fit_on_texts(processed)
# ```
# 
# ### ğŸ”¹ ë™ì‘ ì„¤ëª…
# 
# * **ë¶ˆìš©ì–´ê°€ ì œê±°ëœ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ `processed`** ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì•„
# * ê° ë‹¨ì–´ì— ê³ ìœ í•œ **ì •ìˆ˜ ì¸ë±ìŠ¤**ë¥¼ ë¶€ì—¬í•˜ë©° ë‚´ë¶€ ë‹¨ì–´ ì‚¬ì „ì„ êµ¬ì¶•í•¨.
# 
# ---
# 
# ## âœ… í•µì‹¬ ìš”ì•½
# 
# | ìš©ì–´               | ì„¤ëª…                               |
# | ---------------- | -------------------------------- |
# | `Tokenizer`      | í…ìŠ¤íŠ¸ë¥¼ ìˆ«ì ì¸ë±ìŠ¤ë¡œ ë³€í™˜í•´ì£¼ëŠ” Keras ë„êµ¬      |
# | `fit_on_texts()` | ì „ì²´ ë¬¸ì„œë¡œë¶€í„° ë‹¨ì–´ì˜ ë¹ˆë„ìˆ˜ë¥¼ í•™ìŠµí•´ ì •ìˆ˜ ì¸ë±ìŠ¤ë¥¼ ë§¤í•‘ |
# | `oov_token`      | í›ˆë ¨ ì‹œ ì‚¬ì „ì— ì—†ëŠ” ë‹¨ì–´ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆë„ë¡ ì„¤ì •     |
# | ê²°ê³¼               | ê° ë‹¨ì–´ì— ëŒ€í•´ ì •ìˆ˜ ì¸ë±ìŠ¤ê°€ ë¶€ì—¬ëœ ë‹¨ì–´ ì‚¬ì „ ìƒì„±ë¨   |
# 
# ---
# 

# %%
# ì…ë ¥ëœ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì •ìˆ˜ ì‹œí€€ìŠ¤ë¡œ ë³€í™˜
A_tokenized = A_token.texts_to_sequences(processed)

print('í† í° ì²˜ë¦¬ëœ ë°ì´í„° type:',type(A_tokenized))
print('í† í° ì²˜ë¦¬ëœ ë°ì´í„° type:',len(A_tokenized))
print('í† í° ì²˜ë¦¬ëœ ë°ì´í„° type:',A_tokenized[MY_SAMPLE])

# %% [markdown]
# ë¶ˆìš©ì–´ ì²˜ë¦¬ ë°ì´í„°ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ì„œ ê°ê° ì •ìˆ˜ ì‹œí€€ìŠ¤ë¡œ ë³€í™˜.
# ì´ì „ ë‹¨ê³„ì—ì„œ ìƒì„±í•œ Tokenizer ê°ì²´ì˜ ë‹¨ì–´ ì‚¬ì „ì„ í™œìš©í•˜ì—¬ ê° ë‹¨ì–´ë¥¼ ì •ìˆ˜ë¡œ ë§¤í•‘
# í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ìˆ˜ì¹˜í˜• ë°ì´í„°ë¡œ ë³€í™˜í•˜ë©´, ê¸°ì‚¬ ë°ì´í„°ë¥¼ ì‹ ê²½ë§ ëª¨ë¸ì— ì…ë ¥ìœ¼ë¡œ
# ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.

# %%
longest = max([len(x) for x in A_tokenized])
print('ì œì¼ ê¸´ ê¸°ì‚¬ ë‹¨ì–´ ìˆ˜ :',longest)
shortest = min([len(x) for x in A_tokenized])
print('ì œì¼ ì§§ì€ ê¸°ì‚¬ ë‹¨ì–´ ìˆ˜ :',shortest)

# %%
A_tokenized = pad_sequences(A_tokenized,
maxlen = MY_LEN,
padding='pre',
truncating='pre')
print('ìƒ˜í”Œ ê¸°ì‚¬ ê¸¸ì´ ì²˜ë¦¬ë³¸\n',A_tokenized[MY_SAMPLE])
print('í† í° ì²˜ë¦¬ëœ ë°ì´í„° ìˆ˜;',len(A_tokenized))

# %% [markdown]
# maxlen íŒŒë¼ë¯¸í„° (200)ë¥¼ ì‚¬ìš©í•˜ì—¬ íŒ¨ë”© í›„ ì‹œí€€ìŠ¤ì˜ ìµœëŒ€ ê¸¸ì´ë¥¼ ì§€ì •í•˜ê³ ,
# padding íŒŒë¼ë¯¸í„° (pre)ì™€ truncating íŒŒë¼ë¯¸í„° (pre)ë¥¼ ì‚¬ìš©í•˜ì—¬ íŒ¨ë”©ì˜ ìœ„ì¹˜ë¥¼ ì§€ì •

# %%
longest = max([len(x) for x in A_tokenized])
print('ì œì¼ ê¸´ ê¸°ì‚¬ ë‹¨ì–´ ìˆ˜ :',longest)
shortest = min([len(x) for x in A_tokenized])
print('ì œì¼ ì§§ì€ ê¸°ì‚¬ ë‹¨ì–´ ìˆ˜ :',shortest)

# %%
C_token = Tokenizer()
C_token.fit_on_texts(labels)

print('ì´ ê¸°ì‚¬ ìˆ˜:',C_token.document_count)
print('ì´ ë‹¨ì–´ ìˆ˜:',len(C_token.word_counts))
print('ê° ë‹¨ì–´ì˜ ì‚¬ìš© íšŸìˆ˜:',C_token.word_counts)
print('ë‹¨ì–´ë¥¼ ì •ìˆ˜ë¡œ:',C_token.word_index)

# %%
C_tokenized = C_token.texts_to_sequences(labels)
C_tokenized = np.array(C_tokenized)

print(C_tokenized)

# %% [markdown]
# 1. í…ìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„:
# text_data = ["This is an example sentence", "Another example sentence"]
# 2.Tokenizer ê°ì²´ ìƒì„±:
# tokenizer = Tokenizer()
# 3.í…ìŠ¤íŠ¸ ë°ì´í„° ì…ë ¥í•˜ì—¬ ë‹¨ì–´ ì‚¬ì „ êµ¬ì¶• ë° ì¸ë±ìŠ¤
# tokenizer.fit_on_texts(text_data)
# 4.í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì •ìˆ˜ ì‹œí€€ìŠ¤ë¡œ ë³€í™˜:
# tokenizer.texts_to_sequences(text_data)
# 5.íŒ¨ë”© ë˜ëŠ” ìë¥´ê¸° ìˆ˜í–‰:
# pad_sequences(sequences, maxlen=max_length, padding='post')

# %%
from sklearn.model_selection import train_test_split

X_train, X_test,Y_train,Y_test = train_test_split(A_tokenized,C_tokenized,train_size = MY_SPLIT,shuffle=False)
Y_train = np.array(Y_train) - 1
Y_test = np.array(Y_test) - 1
print('í•™ìŠµìš© ì…ë ¥ ë°ì´í„° ëª¨ì–‘:',X_train.shape)
print('í•™ìŠµìš© ì¶œë ¥ ë°ì´í„° ëª¨ì–‘:',Y_train.shape)
print('ë‹¨ì–´ë¥¼ ì •ìˆ˜ë¡œ:',C_token.word_index)

# %%
model = Sequential()
model.add(Embedding(input_dim=MY_VOCAB,output_dim=MY_EMBED))
model.add(Dropout(rate=0.5))
model.add(LSTM(units=MY_HIDDEN))
model.add(Dense(units=5,activation='softmax'))

# ë°ì´í„° í•œ ë²ˆ ë„£ì–´ì„œ ê°•ì œë¡œ ë¹Œë“œ
model.build(input_shape=(None, MY_LEN)) # Noneì€ ë°°ì¹˜í¬ê¸°
print('RNN ìš”ì•½')
print("=========")
model.summary()

# %%
model = Sequential()
model.add(Embedding(input_dim=MY_VOCAB,output_dim=MY_EMBED))
model.add(Dropout(rate=0.5))
model.add(LSTM(units=MY_HIDDEN))
model.add(Dense(units=5,activation='softmax'))

# ë°ì´í„° í•œ ë²ˆ ë„£ì–´ì„œ ê°•ì œë¡œ ë¹Œë“œ
model.build(input_shape=(None, MY_LEN)) # Noneì€ ë°°ì¹˜í¬ê¸°
print('RNN ìš”ì•½')
print("=========")
model.summary()

# %%
model = Sequential()
model.add(Embedding(input_dim=MY_VOCAB, output_dim=MY_EMBED))
model.add(Bidirectional(LSTM(units=MY_HIDDEN, dropout=0.3, recurrent_dropout=0.3)))
model.add(Dense(units=64, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(units=5, activation='softmax'))
# ë°ì´í„° í•œ ë²ˆ ë„£ì–´ì„œ ê°•ì œë¡œ ë¹Œë“œ
model.build(input_shape=(None, MY_LEN)) # Noneì€ ë°°ì¹˜í¬ê¸°
print('RNN ìš”ì•½')
print("=========")
model.summary()

# %%
model.compile(optimizer='adam',
loss ='sparse_categorical_crossentropy',
metrics=['acc'])
print('í•™ìŠµ ì‹œì‘')

# %%
begin = time()
from tensorflow.keras.callbacks import EarlyStopping

early_stop = EarlyStopping(
    monitor='val_loss',     # ëª¨ë‹ˆí„°í•  ê¸°ì¤€ (val_accuracy ë„ ê°€ëŠ¥)
    patience=20,             # ê°œì„ ì´ ì—†ë”ë¼ë„ ê¸°ë‹¤ë¦´ ì—í¬í¬ ìˆ˜
    restore_best_weights=True  # ê°€ì¥ ì„±ëŠ¥ ì¢‹ì•˜ë˜ ê°€ì¤‘ì¹˜ë¡œ ë³µì›
)

model.fit(X_train,Y_train,epochs=MY_EPOCH,verbose=1,batch_size=64,validation_split=0.2,callbacks=[early_stop])
end = time()
print('training time : {:.2f}ì´ˆ'.format(end-begin))

# %%
import numpy as np
print(np.unique(X_train))
print(np.unique(Y_train))


# %%
score = model.evaluate(X_test, Y_test,verbose=1)
print('ìµœì¢… ì •í™•ë„: {:.2f}'.format(score[1]))

# %%
pred = model.predict(X_test)
print('ì¶”ì¸¡ê°’\n',pred)
pred = pred.argmax(axis=1)
print('ì¶”ì¸¡ê°’(argmax ì²˜ë¦¬ í›„)\n',pred)
print('ì •ë‹µ\n', Y_test.flatten())

# %%
correct_predictions = 0
total_predictions = len(Y_test)
for i in range(total_predictions):
    if pred[i] == Y_test[i]:
        correct_predictions +=1
accuracy = correct_predictions/total_predictions
print('ì •í™•ë„', accuracy)

# %%

news = ['Bayern Munich cruised to an emphatic 7-0 victory over VfL Bochum \
 5 to move to the top of the German Bundesliga.']

news = A_token.texts_to_sequences(news)
print(news)
print('ì´ ë‹¨ì–´ ìˆ˜ :',len(news[0]))

# %%
news = pad_sequences(news, maxlen=MY_LEN,padding='pre',truncating='pre')
print('ì´ë‹¨ì–´ìˆ˜ : ', len(news[0]))

pred = model.predict(news)
pred = pred.argmax(axis=1)
print('RNN ì¶”ì¸¡ê°’ : ',pred)

# %%



