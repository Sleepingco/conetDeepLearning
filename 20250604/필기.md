RNN(Recurrent Neural Netword)
옛날기반, 지금은 트랜스포머가 대세
자연어 처리를 위한 언어모델을 중심으로

순환적인 구조(recurrent)를 가진 인공신경만 모델로, 시퀀스 데이터 처리에 사용
RNN은 이전 단계에서의 출력을 현재 단계의 입력으로 사용하여 내부 상태(메모리)를 유지하면서 정보를 전달

시퀀스(Sequence)

순서 또는 시간에 따라 나열된 요소들의 집함.
"나는 사과를 사고 집에 갔다“ : 각각의 요소는 시퀀스를 형성
1."나는" : 문장의 시작을 나타내는 첫 번째 요소.
2."사과를" : 두 번째 요소로, 사과를 구매했다는 동작의 대상을 나타낸다.
3."사고" : 세 번째 요소로, 사과를 사는 동작을 수행 상황을 나타낸다.
4.＂집에＂ : 네 번째 요소로, 사과를 사고 난 후의 위치를 나타낸다.
5."갔다" : 다섯 번째 요소로, 집으로 이동한 동작을 나타낸다.
6.문장의 시작 (SOS , Start of sequence): 마지막 요소로, 문장의 완료를 나타낸다.
 문장의 끝 (EOS, end of sequence): 마지막 요소로, 문장의 완료를 나타낸다.

시퀀스 데이터는 자연어 처리, 음성 인식, 주식 시계열 데이터 등 다양한 분야에서 활용

언어 모델의 기초

언어 모델이란 문장(시퀀스)에 확률을 부여하는 모델을 의미.
 1) 언어 모델은 특정한 상황에서의 적절한 문장이나 단어를 예측할 수 있다.
 • 기계 번역 예시
 • 𝑃 (난 널 사랑해|𝐼 𝑙𝑜𝑣𝑒 𝑦𝑜𝑢) > 𝑃(난 널 싫어해|𝐼 𝑙𝑜𝑣𝑒 𝑦𝑜𝑢)
 • 다음 단어 예측 예시
 • 𝑃 (먹었다|나는 밥을) > 𝑃(싸웠다|나는 밥을)

 언어 모델이란 문장(시퀀스)에 확률을 부여하는 모델을 의미.
 2) 하나의 문장은(𝑊)은 𝑤 로 구성
 • 𝑃 (𝑊) = 𝑃 (𝑤1, 𝑤2, 𝑤3, 𝑤4, 𝑤5, … 𝑤𝑛 )
 • 𝑃 (친구와 친하게 지낸다) = 𝑃(친구와, 친하게, 지낸다)
    => joint probability
 3) 연쇄 법칙 (Chain Rule)
 • 𝑃(𝑤1, 𝑤2, 𝑤3, … , 𝑤𝑛) = 𝑃(𝑤1) ∗ 𝑃 (𝑤2| 𝑤1) ∗ 𝑃( 𝑤3| 𝑤1, 𝑤2) , … , 𝑃(𝑤𝑛| 𝑤1, 𝑤2, … , 𝑤𝑛−1 )
 • 𝑃 (친구와 친하게 지낸다) = 𝑃(친구와) ∗ 𝑃(친하게|친구와 )∗ 𝑃(지낸다|친구와 친하게)

 언어 모델이란 문장(시퀀스)에 을 부여하는 모델을 의미.
 4) 전통적인 통계적 언어 모델 : 카운트 기반의 접근을 사용.
 • 현실 세계 모든 문장에 대한 확률을 가지려면 매우 방대한 양의 데이터가 필요
 - 긴 문장 처리가 쉽지 않다
 N-gram 언어 모델(인접한 일부 단어만 고려)

 언어 모델이란 문장(시퀀스)에 확률을 부여하는 모델을 의미.
 5) 초창기 RNN 기반의 언어 모델 :입력과 출력의 크기가 같고 단계별 순차 진행
 6) Seq2Seq: LSTM를 이용해 문장 전체의 정보가 담긴 문맥 벡터를 추출
 7) 어텐션(attention);
    문장의의 각 단어가 문맥을 파악하고 다른 단어와의 관련성을 계산.
    시퀀스 요소들 가운데 태스크 수행에 중요한 요소에 집중
    입력 시퀀스의 각 위치에 가중치를 계산하여 중요정보에 더 많은 관심을 줄 수 있다
    RNN의 NLP 알고리즘들은 문맥을 읽기 어렵거나 읽을 수 있다해도 단어와 단어사이의
    거리가 짧을 때만 이해할 수 있었다.
    attention 은 각단어들의 query와 key vector 연산을 통해 관계를 유추하기에 문장내
    단어간 거리가 멀든 가깝든 문제가 되지 않는다.
    어텐션기법은 인코더의 모든 레이어가 디코더의 모든 레이어에 병렬로 반영하여
    주어지 문장에서 단어의 유사도, 관련성 등을 평가한다.
    Seq2Seq 모델에 어텐션(attention) 매커니즘을 사용한다
    디코더는 인코더의 모든 출력(outputs)을 참고함
 8) Transformer: RNN이나 CNN을 사용하지 않고 Self-Attention 기반으로
    구성 Self-Attention은 입력 시퀀스의 각 단어와 다른 단어들과 관성성(유사도)을 계산하여
    중요한 정보에 집중.
    Positional Encoding은 단어의 위치 정보를 모델에 제공하여 입력 순서를 고려
4.Transformer
어텐션(attention): 쿼리와 키 사이의 관련성 계산> 내적(Dot Product)을 사용
 문장의 각 단어가 문맥을 파악하고 다른 단어와의 관령성을 계산.
어텐션은 시퀀스 데이터를 처리하며, 쿼리와 키를 사용하여 가중치를 계산하고,
 가중합을 통해 관련정보를 선택적으로 집중하는 출력을 생성하는 방식
 어텐션 메커니즘은 세 가지 주요한 요소인 Q(Query),K(Key),V(Value)로 구성.
• 쿼리(Q,Query):어텐션의 주체가 되는 벡터로, 관심을 가지고 있는 정보. 일반적으로 이전
계층의 출력이나 모델의 내부 상태가 쿼리로 사용.
• 키(k,Key):어텐션의 대상이 되는 벡터로, 중요한 정보를 포함하고 있는 벡터. 일반적으로
입력 시퀀스의 각 단계에서의 상태나 임베딩이 키로 사용.
• 값(V, Value):키에 해당하는 정보의 실제 값으로, 출력에 반영되는 벡터. 일반적으로 입력
시퀀스의 각 단계에서의 상태나 임베딩이 값으로 사용.

문장 "I love pizza"에서 "love"라는 단어의 어텐션을 계산가정.
 "love“ : Query Vector,
 다른 단어들, 대상(I, pizza) : Key Vector
 "love"와 "I"의 내적, "love"와 "pizza"의 내적, 유사성 :
                            Value Vector => Attention Score

어텐션 스코어는 어텐션 가중치를 계산하는 데 사용되며,
 최종적으로 가중합이 이루어져 출력이 생성.
 어텐션 메커니즘에서는
 쿼리와 키 사이의 관련성을 계산하기 위해 내적(Doy Product)을 사용.
 내적은 두 벡터 간의 유사도를 계산하는 데 사용되며
 두 벡터가 유사할수록 내적 값이 크게 나온다

 토큰의 Q의 입장에서 K의 transpose된 행렬하고 곱해져서 Attention Score가 나온다

 문장의 각 단어가 문맥을 파악하고 다른 단어와의 관련성 을 계산.
 시퀀스 요소들 가운데 태스크 수행에 중요한 요소에 집중
입력 시퀀스의 각 위치에 가중치를 계산하여 중요정보에 더 많은 관심을 줄 수 있다
Attention은 단어들의 Qurey와 Key vector 연산을 통해 관계를 유추하기에 문장내 단어간 거리가 멀든 가깝든 문제가 되지 않는다

주어진 문장 내에서 단어 간의 관련성을 계산
 Self-attention을 적용하기 위해 각 단어 벡터에 Query,Key,Vector 대해 로 변환
 각 단어 벡터의 Query와 모든 단어 벡터의 Key를 내적하여 유사도를 계산
 유사도를 소프트맥스 함수를 통해 정규화하여 각 단어에 대한 가중치를 계산(중요됴)

 벡터의 각 성분을 곱한 후 그 결과를 모두 더하는 연산.
두 벡터의 유사도를 측정하거나 벡터의 방향을 파악하는 데 사용.
예를 들어, 두 벡터 A = [1, 2, 3]와 B = [4, 5, 6]가 있다고 가정.
내적은 A · B = (1 * 4) + (2 * 5) + (3 * 6) = 4 + 10 + 18 = 32
결과적으로, A와 B의 내적은 32.
이 값은 두 벡터가 얼마나 유사한 방향을 가지고 있는지를 나타내는 척도.
내적이 큰 값일수록 두 벡터가 비슷한 방향을 가지고 있으며, 작은 값일수록 두
벡터가 서로 다른 방향을 가지고 있다

각 위치의 단어에 대해 고유한 벡터를 할당하여 위치정보를 나타낸다.
이 위치 정보는 모델이 문장 내의 단어들의 순서를 인식하고 처리하는 데 사용.
"I love pizza"라는 문장을 트랜스포머에 입력할 때 어텐션 메커니즘은 "love"라는 단어와
"I" 또는 "pizza" 사이의 관련성을 계산하여 중요한 정보를 추출.
positional encoding은 "I"와 "love"의 상대적인 위치정보를 벡터로 표현하여 모델에게 순서
정보를 전달. 

attention과 positional encodeing이 함께 작용하여 트랜스포머 모델은 문장내의 구조와 의미를 파악하고 표현할 수 있다.

multihead attention: Attention이 단독으로 쓸 경우 자기자신의 의미에만 지나치게
집중할 수 있다. (특정 단어나 유사단어가 많이 나오는 경우 !!)
 각 layer에서 나온 출력은 그대로 합한 뒤 또다른 weight vecotr를 곱해 하나의 vector로
 취합하여 다른 단어의 정보를 충분히 반영

 어텐션에서 사용되는 유사도(smilarity) 두 벡터 간의 관련성을 측정하는 척도.
유사도는 어텐션 가중치를 계산하는 데 사용되며, 쿼리와 키 간의 관련성을 판단
유사도를 구하는 방법에는 내적(Dot Product), 유클리디안 거리(Euclidean Distance),
코사인 유사도(Cosine Similarity) 등의 측정 방법도 활용될 수 있다. 

Attention is all you need
residual learning
입력값을 출력값에 더해서 출력값을 만든다. 입력값이 출력값에 추가적인 정보를 제공

트랜스포머는
시퀀스 데이터를 처리하기 위해, 어텐션 메커니즘을 중심으로 구성.
트랜스포머는 기존의 RNN 기반 모델보다 병렬화가 가능하며, 학습 속도가 빠르고
장기 의존성 문제를 다루기 쉽다.
단점
계산 자원 요구: 큰 어휘 사전과 많은 수의 파라미터를 사용하는 경우가 많아,
계산 자원이 많이 필요. 상당한 계산 시간이 소요될 수 있다.
데이터 양의 요구: 작은 규모의 데이터셋에서는 과적합이 발생할 수 있다.
시퀀스 길이 제한: 긴 시퀀스를 다루기 위해서는 입력 패딩, 마스킹 등의
추가적인 전처리가 필요.

BERT
Transformer의 인코더 구조를  기반으로 한 양방향 언어 모델 .
양방향 언어 모델은 문맥을 파악하기 위해 좌우 양방향으로 입력 시퀀스를 처리.
BERT는 사전 학습된 언어 모델로 다양한 자연어 처리 작업에서 성능 향상을
이끌어냈다.

양방향 Transformer 아키텍처: 문장 모든 단어를 좌우 방향에서 토큰의 의미를 파악
다양한 태스크 활용: 문장 분류, 개체명 인식, 문장 유사도 측정 등 높은 성능
NSP(Next Sentence Prediction)
두 문장이 입력되었을 때 순서를 예측하는 방식으로 다음 문장을 예측 두 문장간의
연관성을 추정.

GPT
transformer 구조의 디코더를 기반으로 한 생성 모델로, 자연어 처리 작업에서 다양한
텍스트 생성 과제에 사용.
GPT는 문맥을 파악하여 자연스럽게 텍스트를 생성할 수 있는 능력을 가지고 있다.
문장 완성, 기계 번역, 질의응답 시스템, 챗봇 등 다양한 자연어 생성 작업에 사용.
GPT는 왼쪽에서 오른쪽으로 단방향으로 텍스트를 처리하며, 다음 단어를 예측하면서 생성
작업을 수행합니다

실습을 위한 선수 지식

Lexicon(어휘 목록 또는 어휘집): 단어의 의미, 품사, 발음, 문법 등과 같은 정보를 포함.
일반적으로 사전이나 어휘 데이터베이스로 구성되어 언어 처리 작업에 사용.
Vocabulary(어휘):특정 문서, 문장, 언어 모델 또는 작업에 사용되는 단어의 집합.
예를 들어, 특정 코퍼스(말뭉치)에서 사용된 모든 단어를 포함하는 단어 집합을 말할 때
"Vocabulary"라고 말할 수 있다.
한국어 : KoNLPy 에서 제공하는 사전. 대한민국 헌법, 국회법, 경제 화물 운송업 등
다양한 분야의 말뭉치를 기반으로 만들어졌으며, 약 30만 개의 단어가 포함
영어 : WordNet. 영어 단어의 동의어, 반의어, 상하위어 등의 관계를 정의하고, 이를
기반으로 단어의 의미를 추정. 약 15만 개의 단어
Corpus: 분류가 되지 않은 현실세계의 말뭉치,
 이를 기반으로 단어 사전을 만들 수 있다.
 Corpus는 모델 학습을 위한 텍스트 데이터를 수집하고 전처리하는 데 사용

Tokenizer
 텍스트를 처리 가능한, 의미있는 작은 단위, 토큰(단어,형태소,문당, 혹은 문자 등)으로
분리하여 문장 또는 문서를 이해하고 분석하는 데 사용
• 분리기준: 기준에 따라 텍스트를 분리. 공백, 구두점, 대소문자, 특수문자 등을 이용
언어, 문제의 특성, 처리하려는 데이터에 따라 다양하게 설정.
• 단위 설정 : 어떤 단위로 분할할지 결정. 문장,단어,형태소, 혹은 문자 단위 등
• 특수 처리: 특정 단어를 하나의 토큰으로 처리, 띄어쓰기가 없는 언어의 경우
텍스트를 올바르게 분할하는 방법을 설정하는 등의 작업을 수행.

전환과정을 거치면서, 각 영어 단어에 고유한 숫자가 주어지고, 이 숫자를 그 단어의 토큰이라 부름

토크나이저는
각 모델별로 자체적인 어휘(Vocabulary)와 토크나이저를 가지고 있으며,
모델을 통해 텍스트 데이터를 처리하기 전에 해당 모델의 토크나이저를 사용하여
입력 데이터를 토큰화하고 인덱스로 변환해야 한다.

Tokeninzing process
tokenizer:
Keras의 Tokenizer 클래스의 객체를 생성
fit_on_texts:
Tokenizer 객체를 사용하여 텍스트 데이터를 분석하고, 단어 인덱스를 생성
text_to_sequences:
텍스트 데이터를 토큰화하여 단어 인덱스로 변환하는 메서드.
텍스트 데이터의 각 단어를 해당하는 정수 인덱스로 변환하여 리스트 형태로 반환.

1 단어를 정수로 전환
원본 텍스트 데이터에 선처리 작업 진행
 • 기계 학습의 효율성과 정확도를 높이기 위함
 3가지 토큰 처리
전처리 단계 설명
 텍스트 정제 (Text Cleaning) 특수 문자, HTML 태그, 문장 부호 등 불필요한 요소 제거
소문자 변환 (Lowercasing) 모든 문자를 소문자로 변환
토큰화(Tokenization) 텍스트를 작은 단위로 나누는 과정 (단어, 형태소, 음절 등)
불용어 제거 (Stopword Removal) 의미 없는 단어 (불용어) 제거 (ex: and, the, in 등)
어간 추출 /표제어 추출 단어의 어간 또는 표제어 추출 (ex: running -> run)
특수 문자 제거 특정 문자 (구두점, 특수 기호) 제거
숫자 제거 (Number Removal) 숫자 제거
정규화 (Normalization) 단어들을 표준 형태로 변환 (ex: 'isn't' -> 'is not')
토큰 필터링 (Token Filtering) 최소/최대 길이, 빈도 등을 기준으로 토큰 필터링
워드 임베딩 (Word Embedding) 단어를 고차원의 벡터로 표현

단어 기반 토크 나이저(word-based Tokenizer):
문장을 단어 단위로 나누어 처리.
예시: 공백을 기준으로 "Hello,", "how", "are", "you?"로 나눌 수 있다.
장점: 자연어의 의미를 보존하며 의미 있는 단어들을 토큰으로 나눌 수 있다.
단점: 합성어나 특수한 텍스트 처리에 취약할 수 있다.

문자 기반 토크 나이저(Character-based Tokenizer):
문자 하나하나를 토큰으로
나누는 방식.
예시: 문자 단위로 나누면 "H", "e", "l", "l", "o", 로 분할.
장점: 어떤 언어든 처리 가능하며, 미리 정의된 단어 사전이 필요 없다.
단점: 텍스트의 의미 파악이 어려울 수 있으며, 처리할 문자의 수가 많아질 수 있다.

서브워드 기반 토크나이저(Subword-based Tokenizer):
단어를 서브워드(subword)로 분할 방식

BPE(Byte Pair Encoding)
자주 나오는 문자 를 Pair로 묶어 새롭게 토큰을 만들어내는 압축/토크나이징 방식

서브 워드 기반 토크나이저(subword-based tokenizer):
단어를 서브워드(subword)로 분할 방식
예시: "Hello, how are you?"를 서브워드로 나누면
 "He", "ll", "o", ", ", "how", "are", "you", "?"로 나눌 수 있다.
장점:
•어휘 관리: 어휘 크기를 줄이면서도 효율적인 어휘 관리가 가능.
•어휘 확장: 새로운 언어나 도메인에 쉽게 적용하여 어휘를 확장할 수 있다.
•의미 보존: 단어를 더 작은 의미 단위로 나누므로 의미 보존이 가능.
•합성어 처리: 서브워드 단위로 분할되므로 합성어나 특수한 텍스트 처리에도 강건.
단점:
•분할 모호성: 서브워드로 분할되면서 문장의 구조나 의미가 모호해질 수 있다.
•사전 생성: 서브워드 사전을 생성하고 관리해야 하며, 이 과정에서 추가 작업이 필요.
다양한 언어 처리 작업에 효과적으로 사용되며, 언어의 특성을 고려하여 적절한 서브워드
알고리즘과 설정을 선택하여 사용할 수 있다. BERT 등의 모델에 사용


inputs는 딕셔너리 형태의 데이터로, 다양한 정보를 포함.
"input_ids"는 토큰화된 문장의 인덱스로 이루어진 리스트.
tokenizer.decode(inputs["input_ids"]) : 이 인덱스들을 다시 텍스트로 디코딩하여
원래 문장으로 복원하는 역할. 토크나이저의 역변환 과정을 보여주는 것

Embedding
•텍스트나 단어를 수치형 벡터로 변환하는 작업
•변환된 벡터는 기존 텍스트를 모델이 이해할 수 있는 형태로 표현
•임베딩은 단어나 토큰 간의 의미적 유사성을 반영하기 위해 벡터 공간 내에서의
거리와 관계를 보존.
• 통상 토크나이징을 통해 토큰으로 분할 후 임베딩을 통해 벡터화 진행
벡터화 방법: CounterVectorizer (단어의 등장 횟수를 기준으로 숫자로 표현
 단순히 횟수만들 특징으로 잡으므로 큰 의미가 없고
 자주 쓰이는 조사가 큰 특징으로 나타난다)
 예) “나는 정말 열심히 살고 있다” 벡터화
 단어사전 : 나는, 정말, 하루를 열심히 살고 있다
 > [1, 2, 0, 1, 1, 1]
 TfidVectorizer(Term Frequency Inverse Document Frequency Vectorizer)
 의미가 없는 조사를 low value로 처리

Word2Vec 벡터화 방법:(단어가의 관계와 유사도를 분석)
 위치에 나오는 단어는 비슷한 의미를 갖는다”는 분포가설에 기반
 - CBOW:Continuos Bag of Word(주변값의 중간값으로 벡터화/예측)

 - Skip gram:중심 단어의 벡터를 보고, 주변 단어의 벡터를 예측
 예) 예: “나는 고양이를 좋아한다”
 중심 단어: “고양이를”
 주변 단어: [“나는”, “좋아한다”]

 1️⃣ 새로운 문장 입력되면 초기 벡터값이 부여된다
예를 들어: “나는 오늘도 고양이를 좋아한다”
2️⃣ 이 문장의 단어들이 문맥 벡터를 만들 때
CBOW: 주변 단어 벡터 평균으로 중심 단어 예측
Skip-gram: 중심 단어로 주변 단어 예측
3️⃣ 예측 결과를 실제 단어와 비교
손실 함수(cross-entropy loss)가 계산됨
4️⃣ 손실을 최소화하기 위해 모든 단어의 벡터가 역전파로 업데이트됨.
 (= 임베딩 테이블에 있는 각 단어의 벡터)
학습 데이터가 추가될 때마다
전체 임베딩 벡터가 조금씩 움직여서 더 나은 표현(유사 단어를 가까이
위치시키는 방향)으로 가게 된다

항목 Word2Vec Transformer
학습 방식 CBOW/Skip-gram
(윈도우 기반 주변 단어 예측) Self-Attention (문장 전체 관계 학습)
문맥 고려 주변 몇 단어(로컬 문맥)만 본다 문장 전체 (글로벌 문맥) 본다
추가 학습 시 주변 단어 기반의 벡터 업데이트 Self-Attention으로 모든 토큰 관계 업데이트
출력 벡터 고정 벡터(학습 끝나면 바뀌지 않음) 입력 문맥마다 동적 벡터 (문맥 따라 매번 다름!)
트랜스포머의 가중치
• 임베딩 레이어 (토큰 벡터)
• Self-Attention의 Query, Key, Value 가중치
• FeedForward 네트워크 가중치
• LayerNorm 파라미터 등

텍스트 데이터 수집 >
토큰화(Tokenization) 등 전처리>
임베딩 학습: 토큰화된 단어들을 사용하여 단어 임베딩을 학습 >
임베딩 적용: 학습된 단어 임베딩을 적용하여 텍스트 데이터를 벡터로 표현 >
딥러닝 모델의 입력으로 사용, 단어 간의 유사도 측정, 문서 분류 등 다양한 NLP 작업

워드 임베딩은 단어를를 고차원의 실수 벡터로 매핑 하는 과정
임베등 공간에서 단어의 의미와 문맥적인 유사성을 보존하기 위해 수행
단어 간의 의미적 유사성을 반영하면, "apple"과 "orange"는 가깝게 위치.
워드 임베딩이 단어의 의미를 벡터 공간 상에서 수학적인 관계로 표현할 수 있다.
"king - man + woman = queen"과 같은 단어 간의 의미적 관계를 나타내는 벡터 연산

주어진 문장은 단여별로 2차원의 벡터로 표현 가능(워드 임베딩 후)
워드 임베딩은 단어를 고차원의 실수 벡터로 매핑하는 과정
 단어의 의미와 문맥적인 유사성을 보존하기 위해 수행

 벡터의 표현 방식: Sparse vector vs Dense vector
sparse vector(희소 벡터)는대부분의 원소가 0인 벡터.
 One hot encoding으로 표현된 벡터
(원핫인코딩은 희소 벡터의 한 종류)
Dense vector(밀집 벡터)는
대부분의 원소가 0이 아닌 값을 가지는 벡터.
즉, 벡터 내의 대다수 요소들이 0이 아닌 실수값으로 표현.
밀집 벡터는 단어나 개체의 의미를 더 잘 표현할 수 있으며, 유사도 계산이나 기계 학습
알고리즘에 더 적합

주어진 문장의 단어를 one hot encoding표현도 가능
또는 Dense Vector(밀집 벡터)로 표현 가능(word Embedding)

문장이 2개가 아니고 2천 개라면?
사용된 단어가 7개가 아니고 700개라면?

1-hot encoding
700x700-490,000개의
메모리 필요
문장 전환 시 한 단어마다
700개의 숫자 필요
임베딩
2x700-1,400개의
메모리 필요
문장 전환 시 2개의 숫자
필요

Word2Vec은 주어진 텍스트 데이터를 사용하여 단어 간 유사성을 학습하고, 단어의 의미와 관련성을 벡터공간에서 추론할 수 있다.
1) gensim 라이브러리를 사용하여 Word2Vec 모델을 학습.
2) 텍스트 데이터를 sentences라는 단어의 시퀀스로 변환한 후, Word2Vec 모델 객체를 생성.
모델의 파라미터로는 벡터의 크기(size), 윈도우 크기(window size), 최소 등장
횟수(min_count) 등을 설정.
3) 학습된 모델에서는 단어의 벡터 표현을 확인할 수 있다.

model = Word2Vec(sentences, vector_size=100, window=5, min_count=1)
각 단어를 100차원의 벡터로 표현하며, 각 단어는 앞뒤로 5개의 단어를 고려하여 학습되며, 각 단어는 적어도 한 번 이상 나타나는 모든 단어를 학습하는 Word2Vec 모델을 생성

Word2Vec 모델을 생성하는 코드:
sentences: 학습할 문장의 리스트.

vector_size: 생성될 각 단어 벡터의 차원. 클수록 좋지만, 메모리 사용량과 학습 시간에 영향.

window: 단어가 고려해야 하는 컨텍스트의 크기.
window 값이 5라면, 각 단어는 앞 뒤로 5개의 단어를 고려하여 학습.

min_count: 모델에 학습할 때 고려할 단어의 최소 빈도수.
이 값이 1이라면, 각 단어가 적어도 한 번은 나타나야 한다.
vector = model.wv['NLP']
Word2Vec 모델에서 ‘NLP’라는 단어에 해당하는 벡터를 가져와 vector라는 변수에 저장

model: 단어를 벡터로 변환.

wv: Word2Vec 모델에서 단어 벡터를 가져오는 방법
      wv는 “word vector”, 특정 단어에 대한 벡터를 조회하는 데 사용.

‘NLP’: ‘NLP’라는 단어에 대한 벡터를 가져온다.

전체 데이터 셋을 합친 단어의 종류의 수는 약 88,585 종류
그 중 5000개의 단어만 업로드 하여 x,y데이터로 나눔
샘플 데이터 5번의 영화평을 출력 : 정수로 처리된 단어의 인덱스, 단어 수,감성 라벨

다음은 이미지에서 추출한 내용을 **설명 댓글** 형식으로 깔끔하게 정리한 것입니다:

---

### 🟦 시퀀스 데이터 패딩: `pad_sequences` 사용 설명

* `X_test = pad_sequences(sequences=X_test, truncating='post', padding='post', maxlen=MY_LENGTH)`
   → 시퀀스 데이터를 일정한 길이로 맞추기 위해 **패딩(padding)** 또는 **절단(truncating)** 작업을 수행한다.

---

#### 🔹 padding:

* 시퀀스의 길이를 `maxlen`에 맞추기 위해 **뒤쪽에 0을 추가**한다.
* `'post'` 설정 시, 시퀀스의 **뒷부분에 패딩**을 추가한다.
   예: `[1, 2, 3]` → `[1, 2, 3, 0, 0]`

#### 🔹 truncating:

* 시퀀스가 너무 길 경우 **뒷부분을 잘라낸다**.
* `'post'` 설정 시, 시퀀스의 **뒤쪽을 잘라낸다**.
   예: `[1, 2, 3, 4, 5, 6]` → `[1, 2, 3, 4, 5]` (if `maxlen=5`)

---

### ✅ 요약

* `padding='post'`: 시퀀스 뒤에 패딩 추가
* `truncating='post'`: 시퀀스 뒤를 잘라냄
* `maxlen=...`: 최종 시퀀스 길이 지정

---

필요하시면 `pad_sequences`의 앞쪽 패딩 `'pre'`과 비교하거나 실제 데이터 전처리 코드도 제공해 드릴 수 있습니다!
