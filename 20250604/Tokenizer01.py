# %%
# !pip install transformers
# !pip install tensorflow
# !pip install --upgrade --force-reinstall transformers tensorflow

# %%
import tensorflow as tf
print(tf.__version__)


# %%
from transformers import AutoTokenizer

checkpoint = 'distilbert-base-uncased-finetuned-sst-2-english'
tokenizer = AutoTokenizer.from_pretrained(checkpoint)

raw_inputs = [
    "I've been waiting for a HuggingFace course my whole life",
    "I hate this so much"
]

inputs= tokenizer(raw_inputs,padding=True,truncation=True,return_tensors='tf')

# %%
print(inputs)

# %%
from transformers import AutoTokenizer

checkpoint = 'distilbert-base-uncased-finetuned-sst-2-english'
tokenizer = AutoTokenizer.from_pretrained(checkpoint)

raw_inputs = [
    "I've been waiting for a HuggingFace course my whole life",
    "I hate this so much"
]
tokens = [tokenizer.tokenize(sentence) for sentence in raw_inputs]
ids = [tokenizer.convert_tokens_to_ids(token) for token in tokens]


print(ids[0])
print(ids[1])

print(tokenizer(raw_inputs,padding=True))

# %% [markdown]
# inputsëŠ” ë”•ì…”ë„ˆë¦¬ í˜•íƒœì˜ ë°ì´í„°ë¡œ, ë‹¤ì–‘í•œ ì •ë³´ë¥¼ í¬í•¨.
# "input_ids"ëŠ” í† í°í™”ëœ ë¬¸ì¥ì˜ ì¸ë±ìŠ¤ë¡œ ì´ë£¨ì–´ì§„ ë¦¬ìŠ¤íŠ¸.
# tokenizer.decode(inputs["input_ids"]) : ì´ ì¸ë±ìŠ¤ë“¤ì„ ë‹¤ì‹œ í…ìŠ¤íŠ¸ë¡œ ë””ì½”ë”©í•˜ì—¬
# ì›ë˜ ë¬¸ì¥ìœ¼ë¡œ ë³µì›í•˜ëŠ” ì—­í• . í† í¬ë‚˜ì´ì €ì˜ ì—­ë³€í™˜ ê³¼ì •ì„ ë³´ì—¬ì£¼ëŠ” ê²ƒ
# 
# Embedding
# â€¢í…ìŠ¤íŠ¸ë‚˜ ë‹¨ì–´ë¥¼ ìˆ˜ì¹˜í˜• ë²¡í„°ë¡œ ë³€í™˜í•˜ëŠ” ì‘ì—…
# â€¢ë³€í™˜ëœ ë²¡í„°ëŠ” ê¸°ì¡´ í…ìŠ¤íŠ¸ë¥¼ ëª¨ë¸ì´ ì´í•´í•  ìˆ˜ ìˆëŠ” í˜•íƒœë¡œ í‘œí˜„
# â€¢ì„ë² ë”©ì€ ë‹¨ì–´ë‚˜ í† í° ê°„ì˜ ì˜ë¯¸ì  ìœ ì‚¬ì„±ì„ ë°˜ì˜í•˜ê¸° ìœ„í•´ ë²¡í„° ê³µê°„ ë‚´ì—ì„œì˜
# ê±°ë¦¬ì™€ ê´€ê³„ë¥¼ ë³´ì¡´.
# â€¢ í†µìƒ í† í¬ë‚˜ì´ì§•ì„ í†µí•´ í† í°ìœ¼ë¡œ ë¶„í•  í›„ ì„ë² ë”©ì„ í†µí•´ ë²¡í„°í™” ì§„í–‰
# ë²¡í„°í™” ë°©ë²•: CounterVectorizer (ë‹¨ì–´ì˜ ë“±ì¥ íšŸìˆ˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ìˆ«ìë¡œ í‘œí˜„
#  ë‹¨ìˆœíˆ íšŸìˆ˜ë§Œë“¤ íŠ¹ì§•ìœ¼ë¡œ ì¡ìœ¼ë¯€ë¡œ í° ì˜ë¯¸ê°€ ì—†ê³ 
#  ìì£¼ ì“°ì´ëŠ” ì¡°ì‚¬ê°€ í° íŠ¹ì§•ìœ¼ë¡œ ë‚˜íƒ€ë‚œë‹¤)
#  ì˜ˆ) â€œë‚˜ëŠ” ì •ë§ ì—´ì‹¬íˆ ì‚´ê³  ìˆë‹¤â€ ë²¡í„°í™”
#  ë‹¨ì–´ì‚¬ì „ : ë‚˜ëŠ”, ì •ë§, í•˜ë£¨ë¥¼ ì—´ì‹¬íˆ ì‚´ê³  ìˆë‹¤
#  > [1, 2, 0, 1, 1, 1]
#  TfidVectorizer(Term Frequency Inverse Document Frequency Vectorizer)
#  ì˜ë¯¸ê°€ ì—†ëŠ” ì¡°ì‚¬ë¥¼ low valueë¡œ ì²˜ë¦¬
# 
# Word2Vec ë²¡í„°í™” ë°©ë²•:(ë‹¨ì–´ê°€ì˜ ê´€ê³„ì™€ ìœ ì‚¬ë„ë¥¼ ë¶„ì„)
#  ìœ„ì¹˜ì— ë‚˜ì˜¤ëŠ” ë‹¨ì–´ëŠ” ë¹„ìŠ·í•œ ì˜ë¯¸ë¥¼ ê°–ëŠ”ë‹¤â€ëŠ” ë¶„í¬ê°€ì„¤ì— ê¸°ë°˜
#  - CBOW:Continuos Bag of Word(ì£¼ë³€ê°’ì˜ ì¤‘ê°„ê°’ìœ¼ë¡œ ë²¡í„°í™”/ì˜ˆì¸¡)
# 
#  - Skip gram:ì¤‘ì‹¬ ë‹¨ì–´ì˜ ë²¡í„°ë¥¼ ë³´ê³ , ì£¼ë³€ ë‹¨ì–´ì˜ ë²¡í„°ë¥¼ ì˜ˆì¸¡
#  ì˜ˆ) ì˜ˆ: â€œë‚˜ëŠ” ê³ ì–‘ì´ë¥¼ ì¢‹ì•„í•œë‹¤â€
#  ì¤‘ì‹¬ ë‹¨ì–´: â€œê³ ì–‘ì´ë¥¼â€
#  ì£¼ë³€ ë‹¨ì–´: [â€œë‚˜ëŠ”â€, â€œì¢‹ì•„í•œë‹¤â€]
# 
#  1ï¸âƒ£ ìƒˆë¡œìš´ ë¬¸ì¥ ì…ë ¥ë˜ë©´ ì´ˆê¸° ë²¡í„°ê°’ì´ ë¶€ì—¬ëœë‹¤
# ì˜ˆë¥¼ ë“¤ì–´: â€œë‚˜ëŠ” ì˜¤ëŠ˜ë„ ê³ ì–‘ì´ë¥¼ ì¢‹ì•„í•œë‹¤â€
# 2ï¸âƒ£ ì´ ë¬¸ì¥ì˜ ë‹¨ì–´ë“¤ì´ ë¬¸ë§¥ ë²¡í„°ë¥¼ ë§Œë“¤ ë•Œ
# CBOW: ì£¼ë³€ ë‹¨ì–´ ë²¡í„° í‰ê· ìœ¼ë¡œ ì¤‘ì‹¬ ë‹¨ì–´ ì˜ˆì¸¡
# Skip-gram: ì¤‘ì‹¬ ë‹¨ì–´ë¡œ ì£¼ë³€ ë‹¨ì–´ ì˜ˆì¸¡
# 3ï¸âƒ£ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì‹¤ì œ ë‹¨ì–´ì™€ ë¹„êµ
# ì†ì‹¤ í•¨ìˆ˜(cross-entropy loss)ê°€ ê³„ì‚°ë¨
# 4ï¸âƒ£ ì†ì‹¤ì„ ìµœì†Œí™”í•˜ê¸° ìœ„í•´ ëª¨ë“  ë‹¨ì–´ì˜ ë²¡í„°ê°€ ì—­ì „íŒŒë¡œ ì—…ë°ì´íŠ¸ë¨.
#  (= ì„ë² ë”© í…Œì´ë¸”ì— ìˆëŠ” ê° ë‹¨ì–´ì˜ ë²¡í„°)
# í•™ìŠµ ë°ì´í„°ê°€ ì¶”ê°€ë  ë•Œë§ˆë‹¤
# ì „ì²´ ì„ë² ë”© ë²¡í„°ê°€ ì¡°ê¸ˆì”© ì›€ì§ì—¬ì„œ ë” ë‚˜ì€ í‘œí˜„(ìœ ì‚¬ ë‹¨ì–´ë¥¼ ê°€ê¹Œì´
# ìœ„ì¹˜ì‹œí‚¤ëŠ” ë°©í–¥)ìœ¼ë¡œ ê°€ê²Œ ëœë‹¤
# 
# í•­ëª© Word2Vec Transformer
# í•™ìŠµ ë°©ì‹ CBOW/Skip-gram
# (ìœˆë„ìš° ê¸°ë°˜ ì£¼ë³€ ë‹¨ì–´ ì˜ˆì¸¡) Self-Attention (ë¬¸ì¥ ì „ì²´ ê´€ê³„ í•™ìŠµ)
# ë¬¸ë§¥ ê³ ë ¤ ì£¼ë³€ ëª‡ ë‹¨ì–´(ë¡œì»¬ ë¬¸ë§¥)ë§Œ ë³¸ë‹¤ ë¬¸ì¥ ì „ì²´ (ê¸€ë¡œë²Œ ë¬¸ë§¥) ë³¸ë‹¤
# ì¶”ê°€ í•™ìŠµ ì‹œ ì£¼ë³€ ë‹¨ì–´ ê¸°ë°˜ì˜ ë²¡í„° ì—…ë°ì´íŠ¸ Self-Attentionìœ¼ë¡œ ëª¨ë“  í† í° ê´€ê³„ ì—…ë°ì´íŠ¸
# ì¶œë ¥ ë²¡í„° ê³ ì • ë²¡í„°(í•™ìŠµ ëë‚˜ë©´ ë°”ë€Œì§€ ì•ŠìŒ) ì…ë ¥ ë¬¸ë§¥ë§ˆë‹¤ ë™ì  ë²¡í„° (ë¬¸ë§¥ ë”°ë¼ ë§¤ë²ˆ ë‹¤ë¦„!)
# íŠ¸ëœìŠ¤í¬ë¨¸ì˜ ê°€ì¤‘ì¹˜
# â€¢ ì„ë² ë”© ë ˆì´ì–´ (í† í° ë²¡í„°)
# â€¢ Self-Attentionì˜ Query, Key, Value ê°€ì¤‘ì¹˜
# â€¢ FeedForward ë„¤íŠ¸ì›Œí¬ ê°€ì¤‘ì¹˜
# â€¢ LayerNorm íŒŒë¼ë¯¸í„° ë“±
# 
# í…ìŠ¤íŠ¸ ë°ì´í„° ìˆ˜ì§‘ >
# í† í°í™”(Tokenization) ë“± ì „ì²˜ë¦¬>
# ì„ë² ë”© í•™ìŠµ: í† í°í™”ëœ ë‹¨ì–´ë“¤ì„ ì‚¬ìš©í•˜ì—¬ ë‹¨ì–´ ì„ë² ë”©ì„ í•™ìŠµ >
# ì„ë² ë”© ì ìš©: í•™ìŠµëœ ë‹¨ì–´ ì„ë² ë”©ì„ ì ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ë²¡í„°ë¡œ í‘œí˜„ >
# ë”¥ëŸ¬ë‹ ëª¨ë¸ì˜ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©, ë‹¨ì–´ ê°„ì˜ ìœ ì‚¬ë„ ì¸¡ì •, ë¬¸ì„œ ë¶„ë¥˜ ë“± ë‹¤ì–‘í•œ NLP ì‘ì—…
# 
# ì›Œë“œ ì„ë² ë”©ì€ ë‹¨ì–´ë¥¼ë¥¼ ê³ ì°¨ì›ì˜ ì‹¤ìˆ˜ ë²¡í„°ë¡œ ë§¤í•‘ í•˜ëŠ” ê³¼ì •
# ì„ë² ë“± ê³µê°„ì—ì„œ ë‹¨ì–´ì˜ ì˜ë¯¸ì™€ ë¬¸ë§¥ì ì¸ ìœ ì‚¬ì„±ì„ ë³´ì¡´í•˜ê¸° ìœ„í•´ ìˆ˜í–‰
# ë‹¨ì–´ ê°„ì˜ ì˜ë¯¸ì  ìœ ì‚¬ì„±ì„ ë°˜ì˜í•˜ë©´, "apple"ê³¼ "orange"ëŠ” ê°€ê¹ê²Œ ìœ„ì¹˜.
# ì›Œë“œ ì„ë² ë”©ì´ ë‹¨ì–´ì˜ ì˜ë¯¸ë¥¼ ë²¡í„° ê³µê°„ ìƒì—ì„œ ìˆ˜í•™ì ì¸ ê´€ê³„ë¡œ í‘œí˜„í•  ìˆ˜ ìˆë‹¤.
# "king - man + woman = queen"ê³¼ ê°™ì€ ë‹¨ì–´ ê°„ì˜ ì˜ë¯¸ì  ê´€ê³„ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ë²¡í„° ì—°ì‚°
# 
# ì£¼ì–´ì§„ ë¬¸ì¥ì€ ë‹¨ì—¬ë³„ë¡œ 2ì°¨ì›ì˜ ë²¡í„°ë¡œ í‘œí˜„ ê°€ëŠ¥(ì›Œë“œ ì„ë² ë”© í›„)
# ì›Œë“œ ì„ë² ë”©ì€ ë‹¨ì–´ë¥¼ ê³ ì°¨ì›ì˜ ì‹¤ìˆ˜ ë²¡í„°ë¡œ ë§¤í•‘í•˜ëŠ” ê³¼ì •
#  ë‹¨ì–´ì˜ ì˜ë¯¸ì™€ ë¬¸ë§¥ì ì¸ ìœ ì‚¬ì„±ì„ ë³´ì¡´í•˜ê¸° ìœ„í•´ ìˆ˜í–‰
# 
#  ë²¡í„°ì˜ í‘œí˜„ ë°©ì‹: Sparse vector vs Dense vector
# sparse vector(í¬ì†Œ ë²¡í„°)ëŠ”ëŒ€ë¶€ë¶„ì˜ ì›ì†Œê°€ 0ì¸ ë²¡í„°.
#  One hot encodingìœ¼ë¡œ í‘œí˜„ëœ ë²¡í„°
# (ì›í•«ì¸ì½”ë”©ì€ í¬ì†Œ ë²¡í„°ì˜ í•œ ì¢…ë¥˜)
# Dense vector(ë°€ì§‘ ë²¡í„°)ëŠ”
# ëŒ€ë¶€ë¶„ì˜ ì›ì†Œê°€ 0ì´ ì•„ë‹Œ ê°’ì„ ê°€ì§€ëŠ” ë²¡í„°.
# ì¦‰, ë²¡í„° ë‚´ì˜ ëŒ€ë‹¤ìˆ˜ ìš”ì†Œë“¤ì´ 0ì´ ì•„ë‹Œ ì‹¤ìˆ˜ê°’ìœ¼ë¡œ í‘œí˜„.
# ë°€ì§‘ ë²¡í„°ëŠ” ë‹¨ì–´ë‚˜ ê°œì²´ì˜ ì˜ë¯¸ë¥¼ ë” ì˜ í‘œí˜„í•  ìˆ˜ ìˆìœ¼ë©°, ìœ ì‚¬ë„ ê³„ì‚°ì´ë‚˜ ê¸°ê³„ í•™ìŠµ
# ì•Œê³ ë¦¬ì¦˜ì— ë” ì í•©
# 
# ì£¼ì–´ì§„ ë¬¸ì¥ì˜ ë‹¨ì–´ë¥¼ one hot encodingí‘œí˜„ë„ ê°€ëŠ¥
# ë˜ëŠ” Dense Vector(ë°€ì§‘ ë²¡í„°)ë¡œ í‘œí˜„ ê°€ëŠ¥(word Embedding)
# 
# ë¬¸ì¥ì´ 2ê°œê°€ ì•„ë‹ˆê³  2ì²œ ê°œë¼ë©´?
# ì‚¬ìš©ëœ ë‹¨ì–´ê°€ 7ê°œê°€ ì•„ë‹ˆê³  700ê°œë¼ë©´?
# 
# 1-hot encoding
# 700x700-490,000ê°œì˜
# ë©”ëª¨ë¦¬ í•„ìš”
# ë¬¸ì¥ ì „í™˜ ì‹œ í•œ ë‹¨ì–´ë§ˆë‹¤
# 700ê°œì˜ ìˆ«ì í•„ìš”
# ì„ë² ë”©
# 2x700-1,400ê°œì˜
# ë©”ëª¨ë¦¬ í•„ìš”
# ë¬¸ì¥ ì „í™˜ ì‹œ 2ê°œì˜ ìˆ«ì
# í•„ìš”
# 
# Word2Vecì€ ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¨ì–´ ê°„ ìœ ì‚¬ì„±ì„ í•™ìŠµí•˜ê³ , ë‹¨ì–´ì˜ ì˜ë¯¸ì™€ ê´€ë ¨ì„±ì„ ë²¡í„°ê³µê°„ì—ì„œ ì¶”ë¡ í•  ìˆ˜ ìˆë‹¤.
# 1) gensim ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ Word2Vec ëª¨ë¸ì„ í•™ìŠµ.
# 2) í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ sentencesë¼ëŠ” ë‹¨ì–´ì˜ ì‹œí€€ìŠ¤ë¡œ ë³€í™˜í•œ í›„, Word2Vec ëª¨ë¸ ê°ì²´ë¥¼ ìƒì„±.
# ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„°ë¡œëŠ” ë²¡í„°ì˜ í¬ê¸°(size), ìœˆë„ìš° í¬ê¸°(window size), ìµœì†Œ ë“±ì¥
# íšŸìˆ˜(min_count) ë“±ì„ ì„¤ì •.
# 3) í•™ìŠµëœ ëª¨ë¸ì—ì„œëŠ” ë‹¨ì–´ì˜ ë²¡í„° í‘œí˜„ì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.
# 
# model = Word2Vec(sentences, vector_size=100, window=5, min_count=1)
# ê° ë‹¨ì–´ë¥¼ 100ì°¨ì›ì˜ ë²¡í„°ë¡œ í‘œí˜„í•˜ë©°, ê° ë‹¨ì–´ëŠ” ì•ë’¤ë¡œ 5ê°œì˜ ë‹¨ì–´ë¥¼ ê³ ë ¤í•˜ì—¬ í•™ìŠµë˜ë©°, ê° ë‹¨ì–´ëŠ” ì ì–´ë„ í•œ ë²ˆ ì´ìƒ ë‚˜íƒ€ë‚˜ëŠ” ëª¨ë“  ë‹¨ì–´ë¥¼ í•™ìŠµí•˜ëŠ” Word2Vec ëª¨ë¸ì„ ìƒì„±
# 
# Word2Vec ëª¨ë¸ì„ ìƒì„±í•˜ëŠ” ì½”ë“œ:
# sentences: í•™ìŠµí•  ë¬¸ì¥ì˜ ë¦¬ìŠ¤íŠ¸.
# 
# vector_size: ìƒì„±ë  ê° ë‹¨ì–´ ë²¡í„°ì˜ ì°¨ì›. í´ìˆ˜ë¡ ì¢‹ì§€ë§Œ, ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ê³¼ í•™ìŠµ ì‹œê°„ì— ì˜í–¥.
# 
# window: ë‹¨ì–´ê°€ ê³ ë ¤í•´ì•¼ í•˜ëŠ” ì»¨í…ìŠ¤íŠ¸ì˜ í¬ê¸°.
# window ê°’ì´ 5ë¼ë©´, ê° ë‹¨ì–´ëŠ” ì• ë’¤ë¡œ 5ê°œì˜ ë‹¨ì–´ë¥¼ ê³ ë ¤í•˜ì—¬ í•™ìŠµ.
# 
# min_count: ëª¨ë¸ì— í•™ìŠµí•  ë•Œ ê³ ë ¤í•  ë‹¨ì–´ì˜ ìµœì†Œ ë¹ˆë„ìˆ˜.
# ì´ ê°’ì´ 1ì´ë¼ë©´, ê° ë‹¨ì–´ê°€ ì ì–´ë„ í•œ ë²ˆì€ ë‚˜íƒ€ë‚˜ì•¼ í•œë‹¤.
# vector = model.wv['NLP']
# Word2Vec ëª¨ë¸ì—ì„œ â€˜NLPâ€™ë¼ëŠ” ë‹¨ì–´ì— í•´ë‹¹í•˜ëŠ” ë²¡í„°ë¥¼ ê°€ì ¸ì™€ vectorë¼ëŠ” ë³€ìˆ˜ì— ì €ì¥
# 
# model: ë‹¨ì–´ë¥¼ ë²¡í„°ë¡œ ë³€í™˜.
# 
# wv: Word2Vec ëª¨ë¸ì—ì„œ ë‹¨ì–´ ë²¡í„°ë¥¼ ê°€ì ¸ì˜¤ëŠ” ë°©ë²•
#       wvëŠ” â€œword vectorâ€, íŠ¹ì • ë‹¨ì–´ì— ëŒ€í•œ ë²¡í„°ë¥¼ ì¡°íšŒí•˜ëŠ” ë° ì‚¬ìš©.
# 
# â€˜NLPâ€™: â€˜NLPâ€™ë¼ëŠ” ë‹¨ì–´ì— ëŒ€í•œ ë²¡í„°ë¥¼ ê°€ì ¸ì˜¨ë‹¤.
# 

# %%
# !pip install gensim
# !pip install --upgrade numpy gensim
# !pip install nltk

# %%
from gensim.models import Word2Vec

# í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì „ì²˜ë¦¬ í•˜ì—¬ ë‹¨ì–´ì˜ ì‹œí€€ìŠ¤ë¡œ ë³€í™˜
sentences = [['I','love','NLP'],['Word2Vec','is','awesome']]

# Word2Vec ëª¨ë¸ í•™ìŠµ
model =  Word2Vec(sentences,vector_size=100,window=5,min_count=1)

# ë‹¨ì–´ì˜ ë²¡í„° í‘œí˜„ í™•ì¸
vector = model.wv['NLP']
print(vector)

# %%
import nltk
nltk.download('punkt_tab')

# %%
from nltk.tokenize import word_tokenize
from gensim.models import Word2Vec

# ë¬¸ì¥ ë°ì´í„°
sentences = [
 'I love natural language processing',
 'Word2Vec is a popular word embedding model',
 'Natural language processing is an important field in Al'
]

# ë¬¸ì¥ì„ í† í°í™”
tokenized_sentences = [word_tokenize(sentence) for sentence in sentences]
print(tokenized_sentences)

# %%
from tensorflow.keras.datasets import imdb
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding,LSTM,Dense

MY_LENGTH = 80
MY_WORD = 5000
MY_SAMPLE = 10

# %%
from keras.datasets import imdb

# IMDB ë°ì´í„°ì…‹ ë¡œë“œ
(x_train, y_train),(x_test, y_test) = imdb.load_data()

# ì „ì²´ ë°ì´í„°ì…‹ì„ í•©ì¹œ í›„ ë‹¨ì–´ì˜ ì¢…ë¥˜ì˜ ìˆ˜ í™•ì¸(ì•½ 88,585 ì¢…ë¥˜)
word_set = set()
for review in x_train + x_test:
    word_set.update(review)

num_words=len(word_set)
print("ë‹¨ì–´ ì¢…ë¥˜ì˜ ìˆ˜:", num_words)

# %%
(X_train, Y_train),(X_test, Y_test) =  imdb.load_data(num_words =  MY_WORD)
# ìì£¼ ë“±ì¥í•˜ëŠ” ìƒìœ„ My_WORDê°œì˜ ë‹¨ì–´ë¥¼ ì§€ì •í•˜ì—¬ ì—…ë¡œë“œ
# IMDB word index
print('ìƒ˜í”Œ ì˜í™”í‰ \n',X_train[MY_SAMPLE]) # ì§€ì •ëœ ìƒ˜í”Œ(10)ì˜ ë°ì´í„°(ì •ìˆ˜)ë¥¼ ë³´ì—¬ì¤€ë‹¤
print('ì´ ë‹¨ì–´ ìˆ˜: \n',len(X_train[MY_SAMPLE])) # ì§€ì •ëœ ìƒ˜í”Œì˜ ë‹¨ì–´ ê°¯ìˆ˜
print('ê°ì„±(0=ë¶€ì „, 1=ê¸ì •): \n', Y_train[MY_SAMPLE]) # ì§€ì •ëœ ìƒ˜í”Œì˜ ë¼ë²¨ í‘œì‹œ

# %% [markdown]
# ì „ì²´ ë°ì´í„° ì…‹ì„ í•©ì¹œ ë‹¨ì–´ì˜ ì¢…ë¥˜ì˜ ìˆ˜ëŠ” ì•½ 88,585 ì¢…ë¥˜
# ê·¸ ì¤‘ 5000ê°œì˜ ë‹¨ì–´ë§Œ ì—…ë¡œë“œ í•˜ì—¬ x,yë°ì´í„°ë¡œ ë‚˜ëˆ”
# ìƒ˜í”Œ ë°ì´í„° 5ë²ˆì˜ ì˜í™”í‰ì„ ì¶œë ¥ : ì •ìˆ˜ë¡œ ì²˜ë¦¬ëœ ë‹¨ì–´ì˜ ì¸ë±ìŠ¤, ë‹¨ì–´ ìˆ˜,ê°ì„± ë¼ë²¨

# %%
from tensorflow.keras.datasets import  imdb

MY_SAMPLE = 10 #ì‚¬ì „ ë‹¨ì–´ ìˆ˜
MY_WORD = 5000 # ìƒ˜í”Œ ì˜í™”í‰

# ë°ì´í„° ë¡œë“œ
(x_train,_),(_,_) = imdb.load_data(num_words=MY_WORD)

# word-index ë§¤í•‘ ê°€ì ¸ì˜¤ê¸°
word_index = imdb.get_word_index()

# word-index ë§¤í•‘ ë°˜ì „í•˜ì—¬ index to word ë§¤í•‘ ìƒì„±
index_to_word = {index: word for word, index in word_index.items()}

# ìƒ˜í”Œ ì˜í™”í‰ì„ ë‹¨ì–´ë¡œ ë³€í™˜í•˜ì—¬ ì¶œë ¥
decoded_review = ' '.join(index_to_word[index] for index in x_train[MY_SAMPLE])
print('ìƒ˜í”Œ ì˜í™” í‰: \n', decoded_review)

# %% [markdown]
# ğŸŸ¦ IMDB ë°ì´í„°ì…‹ ë¡œë“œ ë° ë‹¨ì–´ ë³µì› íë¦„ ì„¤ëª…
# imdb.load_data(num_words=MY_WORD)
# â€ƒâ†’ IMDB ë°ì´í„°ì…‹ì„ ë¡œë“œí•  ë•Œ ì‚¬ìš©í•  ë‹¨ì–´ ìˆ˜(MY_WORD)ë¡œ ì œí•œí•¨.
# â€ƒâ€ƒì˜ˆ: num_words=5000ì´ë©´ ìƒìœ„ 5000ê°œì˜ ë‹¨ì–´ë§Œ ì‚¬ìš©.
# 
# (x_train, _), (_, _) = imdb.load_data(num_words=MY_WORD)
# â€ƒâ†’ í›ˆë ¨ìš© ë¦¬ë·° ë°ì´í„°ë¥¼ x_trainì—, ë¼ë²¨ì€ _ë¡œ ë¬´ì‹œí•˜ê³  ì €ì¥í•¨.
# 
# imdb.get_word_index()
# â€ƒâ†’ ë‹¨ì–´ë¥¼ í‚¤, ì •ìˆ˜ ì¸ë±ìŠ¤ë¥¼ ê°’ìœ¼ë¡œ ê°–ëŠ” ë”•ì…”ë„ˆë¦¬ í˜•íƒœì˜ word_index ë°˜í™˜.
# â€ƒâ€ƒì˜ˆ: 'fawn': 34701, 'tsukino': 52006 ...
# 
# index_to_word = {index: word for word, index in word_index.items()}
# â€ƒâ†’ ì¸ë±ìŠ¤-ë‹¨ì–´ êµ¬ì¡°ë¡œ word_indexë¥¼ ë°˜ì „ì‹œì¼œ index_to_word ë”•ì…”ë„ˆë¦¬ ìƒì„±.
# â€ƒâ€ƒì¦‰, ì •ìˆ˜ ì¸ë±ìŠ¤ë¥¼ í†µí•´ ë‹¨ì–´ë¥¼ ë‹¤ì‹œ ì°¾ì„ ìˆ˜ ìˆê²Œ í•¨.
# 
# ' '.join(index_to_word[index] for index in x_train[MY_SAMPLE])
# â€ƒâ†’ x_train[MY_SAMPLE]ì— ìˆëŠ” ë‹¨ì–´ ì¸ë±ìŠ¤ë“¤ì„ ì‹¤ì œ ë‹¨ì–´ë¡œ ë°”ê¾¸ê³  ê³µë°±ìœ¼ë¡œ ì—°ê²°.
# â€ƒâ€ƒí•˜ë‚˜ì˜ ë¬¸ì¥(ë¦¬ë·°)ìœ¼ë¡œ ë³µì›í•˜ì—¬ ì¶œë ¥ ê°€ëŠ¥.
# â€ƒâ€ƒì˜ˆ: [1, 14, 20, 6] â†’ "the movie was good"

# %%
print(word_index)

# %%
print(index_to_word)

# %%
top_words = [word for word, index in word_index.items() if index <= MY_WORD]

print('ìƒìœ„ {}ê°œì˜ ë‹¨ì–´ ëª©ë¡:'.format(MY_WORD))
for i, word in enumerate(top_words):
    print(i+1,word)

# %%
# ê° ì˜í™”í‰ì˜ ê¸¸ì´ë¥¼ ì¼ì •í•˜ê²Œ ë§ì¶¤

X_train = pad_sequences(sequences=X_train,truncating='post', #ë’·ë¶€ë¶„ ì‚­ì œ,preëŠ” ì•ë¶€ë¶„
padding ='post', maxlen = MY_LENGTH) # 80ë‹¨ì–´ë³´ë‹¤ ì§§ìœ¼ë©´ ë’¤(post) ë¥¼ 0ìœ¼ë¡œ ì±„ì›€

X_test = pad_sequences(sequences=X_test,truncating='post',
padding = 'post', maxlen=MY_LENGTH)

print('\ní•™ìŠµìš© ì…ë ¥ ë°ì´í„° ëª¨ì–‘: ',X_train.shape)
print('í•™ìŠµìš© ì¶œë ¥ ë°ì´í„° ëª¨ì–‘: ',Y_train.shape)
print('í‰ê°€ìš© ì…ë ¥ ë°ì´í„° ëª¨ì–‘: ',X_test.shape)
print('í‰ê°€ìš© ì¶œë ¥ ë°ì´í„° ëª¨ì–‘: ',Y_test.shape)
        

# %% [markdown]
# ë‹¤ìŒì€ ì´ë¯¸ì§€ì—ì„œ ì¶”ì¶œí•œ ë‚´ìš©ì„ **ì„¤ëª… ëŒ“ê¸€** í˜•ì‹ìœ¼ë¡œ ê¹”ë”í•˜ê²Œ ì •ë¦¬í•œ ê²ƒì…ë‹ˆë‹¤:
# 
# ---
# 
# ### ğŸŸ¦ ì‹œí€€ìŠ¤ ë°ì´í„° íŒ¨ë”©: `pad_sequences` ì‚¬ìš© ì„¤ëª…
# 
# * `X_test = pad_sequences(sequences=X_test, truncating='post', padding='post', maxlen=MY_LENGTH)`
#   â€ƒâ†’ ì‹œí€€ìŠ¤ ë°ì´í„°ë¥¼ ì¼ì •í•œ ê¸¸ì´ë¡œ ë§ì¶”ê¸° ìœ„í•´ **íŒ¨ë”©(padding)** ë˜ëŠ” **ì ˆë‹¨(truncating)** ì‘ì—…ì„ ìˆ˜í–‰í•œë‹¤.
# 
# ---
# 
# #### ğŸ”¹ padding:
# 
# * ì‹œí€€ìŠ¤ì˜ ê¸¸ì´ë¥¼ `maxlen`ì— ë§ì¶”ê¸° ìœ„í•´ **ë’¤ìª½ì— 0ì„ ì¶”ê°€**í•œë‹¤.
# * `'post'` ì„¤ì • ì‹œ, ì‹œí€€ìŠ¤ì˜ **ë’·ë¶€ë¶„ì— íŒ¨ë”©**ì„ ì¶”ê°€í•œë‹¤.
#   â€ƒì˜ˆ: `[1, 2, 3]` â†’ `[1, 2, 3, 0, 0]`
# 
# #### ğŸ”¹ truncating:
# 
# * ì‹œí€€ìŠ¤ê°€ ë„ˆë¬´ ê¸¸ ê²½ìš° **ë’·ë¶€ë¶„ì„ ì˜ë¼ë‚¸ë‹¤**.
# * `'post'` ì„¤ì • ì‹œ, ì‹œí€€ìŠ¤ì˜ **ë’¤ìª½ì„ ì˜ë¼ë‚¸ë‹¤**.
#   â€ƒì˜ˆ: `[1, 2, 3, 4, 5, 6]` â†’ `[1, 2, 3, 4, 5]` (if `maxlen=5`)
# 
# ---
# 
# ### âœ… ìš”ì•½
# 
# * `padding='post'`: ì‹œí€€ìŠ¤ ë’¤ì— íŒ¨ë”© ì¶”ê°€
# * `truncating='post'`: ì‹œí€€ìŠ¤ ë’¤ë¥¼ ì˜ë¼ëƒ„
# * `maxlen=...`: ìµœì¢… ì‹œí€€ìŠ¤ ê¸¸ì´ ì§€ì •
# 
# ---
# 
# í•„ìš”í•˜ì‹œë©´ `pad_sequences`ì˜ ì•ìª½ íŒ¨ë”© `'pre'`ê³¼ ë¹„êµí•˜ê±°ë‚˜ ì‹¤ì œ ë°ì´í„° ì „ì²˜ë¦¬ ì½”ë“œë„ ì œê³µí•´ ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤!
# 

# %%
# RNN êµ¬í˜„
from tensorflow.keras.layers import Bidirectional
from tensorflow.keras.layers import Dropout

model = Sequential()
model.add(Embedding(input_dim=MY_WORD, output_dim=128,
                    input_length=MY_LENGTH))
model.add(Bidirectional(LSTM(units=64)))
model.add(LSTM(64, return_sequences=True))
model.add(LSTM(64))
model.add(Dropout(0.5))
model.add(LSTM(units=64, input_shape=(MY_LENGTH, 32)))
model.add(Dense(units=1, activation='sigmoid'))

print('RNN ìš”ì•½')
model.build(input_shape=(None, MY_LENGTH))
model.summary()



# %% [markdown]
# ì„ë² ë”© ì°¨ì› í™•ì¥: ë‹¨ì–´ ì˜ë¯¸ë¥¼ ë” ì˜ í‘œí˜„
# ì–‘ë°©í–¥ LSTM: ë¬¸ë§¥ì„ ì•ë’¤ë¡œ ì‚´í´ì„œ ë” í’ë¶€í•˜ê²Œ í•™ìŠµ
# LSTM ì¸µ ìŒ“ê¸°: ê¹Šì€ ë„¤íŠ¸ì›Œí¬ë¡œ ì„±ëŠ¥ í–¥ìƒ
# Dropout: ê³¼ì í•© ë°©ì§€
# RMSprop: LSTMì— ìì£¼ ì“°ì´ëŠ” ì˜µí‹°ë§ˆì´ì €
# validation_split: í•™ìŠµ ì¤‘ ê³¼ì í•© ì—¬ë¶€ í™•ì¸
# ì¶”ê°€ ì§€í‘œ(confusion matrix, classification report): ëª¨ë¸ì˜ ê°•ì ê³¼ ì•½ì  íŒŒì•…
# ì‹œê°í™”: í•™ìŠµê³¼ì • íŒŒì•…

# %%
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional

model = Sequential()
model.add(Embedding(input_dim=MY_WORD, output_dim=128, input_length=MY_LENGTH))

# ì–‘ë°©í–¥ LSTM â†’ ì‹œí€€ìŠ¤ ì¶œë ¥ ìœ ì§€
model.add(Bidirectional(LSTM(units=64, return_sequences=True)))

# Stacked LSTM (ê³„ì† ì‹œí€€ìŠ¤ ìœ ì§€)
model.add(LSTM(64, return_sequences=True))

# ë§ˆì§€ë§‰ LSTM â†’ ì‹œí€€ìŠ¤ X, ìµœì¢… ë²¡í„°ë§Œ ì¶œë ¥
model.add(LSTM(64))

# Dropout ì ìš©
model.add(Dropout(0.5))

# ì¶œë ¥ì¸µ
model.add(Dense(units=1, activation='sigmoid'))

# ëª¨ë¸ êµ¬ì¡° í™•ì¸
print('RNN ìš”ì•½')
model.build(input_shape=(None, MY_LENGTH))
model.summary()


# %% [markdown]
# ë‹¤ìŒì€ ë‘ ì´ë¯¸ì§€ì—ì„œ ì¶”ì¶œí•œ **í…ìŠ¤íŠ¸ ì „ì²´ ë‚´ìš©**ì…ë‹ˆë‹¤:
# 
# ---
# 
# ## âœ… ì´ë¯¸ì§€ 1 í…ìŠ¤íŠ¸
# 
# ```python
# model.add(Embedding(input_dim=MY_WORD, output_dim=32, input_length=MY_LENGTH))
# ```
# 
# * model.add() ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ê·¸ ì•ˆì— Embedding ë ˆì´ì–´ë¥¼ ì¶”ê°€.
# * Embeddingì€ ë‹¨ì–´ë¥¼ ê³ ì •ëœ í¬ê¸°ì˜ ë²¡í„°ë¡œ ë³€í™˜í•´ì£¼ëŠ” ì¸µ.
# * input\_dim: ëª¨ë¸ì´ ì‚¬ìš©í•  ì „ì²´ ë‹¨ì–´ì˜ ê°œìˆ˜. ì „ì²´ ë‹¨ì–´ ì§‘í•©ì˜ í¬ê¸°.
# * output\_dim: ë‹¨ì–´ ì„ë² ë”© ì°¨ì› ìˆ˜. ì„ë² ë”© ë²¡í„°ì˜ ì°¨ì›
# * input\_length: ì…ë ¥ ì‹œí€€ìŠ¤ì˜ ê¸¸ì´ ì„¤ì •.
# 
# \*Embedding: ë‹¨ì–´ì— ë²¡í„°ë¥¼ í• ë‹¹ í‘œí˜„í•˜ì—¬ ê³ ì •ëœ í¬ê¸°ë¡œ ì¶”í›„ ì‚¬ìš©.
# 
# * ì˜ˆ) ë‹¨ì–´ ìˆ˜ê°€ ë§ì€ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ìˆ«ìë¡œ ë°”ê¿”ì•¼ í•˜ë¯€ë¡œ ì„ë² ë”© ì¸µì„ ì‚¬ìš©.
# * ì…ë ¥ ì •ìˆ˜ â†’ ì„ë² ë”© ë²¡í„°ë¡œ ë§¤í•‘
# * Embeddingì€ ëª¨ë¸ì˜ ì…ë ¥ì¸µ ë°”ë¡œ ë‹¤ìŒì— ìœ„ì¹˜í•˜ë©°, í›ˆë ¨ ê³¼ì •ì—ì„œ í•™ìŠµëœë‹¤.
# * í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ê³¼ì •ì—ì„œ ìˆ˜ì¹˜í™”ëœ ë°ì´í„°ë¥¼ ì„ë² ë”© ì¸µì— í†µê³¼ì‹œì¼œ ë‹¨ì–´ ë²¡í„° ìƒì„±
# * ì…ë ¥ ì •ìˆ˜ëŠ” ë‹¨ì–´ì˜ ì¸ë±ìŠ¤ì´ë©°, ê° ì¸ë±ìŠ¤ëŠ” ê³ ìœ í•œ ì„ë² ë”© ë²¡í„°ì— ë§¤í•‘ëœë‹¤.
# * Embeddingì¸µì€ í•™ìŠµì„ í†µí•´ ê° ë‹¨ì–´ì˜ ì˜ë¯¸ë¥¼ íŒŒì•…í•˜ì—¬ ë²¡í„°ë¥¼ êµ¬ì„±
# 
# ```python
# model.add(LSTM(units=64, input_shape=(MY_LENGTH, 32)))
# ```
# 
# * LSTMì€ ì¥ê¸° ì˜ì¡´ì„±ì„ í•™ìŠµí•  ìˆ˜ ìˆëŠ” ìˆœí™˜ ì‹ ê²½ë§ ë ˆì´ì–´.
# * \*unitsëŠ” LSTM ë ˆì´ì–´ì˜ ì¶œë ¥ ì°¨ì› ë˜ëŠ” ë…¸ë“œì˜ ê°œìˆ˜
# * \*input\_shapeëŠ” ì…ë ¥ ì‹œí€€ìŠ¤ì˜ í˜•íƒœ. (ì‹œí€€ìŠ¤ ê¸¸ì´, ì…ë ¥ ì°¨ì›)
# 
# ---
# 
# ## âœ… ì´ë¯¸ì§€ 2 í…ìŠ¤íŠ¸
# 
# ### íŒŒë¼ë¯¸í„° ê³„ì‚° ê³µì‹
# 
# **1. Embedding ë ˆì´ì–´:**
# 
# * íŒŒë¼ë¯¸í„° ê°œìˆ˜ = ì…ë ¥ ì°¨ì› Ã— ì¶œë ¥ ì°¨ì›
#   ì˜ˆ) ì…ë ¥ ì°¨ì›: ë‹¨ì–´ ìˆ˜(5000), ì¶œë ¥ ì°¨ì›: ì„ë² ë”© í¬ê¸°(32)
#   â†’ 5000 Ã— 32 = **160,000**
# 
# **2. LSTM ë ˆì´ì–´:**
# 
# * íŒŒë¼ë¯¸í„° ê°œìˆ˜ = (ì…ë ¥ ì°¨ì› + LSTM ë‚´ë¶€ ìƒíƒœ í¬ê¸°) Ã— 4 Ã— LSTM ë‚´ë¶€ ìƒíƒœ í¬ê¸°
#   ì˜ˆ) ì…ë ¥ ì°¨ì› 32, LSTM ìƒíƒœ í¬ê¸° 64
#   â†’ (32 + 64) Ã— 4 Ã— 64 = **24,576**
# 
# **3. Dense ë ˆì´ì–´:**
# 
# * íŒŒë¼ë¯¸í„° ê°œìˆ˜ = (ì…ë ¥ ì°¨ì› + í¸í–¥) Ã— ì¶œë ¥ ì°¨ì›
#   ì˜ˆ) ì…ë ¥ ì°¨ì› 64, ì¶œë ¥ ì°¨ì› 1
#   â†’ (64 + 1) Ã— 1 = **65**
# 
# ---
# 
# ### LSTM ë ˆì´ì–´ëŠ”
# 
# ê²Œì´íŠ¸(gate)ë¼ê³  ë¶ˆë¦¬ëŠ” êµ¬ì¡°ë¥¼ ê°€ì§€ê³  ìˆìœ¼ë©°,
# ê²Œì´íŠ¸ì˜ ì—­í• ì€ ì–´ë–¤ ì •ë³´ë¥¼ ê¸°ì–µí•˜ê³  ì–´ë–¤ ì •ë³´ë¥¼ ì „ë‹¬í• ì§€ë¥¼ ê²°ì •í•˜ëŠ” ê²ƒ.
# LSTMì˜ íŒŒë¼ë¯¸í„°ëŠ” ì…ë ¥ ê²Œì´íŠ¸, ë§ê° ê²Œì´íŠ¸, ì¶œë ¥ ê²Œì´íŠ¸, ì…€ ìƒíƒœë¥¼ ì¡°ì ˆ
# 
# LSTM ë ˆì´ì–´ì˜ íŒŒë¼ë¯¸í„° ê°œìˆ˜ ê³„ì‚°ì‹:
# 
# * ì…ë ¥ ì°¨ì›(input\_dim): ì´ì „ ì¸µì˜ ì¶œë ¥ ì°¨ì› ë“±
# * LSTM ë‚´ë¶€ ìƒíƒœ í¬ê¸°(units): LSTM ë ˆì´ì–´ê°€ ê°€ì§€ëŠ” ë©”ëª¨ë¦¬ ì…€ì˜ ê°œìˆ˜ë¥¼ ì˜ë¯¸.
# 
# > ì˜ˆ: input\_dim=32, units=64ì¸ ê²½ìš°
# > â†’ (32 + 64) Ã— 4 Ã— 64 = **24,576**
# 
# * ì™œ Ã—4?: LSTMì€ 4ê°œì˜ ê²Œì´íŠ¸ë¥¼ í•™ìŠµí•˜ê¸° ë•Œë¬¸ (ì…ë ¥ ê²Œì´íŠ¸, ë§ê° ê²Œì´íŠ¸, ì¶œë ¥ ê²Œì´íŠ¸, ì…€ ìƒíƒœ ê²Œì´íŠ¸)
# * LSTMì€ gateë§ˆë‹¤ weightë¥¼ ê°€ì§€ë¯€ë¡œ, ì´ íŒŒë¼ë¯¸í„° ìˆ˜ëŠ” 4ë°°ë¡œ ê³„ì‚°ë¨.
# 
# ---
# 
# í•„ìš”í•˜ì‹œë©´ ì´ ë‚´ìš©ì„ ìŠ¬ë¼ì´ë“œ í˜•íƒœë‚˜ ìš”ì•½í‘œë¡œë„ ì •ë¦¬í•´ë“œë¦´ ìˆ˜ ìˆì–´ìš”!
# 

# %%
# RNN í•™ìŠµ
# model.compile(optimizer='adam', loss='binary_crossentropy',
# metrics=['acc'])

from tensorflow.keras.callbacks import EarlyStopping

early_stop = EarlyStopping(
    monitor='val_loss',     # ëª¨ë‹ˆí„°í•  ê¸°ì¤€ (val_accuracy ë„ ê°€ëŠ¥)
    patience=10,             # ê°œì„ ì´ ì—†ë”ë¼ë„ ê¸°ë‹¤ë¦´ ì—í¬í¬ ìˆ˜
    restore_best_weights=True  # ê°€ì¥ ì„±ëŠ¥ ì¢‹ì•˜ë˜ ê°€ì¤‘ì¹˜ë¡œ ë³µì›
)

model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])

print('\n í•™ìŠµ ì‹œì‘')
history = model.fit(
    X_train, Y_train,
    epochs=100,
    batch_size=200,
    validation_split=0.2,
    callbacks=[early_stop]
)

# %%
# RNN í‰ê°€
score = model.evaluate(x=X_test, y=Y_test,verbose=1)
print("ìµœì¢… ì •í™•ë„: {:.2f}".format(score[1]))

# %%
# RNN ì˜ˆì¸¡

test = X_test[MY_SAMPLE].reshape(1,80)
pred = model.predict(test)
pred = (pred>0.5)
from sklearn.metrics import classification_report, confusion_matrix
pred = (model.predict(X_test) > 0.5).astype("int32")
print(confusion_matrix(y_test, pred))
print(classification_report(y_test, pred))

print('\n ìƒ˜í”Œ ì˜í™”í‰ :\n', test)
print('RNN ê°ì„± ì˜ˆì¸¡ :', pred)
print('ì •ë‹´(0=ë¶€ì •,1=ê¸ì •):', Y_test[MY_SAMPLE])

# %%
import matplotlib.pyplot as plt

# history = model.fit(...)
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('ëª¨ë¸ ì •í™•ë„')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend(['Train', 'Val'], loc='upper left')
plt.show()



