{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOeKNMP4llM37PXZeqC6m3o"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":16,"metadata":{"id":"JTzbcPNeduhq","executionInfo":{"status":"ok","timestamp":1747977504918,"user_tz":-540,"elapsed":13,"user":{"displayName":"seongho Park","userId":"07258861152940429061"}}},"outputs":[],"source":["import numpy as np\n","import tensorflow as tf\n","from sklearn import datasets\n","from sklearn.model_selection import train_test_split\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","import numpy as np\n","from tensorflow import keras\n","\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import confusion_matrix,classification_report"]},{"cell_type":"code","source":["class IrisModel:\n","  def __init__(self):\n","    # 가중치와 편향 초기화\n","    self.w1 = tf.Variable(tf.random.normal([4,50]), dtype=tf.float32)\n","    self.b1 = tf.Variable(tf.zeros([50]), dtype=tf.float32)\n","    self.w2 = tf.Variable(tf.random.normal([50,30]), dtype=tf.float32)  # 올바름\n","    self.b2 = tf.Variable(tf.zeros([30]),dtype=tf.float32)\n","    self.w3 = tf.Variable(tf.random.normal([30,3]), dtype=tf.float32)\n","    self.b3 = tf.Variable(tf.zeros([3]),dtype=tf.float32)\n","\n","  def __call__(self, x):\n","    x = tf.nn.sigmoid(tf.matmul(x,self.w1) + self.b1)\n","    x = tf.nn.sigmoid(tf.matmul(x,self.w2) + self.b2)\n","    return tf.nn.softmax(tf.matmul(x,self.w3) + self.b3)"],"metadata":{"id":"tvwjOZetN49C","executionInfo":{"status":"ok","timestamp":1747977736198,"user_tz":-540,"elapsed":40,"user":{"displayName":"seongho Park","userId":"07258861152940429061"}}},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":["IrisModel이라는 커스텀 딥러닝 모델을 클래스로 정의\n","이 클래스는 3층 MLP (다층 퍼셉트론) 구조\n","tf.Variable(...)을 사용한 이유: 학습 도중 업데이트 가능한 파라미터\n","입력 차원: 4 (Iris 데이터의 특징 수)\n","1층 은닉층: 50개의 뉴런W1: (4, 50), b1: (50,)\n","2층 은닉층: 30개의 뉴런W2: (50, 30), b2: (30,)\n","출력층: 3개의 뉴런 (클래스 개수)W3: (30, 3), b3: (3,)\n","_call__() 메서드를 구현\n","model(x)처럼 호출 가능 (함수처럼 사용 가능)\n","입력 x (예: (None, 4))를 W1과 곱하고 b1을 더해 은닉층 1의 출력 계산\n","시그모이드 활성화 함수 적용\n","은닉층 2로 다시 입력 후 같은 방식 반복\n","마지막에 softmax로 다중 분류 확률값 출력"],"metadata":{"id":"PE7TpNYCPkLP"}},{"cell_type":"code","source":["# 손실 함수 정의 (CrossEntropy)\n","def loss_fn(model,inputs,labels):\n","  predictions = model(inputs)\n","  labels_one_hot = tf.one_hot(labels, depth=3) # one hot encoding\n","  loss = tf.reduce_mean(tf.losses.categorical_crossentropy(labels_one_hot,predictions))\n","  return loss\n","\n","optimizer  = tf.optimizers.Adam(learning_rate=0.001)\n","\n","def train_step(model, inputs, labels):\n","  with tf.GradientTape() as tape:\n","    loss = loss_fn(model, inputs, labels)  # 손실 계산\n","\n","  gradients = tape.gradient(loss, [model.w1, model.b1, model.w2, model.b2, model.w3, model.b3])  # 그래디언트 계산\n","  optimizer.apply_gradients(zip(gradients, [model.w1, model.b1, model.w2, model.b2, model.w3, model.b3]))  # 가중치 업데이트\n","  return loss\n","\n",""],"metadata":{"id":"izcjRpuXPhbE","executionInfo":{"status":"ok","timestamp":1747978164819,"user_tz":-540,"elapsed":36,"user":{"displayName":"seongho Park","userId":"07258861152940429061"}}},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":["입력 데이터 → 모델 → 예측값\n","↘ 손실 계산 (one-hot + crossentropy)\n","↘ GradientTape로 미분\n","↘ 옵티마이저가 가중치 업데이트\n","loss = tf.reduce_mean(tf.losses.categorical_crossentropy(labels_one_hot, predictions))\n","• categorical cross-entropy 손실 계산\n","• one-hot 라벨 vs softmax 예측 확률 비교\n","• reduce_mean: 배치 전체 평균 손실값\n","optimizer = tf.optimizers.Adam(learning_rate=0.001)\n","• Adam 옵티마이저 생성 (기본은 0.001 학습률)\n","• 모델의 가중치를 업데이트하는 역할 담당\n","def train_step(model, inputs, labels):한 배치에 대해 모델을 학습시키는 함수\n","with tf.GradientTape() as tape:\n","자동 미분을 위한 컨텍스트, 이 블록 안의 연산을 테이프에 기록하여 나중에 미분 가능"],"metadata":{"id":"Z4MN0Zz6hXm5"}},{"cell_type":"code","source":["# 정확도 계산 함수\n","def compute_accuracy(model,inputs,labels):\n","  predictions = model(inputs)\n","  predicted_class = tf.argmax(predictions, axis=1)\n","  accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted_class, labels),tf.float32))\n","  return accuracy\n","# 데이터 로드\n","iris = datasets.load_iris()\n","X, y = iris.data,iris.target\n","\n","# Train/test split\n","X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=1)\n","\n","# 데이터를 텐서로 변환\n","X_train = tf.convert_to_tensor(X_train, dtype=tf.float32)\n","y_train = tf.convert_to_tensor(y_train, dtype=tf.int64)\n","X_test = tf.convert_to_tensor(X_test, dtype=tf.float32)\n","y_test = tf.convert_to_tensor(y_test, dtype=tf.int64)"],"metadata":{"id":"ViqSxMwshYex","executionInfo":{"status":"ok","timestamp":1747978166934,"user_tz":-540,"elapsed":13,"user":{"displayName":"seongho Park","userId":"07258861152940429061"}}},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":["정확도 계산 함수 정의\n","def compute_accuracy(model, inputs, labels):\n","모델과 정답 라벨을 받아 **분류 정확도(accuracy)**를 계산\n","predicted_class = tf.argmax(predictions, axis=1)\n","가장 확률이 높은 클래스 인덱스를 선택\n","예: [0.1, 0.7, 0.2] → 1\n","accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted_class, labels), tf.float32))\n","예측값과 실제 정답을 비교해서 맞은 것만 1.0, 틀리면 0.0\n","평균을 내어 전체 정확도 산출\n","X_train = tf.convert_to_tensor(X_train, dtype=tf.float32)\n","y_train = tf.convert_to_tensor(y_train, dtype=tf.int64)\n","X_test = tf.convert_to_tensor(X_test, dtype=tf.float32)\n","y_test = tf.convert_to_tensor(y_test, dtype=tf.int64)\n","NumPy 배열 → TensorFlow 텐서로 변환,\n","입력값(X)은 부동소수점(float32), 레이블(y)은 정수형(int64)\n","TensorFlow는 학습 및 예측 시 내부적으로 Tensor 객체를 사용하므로 이 변환은 필수"],"metadata":{"id":"K1avSy_siuhL"}},{"cell_type":"code","source":["model = IrisModel()\n","\n","# 학습\n","num_epochs = 20\n","batch_size = 16\n","num_batches = int(np.ceil(len(X_train)/ batch_size))\n","\n","for epoch in range(num_epochs):\n","  for i in range(num_batches):\n","    start_idx = i * batch_size\n","    end_idx = start_idx + batch_size\n","    X_batch = X_train[start_idx:end_idx]\n","    y_batch = y_train[start_idx:end_idx]\n","\n","    loss = train_step(model, X_batch, y_batch) # 학습 단계\n","\n","  if epoch % 5 == 0:\n","    train_accuracy = compute_accuracy(model,X_train,y_train)\n","    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss:.4f}, Accuracy: {train_accuracy:.4f}')\n","# 평가\n","test_accuracy = compute_accuracy(model,X_test,y_test)\n","print(f'Test Accuracy: {test_accuracy:.4f}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":384},"id":"g5cJJfiUidcJ","executionInfo":{"status":"error","timestamp":1747978169149,"user_tz":-540,"elapsed":55,"user":{"displayName":"seongho Park","userId":"07258861152940429061"}},"outputId":"5052aee8-3a22-4e36-c535-4bd7b9e99ccd"},"execution_count":35,"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"'IrisModel' object has no attribute 'w1'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-35-8c3e68773058>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart_idx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 학습 단계\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m5\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-33-655af4aeaebd>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(model, inputs, labels)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 손실 계산\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m   \u001b[0mgradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 그래디언트 계산\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m   \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 가중치 업데이트\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'IrisModel' object has no attribute 'w1'"]}]},{"cell_type":"code","source":["import numpy as np\n","import tensorflow as tf\n","from sklearn import datasets\n","from sklearn.model_selection import train_test_split\n","\n","class IrisModel:\n","  def __init__(self):\n","    self.W1=tf.Variable(tf.random.normal([4,50]), dtype=tf.float32)\n","    self.b1=tf.Variable(tf.zeros([50]), dtype=tf.float32)\n","    self.W2=tf.Variable(tf.random.normal([50,30]), dtype=tf.float32)\n","    self.b2=tf.Variable(tf.zeros([30]), dtype=tf.float32)\n","    self.W3=tf.Variable(tf.random.normal([30,3]), dtype=tf.float32)\n","    self.b3=tf.Variable(tf.zeros([3]), dtype=tf.float32)\n","\n","  def __call__(self, x):\n","    x=tf.nn.sigmoid(tf.matmul(x, self.W1)+self.b1)\n","    x=tf.nn.sigmoid(tf.matmul(x, self.W2)+self.b2)\n","    return tf.nn.softmax(tf.matmul(x, self.W3)+self.b3)\n","\n","def loss_fn(model, inputs, labels):\n","  predictions = model(inputs)\n","  labels_one_hot = tf.one_hot(labels, depth=3)\n","  loss = tf.reduce_mean(tf.losses.categorical_crossentropy(labels_one_hot, predictions))\n","  return loss\n","\n","optimizer = tf.optimizers.Adam(learning_rate=0.001)\n","\n","def train_step(model, inputs, labels):\n","  with tf.GradientTape() as tape:\n","    loss = loss_fn(model, inputs, labels)\n","  gradients = tape.gradient(loss, [model.W1, model.b1, model.W2, model.b2, model.W3, model.b3])\n","  optimizer.apply_gradients(zip(gradients, [model.W1, model.b1, model.W2, model.b2, model.W3, model.b3]))\n","  return loss\n","\n","\n","def compute_accuracy(model, inputs, labels):\n","  predictions = model(inputs)\n","  predicted_class=tf.argmax(predictions, axis=1)\n","  accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted_class, labels), tf.float32))\n","  return accuracy\n","\n","iris = datasets.load_iris()\n","X, y = iris.data, iris.target\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n","\n","X_train = tf.convert_to_tensor(X_train, dtype=tf.float32)\n","y_train = tf.convert_to_tensor(y_train, dtype=tf.int64)\n","X_test = tf.convert_to_tensor(X_test, dtype=tf.float32)\n","y_test = tf.convert_to_tensor(y_test, dtype=tf.int64)\n","\n","model = IrisModel()\n","\n","num_epochs =20\n","batch_size=16\n","num_batches = int(np.ceil(len(X_train)/batch_size))\n","\n","for epoch in range(num_epochs):\n","  for i in range(num_batches):\n","    start_idx = i * batch_size\n","    end_idx = start_idx + batch_size\n","    X_batch = X_train[start_idx:end_idx]\n","    y_batch = y_train[start_idx:end_idx]\n","\n","    loss = train_step(model, X_batch, y_batch)\n","\n","  if epoch % 5 == 0:\n","    train_accuracy = compute_accuracy(model, X_train, y_train)\n","    print(f\"Epoch {epoch+1}/{num_epochs}, Loss:{loss:.4f}, Accuracy:{train_accuracy:.4f}\")\n","\n","    test_accuracy = compute_accuracy(model, X_test, y_test)\n","    print(f\"Test Accuracy: { test_accuracy: .4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_Mxx_SrEmPlc","executionInfo":{"status":"ok","timestamp":1747977837365,"user_tz":-540,"elapsed":6669,"user":{"displayName":"seongho Park","userId":"07258861152940429061"}},"outputId":"65aa8bfe-9cc5-45b2-ba34-74f71849db24"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/20, Loss:3.2042, Accuracy:0.3048\n","Test Accuracy:  0.4000\n","Epoch 6/20, Loss:1.3733, Accuracy:0.3048\n","Test Accuracy:  0.4000\n","Epoch 11/20, Loss:0.8750, Accuracy:0.4286\n","Test Accuracy:  0.3111\n","Epoch 16/20, Loss:0.8016, Accuracy:0.8000\n","Test Accuracy:  0.6222\n"]}]}]}